@ARTICLE{Sukhbaatar2015-iy,
  title         = "Weakly Supervised Memory Networks",
  author        = "Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason
                   and Fergus, Rob",
  abstract      = "We introduce a neural network with a recurrent attention
                   model over a possibly large external memory. The
                   architecture is a form of Memory Network but unlike the
                   model in that work, it is trained end-to-end, and hence
                   requires significantly less supervision during training,
                   making it more generally applicable in realistic settings.
                   It can also be seen as an extension of RNNsearch to the case
                   where multiple computational steps (hops) are performed per
                   output symbol. The flexibility of the model allows us to
                   apply it to tasks as diverse as (synthetic) question
                   answering and to language modeling. For the former our
                   approach is competitive with Memory Networks, but with less
                   supervision. For the latter, on the Penn TreeBank and Text8
                   datasets our approach demonstrates slightly better
                   performance than RNNs and LSTMs. In both cases we show that
                   the key concept of multiple computational hops yields
                   improved results.",
  month         =  mar,
  year          =  2015,
  keywords      = "Models;NLP",
  archivePrefix = "arXiv",
  eprint        = "1503.08895",
  primaryClass  = "cs.NE",
  arxivid       = "1503.08895"
}

@ARTICLE{Graves2014-jg,
  title         = "Neural Turing Machines",
  author        = "Graves, Alex and Wayne, Greg and Danihelka, Ivo",
  abstract      = "We extend the capabilities of neural networks by coupling
                   them to external memory resources, which they can interact
                   with by attentional processes. The combined system is
                   analogous to a Turing Machine or Von Neumann architecture
                   but is differentiable end-to-end, allowing it to be
                   efficiently trained with gradient descent. Preliminary
                   results demonstrate that Neural Turing Machines can infer
                   simple algorithms such as copying, sorting, and associative
                   recall from input and output examples.",
  month         =  oct,
  year          =  2014,
  keywords      = "Models;NLP",
  archivePrefix = "arXiv",
  eprint        = "1410.5401",
  primaryClass  = "cs.NE",
  arxivid       = "1410.5401"
}

@ARTICLE{Weston2014-tq,
  title         = "Memory Networks",
  author        = "Weston, Jason and Chopra, Sumit and Bordes, Antoine",
  abstract      = "We describe a new class of learning models called memory
                   networks. Memory networks reason with inference components
                   combined with a long-term memory component; they learn how
                   to use these jointly. The long-term memory can be read and
                   written to, with the goal of using it for prediction. We
                   investigate these models in the context of question
                   answering (QA) where the long-term memory effectively acts
                   as a (dynamic) knowledge base, and the output is a textual
                   response. We evaluate them on a large-scale QA task, and a
                   smaller, but more complex, toy task generated from a
                   simulated world. In the latter, we show the reasoning power
                   of such models by chaining multiple supporting sentences to
                   answer questions that require understanding the intension of
                   verbs.",
  month         =  oct,
  year          =  2014,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "1410.3916v10",
  primaryClass  = "cs.AI",
  arxivid       = "1410.3916v10"
}

@ARTICLE{Xu2017-hq,
  title         = "Jointly Attentive {Spatial-Temporal} Pooling Networks for
                   Video-based Person {Re-Identification}",
  author        = "Xu, Shuangjie and Cheng, Yu and Gu, Kang and Yang, Yang and
                   Chang, Shiyu and Zhou, Pan",
  abstract      = "Person Re-Identification (person re-id) is a crucial task as
                   its applications in visual surveillance and human-computer
                   interaction. In this work, we present a novel joint Spatial
                   and Temporal Attention Pooling Network (ASTPN) for
                   video-based person re-identification, which enables the
                   feature extractor to be aware of the current input video
                   sequences, in a way that interdependency from the matching
                   items can directly influence the computation of each other's
                   representation. Specifically, the spatial pooling layer is
                   able to select regions from each frame, while the attention
                   temporal pooling performed can select informative frames
                   over the sequence, both pooling guided by the information
                   from distance matching. Experiments are conduced on the
                   iLIDS-VID, PRID-2011 and MARS datasets and the results
                   demonstrate that this approach outperforms existing
                   state-of-art methods. We also analyze how the joint pooling
                   in both dimensions can boost the person re-id performance
                   more effectively than using either of them separately.",
  month         =  aug,
  year          =  2017,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "1708.02286",
  primaryClass  = "cs.CV",
  arxivid       = "1708.02286"
}

@ARTICLE{Jaderberg2015-xb,
  title         = "Spatial Transformer Networks",
  author        = "Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and
                   Kavukcuoglu, Koray",
  abstract      = "Convolutional Neural Networks define an exceptionally
                   powerful class of models, but are still limited by the lack
                   of ability to be spatially invariant to the input data in a
                   computationally and parameter efficient manner. In this work
                   we introduce a new learnable module, the Spatial
                   Transformer, which explicitly allows the spatial
                   manipulation of data within the network. This differentiable
                   module can be inserted into existing convolutional
                   architectures, giving neural networks the ability to
                   actively spatially transform feature maps, conditional on
                   the feature map itself, without any extra training
                   supervision or modification to the optimisation process. We
                   show that the use of spatial transformers results in models
                   which learn invariance to translation, scale, rotation and
                   more generic warping, resulting in state-of-the-art
                   performance on several benchmarks, and for a number of
                   classes of transformations.",
  month         =  jun,
  year          =  2015,
  keywords      = "CNN - Classification;Theory/Tuning/Tricks;LLMs",
  archivePrefix = "arXiv",
  eprint        = "1506.02025",
  primaryClass  = "cs.CV",
  arxivid       = "1506.02025"
}

@ARTICLE{Chen2015-hs,
  title         = "Attention to Scale: Scale-aware Semantic Image Segmentation",
  author        = "Chen, Liang-Chieh and Yang, Yi and Wang, Jiang and Xu, Wei
                   and Yuille, Alan L",
  abstract      = "Incorporating multi-scale features in fully convolutional
                   neural networks (FCNs) has been a key element to achieving
                   state-of-the-art performance on semantic image segmentation.
                   One common way to extract multi-scale features is to feed
                   multiple resized input images to a shared deep network and
                   then merge the resulting features for pixelwise
                   classification. In this work, we propose an attention
                   mechanism that learns to softly weight the multi-scale
                   features at each pixel location. We adapt a state-of-the-art
                   semantic image segmentation model, which we jointly train
                   with multi-scale input images and the attention model. The
                   proposed attention model not only outperforms average- and
                   max-pooling, but allows us to diagnostically visualize the
                   importance of features at different positions and scales.
                   Moreover, we show that adding extra supervision to the
                   output at each scale is essential to achieving excellent
                   performance when merging multi-scale features. We
                   demonstrate the effectiveness of our model with extensive
                   experiments on three challenging datasets, including
                   PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO
                   2014.",
  month         =  nov,
  year          =  2015,
  keywords      = "Dense Pixel (eg Segmentation);Models",
  archivePrefix = "arXiv",
  eprint        = "1511.03339",
  primaryClass  = "cs.CV",
  arxivid       = "1511.03339"
}

@ARTICLE{Mansimov2015-wf,
  title         = "Generating Images from Captions with Attention",
  author        = "Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and
                   Salakhutdinov, Ruslan",
  abstract      = "Motivated by the recent progress in generative models, we
                   introduce a model that generates images from natural
                   language descriptions. The proposed model iteratively draws
                   patches on a canvas, while attending to the relevant words
                   in the description. After training on Microsoft COCO, we
                   compare our model with several baseline generative models on
                   image generation and retrieval tasks. We demonstrate that
                   our model produces higher quality samples than other
                   approaches and generates images with novel scene
                   compositions corresponding to previously unseen captions in
                   the dataset.",
  month         =  nov,
  year          =  2015,
  keywords      = "VisualQA/Captioning;Models;CNN -
                   Classification;Detection/Recognition;Visualization;NLP",
  archivePrefix = "arXiv",
  eprint        = "1511.02793",
  primaryClass  = "cs.LG",
  arxivid       = "1511.02793"
}

@ARTICLE{Bahdanau2014-qa,
  title         = "Neural Machine Translation by Jointly Learning to Align and
                   Translate",
  author        = "Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua",
  abstract      = "Neural machine translation is a recently proposed approach
                   to machine translation. Unlike the traditional statistical
                   machine translation, the neural machine translation aims at
                   building a single neural network that can be jointly tuned
                   to maximize the translation performance. The models proposed
                   recently for neural machine translation often belong to a
                   family of encoder-decoders and consists of an encoder that
                   encodes a source sentence into a fixed-length vector from
                   which a decoder generates a translation. In this paper, we
                   conjecture that the use of a fixed-length vector is a
                   bottleneck in improving the performance of this basic
                   encoder-decoder architecture, and propose to extend this by
                   allowing a model to automatically (soft-)search for parts of
                   a source sentence that are relevant to predicting a target
                   word, without having to form these parts as a hard segment
                   explicitly. With this new approach, we achieve a translation
                   performance comparable to the existing state-of-the-art
                   phrase-based system on the task of English-to-French
                   translation. Furthermore, qualitative analysis reveals that
                   the (soft-)alignments found by the model agree well with our
                   intuition.",
  month         =  sep,
  year          =  2014,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "1409.0473",
  primaryClass  = "cs.CL",
  arxivid       = "1409.0473"
}

@ARTICLE{Raffel2019-tp,
  title         = "Exploring the limits of transfer learning with a unified
                   text-to-text transformer",
  author        = "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee,
                   Katherine and Narang, Sharan and Matena, Michael and Zhou,
                   Yanqi and Li, Wei and Liu, Peter J",
  abstract      = "Transfer learning, where a model is first pre-trained on a
                   data-rich task before being fine-tuned on a downstream task,
                   has emerged as a powerful technique in natural language
                   processing (NLP). The effectiveness of transfer learning has
                   given rise to a diversity of approaches, methodology, and
                   practice. In this paper, we explore the landscape of
                   transfer learning techniques for NLP by introducing a
                   unified framework that converts all text-based language
                   problems into a text-to-text format. Our systematic study
                   compares pre-training objectives, architectures, unlabeled
                   data sets, transfer approaches, and other factors on dozens
                   of language understanding tasks. By combining the insights
                   from our exploration with scale and our new ``Colossal Clean
                   Crawled Corpus'', we achieve state-of-the-art results on
                   many benchmarks covering summarization, question answering,
                   text classification, and more. To facilitate future work on
                   transfer learning for NLP, we release our data set,
                   pre-trained models, and code.",
  month         =  oct,
  year          =  2019,
  keywords      = "Encoder-Decoder;In-Context",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "1910.10683",
  primaryClass  = "cs.LG",
  arxivid       = "1910.10683"
}

@ARTICLE{Vaswani2017-qw,
  title         = "Attention Is All You Need",
  author        = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                   Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                   Kaiser, Lukasz and Polosukhin, Illia",
  abstract      = "The dominant sequence transduction models are based on
                   complex recurrent or convolutional neural networks in an
                   encoder-decoder configuration. The best performing models
                   also connect the encoder and decoder through an attention
                   mechanism. We propose a new simple network architecture, the
                   Transformer, based solely on attention mechanisms,
                   dispensing with recurrence and convolutions entirely.
                   Experiments on two machine translation tasks show these
                   models to be superior in quality while being more
                   parallelizable and requiring significantly less time to
                   train. Our model achieves 28.4 BLEU on the WMT 2014
                   English-to-German translation task, improving over the
                   existing best results, including ensembles by over 2 BLEU.
                   On the WMT 2014 English-to-French translation task, our
                   model establishes a new single-model state-of-the-art BLEU
                   score of 41.8 after training for 3.5 days on eight GPUs, a
                   small fraction of the training costs of the best models from
                   the literature. We show that the Transformer generalizes
                   well to other tasks by applying it successfully to English
                   constituency parsing both with large and limited training
                   data.",
  month         =  jun,
  year          =  2017,
  keywords      = "Fundamental Papers;Highly cited;Models;Machine
                   Translation;LLMs",
  archivePrefix = "arXiv",
  eprint        = "1706.03762",
  primaryClass  = "cs.CL",
  arxivid       = "1706.03762"
}

@ARTICLE{Brown2020-sc,
  title         = "Language Models are {Few-Shot} Learners",
  author        = "Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah,
                   Melanie and Kaplan, Jared and Dhariwal, Prafulla and
                   Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and
                   Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel
                   and Krueger, Gretchen and Henighan, Tom and Child, Rewon and
                   Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and
                   Winter, Clemens and Hesse, Christopher and Chen, Mark and
                   Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess,
                   Benjamin and Clark, Jack and Berner, Christopher and
                   McCandlish, Sam and Radford, Alec and Sutskever, Ilya and
                   Amodei, Dario",
  abstract      = "Recent work has demonstrated substantial gains on many NLP
                   tasks and benchmarks by pre-training on a large corpus of
                   text followed by fine-tuning on a specific task. While
                   typically task-agnostic in architecture, this method still
                   requires task-specific fine-tuning datasets of thousands or
                   tens of thousands of examples. By contrast, humans can
                   generally perform a new language task from only a few
                   examples or from simple instructions - something which
                   current NLP systems still largely struggle to do. Here we
                   show that scaling up language models greatly improves
                   task-agnostic, few-shot performance, sometimes even reaching
                   competitiveness with prior state-of-the-art fine-tuning
                   approaches. Specifically, we train GPT-3, an autoregressive
                   language model with 175 billion parameters, 10x more than
                   any previous non-sparse language model, and test its
                   performance in the few-shot setting. For all tasks, GPT-3 is
                   applied without any gradient updates or fine-tuning, with
                   tasks and few-shot demonstrations specified purely via text
                   interaction with the model. GPT-3 achieves strong
                   performance on many NLP datasets, including translation,
                   question-answering, and cloze tasks, as well as several
                   tasks that require on-the-fly reasoning or domain
                   adaptation, such as unscrambling words, using a novel word
                   in a sentence, or performing 3-digit arithmetic. At the same
                   time, we also identify some datasets where GPT-3's few-shot
                   learning still struggles, as well as some datasets where
                   GPT-3 faces methodological issues related to training on
                   large web corpora. Finally, we find that GPT-3 can generate
                   samples of news articles which human evaluators have
                   difficulty distinguishing from articles written by humans.
                   We discuss broader societal impacts of this finding and of
                   GPT-3 in general.",
  month         =  may,
  year          =  2020,
  keywords      = "Readling List;Models;NLP;Language;Decorder only",
  archivePrefix = "arXiv",
  eprint        = "2005.14165",
  primaryClass  = "cs.CL",
  arxivid       = "2005.14165"
}

@ARTICLE{Ouyang2022-lb,
  title         = "Training language models to follow instructions with human
                   feedback",
  author        = "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo
                   and Wainwright, Carroll L and Mishkin, Pamela and Zhang,
                   Chong and Agarwal, Sandhini and Slama, Katarina and Ray,
                   Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser
                   and Miller, Luke and Simens, Maddie and Askell, Amanda and
                   Welinder, Peter and Christiano, Paul and Leike, Jan and
                   Lowe, Ryan",
  editor        = "Koyejo, S and Mohamed, S and Agarwal, A and Belgrave, D and
                   Cho, K and Oh, A",
  abstract      = "Making language models bigger does not inherently make them
                   better at following a user's intent. For example, large
                   language models can generate outputs that are untruthful,
                   toxic, or simply not helpful to the user. In other words,
                   these models are not aligned with their users. In this
                   paper, we show an avenue for aligning language models with
                   user intent on a wide range of tasks by fine-tuning with
                   human feedback. Starting with a set of labeler-written
                   prompts and prompts submitted through the OpenAI API, we
                   collect a dataset of labeler demonstrations of the desired
                   model behavior, which we use to fine-tune GPT-3 using
                   supervised learning. We then collect a dataset of rankings
                   of model outputs, which we use to further fine-tune this
                   supervised model using reinforcement learning from human
                   feedback. We call the resulting models InstructGPT. In human
                   evaluations on our prompt distribution, outputs from the
                   1.3B parameter InstructGPT model are preferred to outputs
                   from the 175B GPT-3, despite having 100x fewer parameters.
                   Moreover, InstructGPT models show improvements in
                   truthfulness and reductions in toxic output generation while
                   having minimal performance regressions on public NLP
                   datasets. Even though InstructGPT still makes simple
                   mistakes, our results show that fine-tuning with human
                   feedback is a promising direction for aligning language
                   models with human intent.",
  pages         = "27730--27744",
  month         =  mar,
  year          =  2022,
  keywords      = "Alignment (RLHF, etc)",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2203.02155",
  primaryClass  = "cs.CL",
  arxivid       = "2203.02155"
}

@ARTICLE{Touvron2023-to,
  title         = "Llama 2: Open Foundation and {Fine-Tuned} Chat Models",
  author        = "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert,
                   Peter and Almahairi, Amjad and Babaei, Yasmine and
                   Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal
                   and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and
                   Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem
                   and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu,
                   Wenyin and Fuller, Brian and Gao, Cynthia and Goswami,
                   Vedanuj and Goyal, Naman and Hartshorn, Anthony and
                   Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas,
                   Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann,
                   Isabel and Korenev, Artem and Koura, Punit Singh and
                   Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and
                   Liskovich, Diana and Lu, Yinghai and Mao, Yuning and
                   Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and
                   Molybog, Igor and Nie, Yixin and Poulton, Andrew and
                   Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and
                   Schelten, Alan and Silva, Ruan and Smith, Eric Michael and
                   Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh
                   and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang
                   and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang,
                   Yuchen and Fan, Angela and Kambadur, Melanie and Narang,
                   Sharan and Rodriguez, Aurelien and Stojnic, Robert and
                   Edunov, Sergey and Scialom, Thomas",
  abstract      = "In this work, we develop and release Llama 2, a collection
                   of pretrained and fine-tuned large language models (LLMs)
                   ranging in scale from 7 billion to 70 billion parameters.
                   Our fine-tuned LLMs, called Llama 2-Chat, are optimized for
                   dialogue use cases. Our models outperform open-source chat
                   models on most benchmarks we tested, and based on our human
                   evaluations for helpfulness and safety, may be a suitable
                   substitute for closed-source models. We provide a detailed
                   description of our approach to fine-tuning and safety
                   improvements of Llama 2-Chat in order to enable the
                   community to build on our work and contribute to the
                   responsible development of LLMs.",
  month         =  jul,
  year          =  2023,
  keywords      = "LLMs;Models",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2307.09288",
  primaryClass  = "cs.CL",
  arxivid       = "2307.09288"
}

@ARTICLE{Christiano2017-or,
  title         = "Deep reinforcement learning from human preferences",
  author        = "Christiano, Paul and Leike, Jan and Brown, Tom B and Martic,
                   Miljan and Legg, Shane and Amodei, Dario",
  abstract      = "For sophisticated reinforcement learning (RL) systems to
                   interact usefully with real-world environments, we need to
                   communicate complex goals to these systems. In this work, we
                   explore goals defined in terms of (non-expert) human
                   preferences between pairs of trajectory segments. We show
                   that this approach can effectively solve complex RL tasks
                   without access to the reward function, including Atari games
                   and simulated robot locomotion, while providing feedback on
                   less than one percent of our agent's interactions with the
                   environment. This reduces the cost of human oversight far
                   enough that it can be practically applied to
                   state-of-the-art RL systems. To demonstrate the flexibility
                   of our approach, we show that we can successfully train
                   complex novel behaviors with about an hour of human time.
                   These behaviors and environments are considerably more
                   complex than any that have been previously learned from
                   human feedback.",
  month         =  jun,
  year          =  2017,
  keywords      = "Alignment (RLHF, etc);LLMs;Reinforcement Learning",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "1706.03741",
  primaryClass  = "stat.ML",
  arxivid       = "1706.03741",
  doi           = "10.48550/ARXIV.1706.03741"
}

@ARTICLE{Sharma2015-ox,
  title         = "Action Recognition using Visual Attention",
  author        = "Sharma, Shikhar and Kiros, Ryan and Salakhutdinov, Ruslan",
  abstract      = "We propose a soft attention based model for the task of
                   action recognition in videos. We use multi-layered Recurrent
                   Neural Networks (RNNs) with Long Short-Term Memory (LSTM)
                   units which are deep both spatially and temporally. Our
                   model learns to focus selectively on parts of the video
                   frames and classifies videos after taking a few glimpses.
                   The model essentially learns which parts in the frames are
                   relevant for the task at hand and attaches higher importance
                   to them. We evaluate the model on UCF-11 (YouTube Action),
                   HMDB-51 and Hollywood2 datasets and analyze how the model
                   focuses its attention depending on the scene and the action
                   being performed.",
  month         =  nov,
  year          =  2015,
  keywords      = "Public Code;Models;CNN - Classification;Spatio-Temporal Data
                   ;Detection/Recognition",
  archivePrefix = "arXiv",
  eprint        = "1511.04119",
  primaryClass  = "cs.LG",
  arxivid       = "1511.04119"
}

@ARTICLE{noauthor_undated-yw,
  title         = "{BERT}: Pre-training of Deep Bidirectional Transformers for
                   Language Understanding",
  author        = "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and
                   Toutanova, Kristina",
  abstract      = "We introduce a new language representation model called
                   BERT, which stands for Bidirectional Encoder Representations
                   from Transformers. Unlike recent language representation
                   models, BERT is designed to pre-train deep bidirectional
                   representations by jointly conditioning on both left and
                   right context in all layers. As a result, the pre-trained
                   BERT representations can be fine-tuned with just one
                   additional output layer to create state-of-the-art models
                   for a wide range of tasks, such as question answering and
                   language inference, without substantial task-specific
                   architecture modifications. BERT is conceptually simple and
                   empirically powerful. It obtains new state-of-the-art
                   results on eleven natural language processing tasks,
                   including pushing the GLUE benchmark to 80.4\% (7.6\%
                   absolute improvement), MultiNLI accuracy to 86.7 (5.6\%
                   absolute improvement) and the SQuAD v1.1 question answering
                   Test F1 to 93.2 (1.5\% absolute improvement), outperforming
                   human performance by 2.0\%.",
  journal       = "arXiv [cs.CL]",
  month         =  oct,
  year          =  2018,
  keywords      = "HasNotes;Language;LLMs",
  archivePrefix = "arXiv",
  eprint        = "1810.04805",
  primaryClass  = "cs.CL",
  arxivid       = "1810.04805"
}

@MISC{noauthor_undated-ma,
  title        = "Alpaca: A Strong, Replicable {Instruction-Following} Model",
  howpublished = "\url{https://crfm.stanford.edu/2023/03/13/alpaca.html}",
  note         = "Accessed: 2023-7-26",
  keywords     = "Public Code;Fine-tuning",
  language     = "en"
}

@ARTICLE{Hendrycks2020-te,
  title         = "Measuring Massive Multitask Language Understanding",
  author        = "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou,
                   Andy and Mazeika, Mantas and Song, Dawn and Steinhardt,
                   Jacob",
  abstract      = "We propose a new test to measure a text model's multitask
                   accuracy. The test covers 57 tasks including elementary
                   mathematics, US history, computer science, law, and more. To
                   attain high accuracy on this test, models must possess
                   extensive world knowledge and problem solving ability. We
                   find that while most recent models have near random-chance
                   accuracy, the very largest GPT-3 model improves over random
                   chance by almost 20 percentage points on average. However,
                   on every one of the 57 tasks, the best models still need
                   substantial improvements before they can reach expert-level
                   accuracy. Models also have lopsided performance and
                   frequently do not know when they are wrong. Worse, they
                   still have near-random accuracy on some socially important
                   subjects such as morality and law. By comprehensively
                   evaluating the breadth and depth of a model's academic and
                   professional understanding, our test can be used to analyze
                   models across many tasks and to identify important
                   shortcomings.",
  month         =  sep,
  year          =  2020,
  keywords      = "Public Code;Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2009.03300",
  primaryClass  = "cs.CY",
  arxivid       = "2009.03300"
}

@ARTICLE{Santurkar2023-zy,
  title         = "Whose opinions do language models reflect?",
  author        = "Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and
                   Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori",
  abstract      = "Language models (LMs) are increasingly being used in
                   open-ended contexts, where the opinions reflected by LMs in
                   response to subjective queries can have a profound impact,
                   both on user satisfaction, as well as shaping the views of
                   society at large. In this work, we put forth a quantitative
                   framework to investigate the opinions reflected by LMs -- by
                   leveraging high-quality public opinion polls and their
                   associated human responses. Using this framework, we create
                   OpinionsQA, a new dataset for evaluating the alignment of LM
                   opinions with those of 60 US demographic groups over topics
                   ranging from abortion to automation. Across topics, we find
                   substantial misalignment between the views reflected by
                   current LMs and those of US demographic groups: on par with
                   the Democrat-Republican divide on climate change. Notably,
                   this misalignment persists even after explicitly steering
                   the LMs towards particular demographic groups. Our analysis
                   not only confirms prior observations about the left-leaning
                   tendencies of some human feedback-tuned LMs, but also
                   surfaces groups whose opinions are poorly reflected by
                   current LMs (e.g., 65+ and widowed individuals). Our code
                   and data are available at
                   https://github.com/tatsu-lab/opinions\_qa.",
  month         =  mar,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2303.17548",
  primaryClass  = "cs.CL",
  arxivid       = "2303.17548"
}

@ARTICLE{Kim2023-gq,
  title         = "Full stack optimization of Transformer inference: A survey",
  author        = "Kim, Sehoon and Hooper, Coleman and Wattanawong, Thanakul
                   and Kang, Minwoo and Yan, Ruohan and Genc, Hasan and Dinh,
                   Grace and Huang, Qijing and Keutzer, Kurt and Mahoney,
                   Michael W and Shao, Yakun Sophia and Gholami, Amir",
  abstract      = "Recent advances in state-of-the-art DNN architecture design
                   have been moving toward Transformer models. These models
                   achieve superior accuracy across a wide range of
                   applications. This trend has been consistent over the past
                   several years since Transformer models were originally
                   introduced. However, the amount of compute and bandwidth
                   required for inference of recent Transformer models is
                   growing at a significant rate, and this has made their
                   deployment in latency-sensitive applications challenging. As
                   such, there has been an increased focus on making
                   Transformer models more efficient, with methods that range
                   from changing the architecture design, all the way to
                   developing dedicated domain-specific accelerators. In this
                   work, we survey different approaches for efficient
                   Transformer inference, including: (i) analysis and profiling
                   of the bottlenecks in existing Transformer architectures and
                   their similarities and differences with previous
                   convolutional models; (ii) implications of Transformer
                   architecture on hardware, including the impact of non-linear
                   operations such as Layer Normalization, Softmax, and GELU,
                   as well as linear operations, on hardware design; (iii)
                   approaches for optimizing a fixed Transformer architecture;
                   (iv) challenges in finding the right mapping and scheduling
                   of operations for Transformer models; and (v) approaches for
                   optimizing Transformer models by adapting the architecture
                   using neural architecture search. Finally, we perform a case
                   study by applying the surveyed optimizations on Gemmini, the
                   open-source, full-stack DNN accelerator generator, and we
                   show how each of these approaches can yield improvements,
                   compared to previous benchmark results on Gemmini. Among
                   other things, we find that a full-stack co-design approach
                   with the aforementioned methods can result in up to 88.7x
                   speedup with a minimal performance degradation for
                   Transformer inference.",
  month         =  feb,
  year          =  2023,
  keywords      = "Efficient",
  archivePrefix = "arXiv",
  eprint        = "2302.14017",
  primaryClass  = "cs.CL",
  arxivid       = "2302.14017"
}

@ARTICLE{Shi2022-wz,
  title         = "Language Models are Multilingual {Chain-of-Thought}
                   Reasoners",
  author        = "Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang,
                   Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung,
                   Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny
                   and Das, Dipanjan and Wei, Jason",
  abstract      = "We evaluate the reasoning abilities of large language models
                   in multilingual settings. We introduce the Multilingual
                   Grade School Math (MGSM) benchmark, by manually translating
                   250 grade-school math problems from the GSM8K dataset (Cobbe
                   et al., 2021) into ten typologically diverse languages. We
                   find that the ability to solve MGSM problems via
                   chain-of-thought prompting emerges with increasing model
                   scale, and that models have strikingly strong multilingual
                   reasoning abilities, even in underrepresented languages such
                   as Bengali and Swahili. Finally, we show that the
                   multilingual reasoning abilities of language models extend
                   to other tasks such as commonsense reasoning and
                   word-in-context semantic judgment. The MGSM benchmark is
                   publicly available at
                   https://github.com/google-research/url-nlp.",
  month         =  oct,
  year          =  2022,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2210.03057",
  primaryClass  = "cs.CL",
  arxivid       = "2210.03057"
}

@ARTICLE{Li2023-vb,
  title         = "{Self-Alignment} with Instruction Backtranslation",
  author        = "Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo
                   and Zettlemoyer, Luke and Levy, Omer and Weston, Jason and
                   Lewis, Mike",
  abstract      = "We present a scalable method to build a high quality
                   instruction following language model by automatically
                   labelling human-written text with corresponding
                   instructions. Our approach, named instruction
                   backtranslation, starts with a language model finetuned on a
                   small amount of seed data, and a given web corpus. The seed
                   model is used to construct training examples by generating
                   instruction prompts for web documents (self-augmentation),
                   and then selecting high quality examples from among these
                   candidates (self-curation). This data is then used to
                   finetune a stronger model. Finetuning LLaMa on two
                   iterations of our approach yields a model that outperforms
                   all other LLaMa-based models on the Alpaca leaderboard not
                   relying on distillation data, demonstrating highly effective
                   self-alignment.",
  month         =  aug,
  year          =  2023,
  keywords      = "Fine-tuning;LLMs",
  archivePrefix = "arXiv",
  eprint        = "2308.06259",
  primaryClass  = "cs.CL",
  arxivid       = "2308.06259"
}

@MISC{noauthor_2022-sl,
  title        = "How does in-context learning work? A framework for
                  understanding the differences from traditional supervised
                  learning",
  booktitle    = "{SAIL} Blog",
  abstract     = "The official Stanford AI Lab blog",
  month        =  aug,
  year         =  2022,
  howpublished = "\url{https://ai.stanford.edu/blog/understanding-incontext/}",
  note         = "Accessed: 2023-8-17",
  keywords     = "In-Context",
  language     = "en"
}

@ARTICLE{Mialon2023-sm,
  title         = "{GAIA}: a benchmark for General {AI} Assistants",
  author        = "Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Swift,
                   Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas",
  abstract      = "We introduce GAIA, a benchmark for General AI Assistants
                   that, if solved, would represent a milestone in AI research.
                   GAIA proposes real-world questions that require a set of
                   fundamental abilities such as reasoning, multi-modality
                   handling, web browsing, and generally tool-use proficiency.
                   GAIA questions are conceptually simple for humans yet
                   challenging for most advanced AIs: we show that human
                   respondents obtain 92\% vs. 15\% for GPT-4 equipped with
                   plugins. This notable performance disparity contrasts with
                   the recent trend of LLMs outperforming humans on tasks
                   requiring professional skills in e.g. law or chemistry.
                   GAIA's philosophy departs from the current trend in AI
                   benchmarks suggesting to target tasks that are ever more
                   difficult for humans. We posit that the advent of Artificial
                   General Intelligence (AGI) hinges on a system's capability
                   to exhibit similar robustness as the average human does on
                   such questions. Using GAIA's methodology, we devise 466
                   questions and their answer. We release our questions while
                   retaining answers to 300 of them to power a leader-board
                   available at https://huggingface.co/gaia-benchmark.",
  month         =  nov,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2311.12983",
  primaryClass  = "cs.CL",
  arxivid       = "2311.12983"
}

@ARTICLE{He2023-cv,
  title         = "Simplifying Transformer Blocks",
  author        = "He, Bobby and Hofmann, Thomas",
  abstract      = "A simple design recipe for deep Transformers is to compose
                   identical building blocks. But standard transformer blocks
                   are far from simple, interweaving attention and MLP
                   sub-blocks with skip connections \& normalisation layers in
                   precise arrangements. This complexity leads to brittle
                   architectures, where seemingly minor changes can
                   significantly reduce training speed, or render models
                   untrainable. In this work, we ask to what extent the
                   standard transformer block can be simplified? Combining
                   signal propagation theory and empirical observations, we
                   motivate modifications that allow many block components to
                   be removed with no loss of training speed, including skip
                   connections, projection or value parameters, sequential
                   sub-blocks and normalisation layers. In experiments on both
                   autoregressive decoder-only and BERT encoder-only models,
                   our simplified transformers emulate the per-update training
                   speed and performance of standard transformers, while
                   enjoying 15\% faster training throughput, and using 15\%
                   fewer parameters.",
  month         =  nov,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2311.01906",
  primaryClass  = "cs.LG",
  arxivid       = "2311.01906"
}

@ARTICLE{Solaiman2023-sr,
  title         = "Evaluating the social impact of generative {AI} systems in
                   systems and society",
  author        = "Solaiman, Irene and Talat, Zeerak and Agnew, William and
                   Ahmad, Lama and Baker, Dylan and Blodgett, Su Lin and
                   Daum{\'e}, III, Hal and Dodge, Jesse and Evans, Ellie and
                   Hooker, Sara and Jernite, Yacine and Luccioni, Alexandra
                   Sasha and Lusoli, Alberto and Mitchell, Margaret and Newman,
                   Jessica and Png, Marie-Therese and Strait, Andrew",
  abstract      = "Generative AI systems across modalities, ranging from text,
                   image, audio, and video, have broad social impacts, but
                   there exists no official standard for means of evaluating
                   those impacts and which impacts should be evaluated. We move
                   toward a standard approach in evaluating a generative AI
                   system for any modality, in two overarching categories: what
                   is able to be evaluated in a base system that has no
                   predetermined application and what is able to be evaluated
                   in society. We describe specific social impact categories
                   and how to approach and conduct evaluations in the base
                   technical system, then in people and society. Our framework
                   for a base system defines seven categories of social impact:
                   bias, stereotypes, and representational harms; cultural
                   values and sensitive content; disparate performance; privacy
                   and data protection; financial costs; environmental costs;
                   and data and content moderation labor costs. Suggested
                   methods for evaluation apply to all modalities and analyses
                   of the limitations of existing evaluations serve as a
                   starting point for necessary investment in future
                   evaluations. We offer five overarching categories for what
                   is able to be evaluated in society, each with their own
                   subcategories: trustworthiness and autonomy; inequality,
                   marginalization, and violence; concentration of authority;
                   labor and creativity; and ecosystem and environment. Each
                   subcategory includes recommendations for mitigating harm. We
                   are concurrently crafting an evaluation repository for the
                   AI research community to contribute existing evaluations
                   along the given categories. This version will be updated
                   following a CRAFT session at ACM FAccT 2023.",
  month         =  jun,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2306.05949",
  primaryClass  = "cs.CY",
  arxivid       = "2306.05949"
}

@ARTICLE{Shen2023-tw,
  title         = "Positional description matters for transformers arithmetic",
  author        = "Shen, Ruoqi and Bubeck, S{\'e}bastien and Eldan, Ronen and
                   Lee, Yin Tat and Li, Yuanzhi and Zhang, Yi",
  abstract      = "Transformers, central to the successes in modern Natural
                   Language Processing, often falter on arithmetic tasks
                   despite their vast capabilities --which paradoxically
                   include remarkable coding abilities. We observe that a
                   crucial challenge is their naive reliance on positional
                   information to solve arithmetic problems with a small number
                   of digits, leading to poor performance on larger numbers.
                   Herein, we delve deeper into the role of positional
                   encoding, and propose several ways to fix the issue, either
                   by modifying the positional encoding directly, or by
                   modifying the representation of the arithmetic task to
                   leverage standard positional encoding differently. We
                   investigate the value of these modifications for three
                   tasks: (i) classical multiplication, (ii) length
                   extrapolation in addition, and (iii) addition in natural
                   language context. For (i) we train a small model on a small
                   dataset (100M parameters and 300k samples) with remarkable
                   aptitude in (direct, no scratchpad) 15 digits multiplication
                   and essentially perfect up to 12 digits, while usual
                   training in this context would give a model failing at 4
                   digits multiplication. In the experiments on addition, we
                   use a mere 120k samples to demonstrate: for (ii)
                   extrapolation from 10 digits to testing on 12 digits numbers
                   while usual training would have no extrapolation, and for
                   (iii) almost perfect accuracy up to 5 digits while usual
                   training would be correct only up to 3 digits (which is
                   essentially memorization with a training set of 120k
                   samples).",
  month         =  nov,
  year          =  2023,
  keywords      = "Applications",
  archivePrefix = "arXiv",
  eprint        = "2311.14737",
  primaryClass  = "cs.CL",
  arxivid       = "2311.14737"
}

@ARTICLE{Chen2023-qi,
  title         = "Accelerating large language model decoding with speculative
                   sampling",
  author        = "Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey
                   and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper,
                   John",
  abstract      = "We present speculative sampling, an algorithm for
                   accelerating transformer decoding by enabling the generation
                   of multiple tokens from each transformer call. Our algorithm
                   relies on the observation that the latency of parallel
                   scoring of short continuations, generated by a faster but
                   less powerful draft model, is comparable to that of sampling
                   a single token from the larger target model. This is
                   combined with a novel modified rejection sampling scheme
                   which preserves the distribution of the target model within
                   hardware numerics. We benchmark speculative sampling with
                   Chinchilla, a 70 billion parameter language model, achieving
                   a 2-2.5x decoding speedup in a distributed setup, without
                   compromising the sample quality or making modifications to
                   the model itself.",
  month         =  feb,
  year          =  2023,
  keywords      = "Efficient",
  archivePrefix = "arXiv",
  eprint        = "2302.01318",
  primaryClass  = "cs.CL",
  arxivid       = "2302.01318"
}

@ARTICLE{Stern2018-rb,
  title         = "Blockwise parallel decoding for deep autoregressive models",
  author        = "Stern, Mitchell and Shazeer, Noam and Uszkoreit, Jakob",
  abstract      = "Deep autoregressive sequence-to-sequence models have
                   demonstrated impressive performance across a wide variety of
                   tasks in recent years. While common architecture classes
                   such as recurrent, convolutional, and self-attention
                   networks make different trade-offs between the amount of
                   computation needed per layer and the length of the critical
                   path at training time, generation still remains an
                   inherently sequential process. To overcome this limitation,
                   we propose a novel blockwise parallel decoding scheme in
                   which we make predictions for multiple time steps in
                   parallel then back off to the longest prefix validated by a
                   scoring model. This allows for substantial theoretical
                   improvements in generation speed when applied to
                   architectures that can process output sequences in parallel.
                   We verify our approach empirically through a series of
                   experiments using state-of-the-art self-attention models for
                   machine translation and image super-resolution, achieving
                   iteration reductions of up to 2x over a baseline greedy
                   decoder with no loss in quality, or up to 7x in exchange for
                   a slight decrease in performance. In terms of wall-clock
                   time, our fastest models exhibit real-time speedups of up to
                   4x over standard greedy decoding.",
  month         =  nov,
  year          =  2018,
  keywords      = "Efficient",
  archivePrefix = "arXiv",
  eprint        = "1811.03115",
  primaryClass  = "cs.LG",
  arxivid       = "1811.03115"
}

@ARTICLE{Leviathan2022-cz,
  title         = "Fast inference from Transformers via speculative decoding",
  author        = "Leviathan, Yaniv and Kalman, Matan and Matias, Yossi",
  abstract      = "Inference from large autoregressive models like Transformers
                   is slow - decoding K tokens takes K serial runs of the
                   model. In this work we introduce speculative decoding - an
                   algorithm to sample from autoregressive models faster
                   without any changes to the outputs, by computing several
                   tokens in parallel. At the heart of our approach lie the
                   observations that (1) hard language-modeling tasks often
                   include easier subtasks that can be approximated well by
                   more efficient models, and (2) using speculative execution
                   and a novel sampling method, we can make exact decoding from
                   the large models faster, by running them in parallel on the
                   outputs of the approximation models, potentially generating
                   several tokens concurrently, and without changing the
                   distribution. Our method can accelerate existing
                   off-the-shelf models without retraining or architecture
                   changes. We demonstrate it on T5-XXL and show a 2X-3X
                   acceleration compared to the standard T5X implementation,
                   with identical outputs.",
  month         =  nov,
  year          =  2022,
  keywords      = "Efficient",
  archivePrefix = "arXiv",
  eprint        = "2211.17192",
  primaryClass  = "cs.LG",
  arxivid       = "2211.17192"
}

@ARTICLE{Zhang2023-js,
  title         = "Instruction tuning for large language models: A survey",
  author        = "Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang,
                   Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu,
                   Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin",
  abstract      = "This paper surveys research works in the quickly advancing
                   field of instruction tuning (IT), a crucial technique to
                   enhance the capabilities and controllability of large
                   language models (LLMs). Instruction tuning refers to the
                   process of further training LLMs on a dataset consisting of
                   \textbackslashtextsc\{(instruction, output)\} pairs in a
                   supervised fashion, which bridges the gap between the
                   next-word prediction objective of LLMs and the users'
                   objective of having LLMs adhere to human instructions. In
                   this work, we make a systematic review of the literature,
                   including the general methodology of IT, the construction of
                   IT datasets, the training of IT models, and applications to
                   different modalities, domains and applications, along with
                   an analysis on aspects that influence the outcome of IT
                   (e.g., generation of instruction outputs, size of the
                   instruction dataset, etc). We also review the potential
                   pitfalls of IT along with criticism against it, along with
                   efforts pointing out current deficiencies of existing
                   strategies and suggest some avenues for fruitful research.",
  month         =  aug,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  eprint        = "2308.10792",
  primaryClass  = "cs.CL",
  arxivid       = "2308.10792"
}

@ARTICLE{Agrawal2023-jn,
  title         = "{SARATHI}: Efficient {LLM} Inference by Piggybacking Decodes
                   with Chunked Prefills",
  author        = "Agrawal, Amey and Panwar, Ashish and Mohan, Jayashree and
                   Kwatra, Nipun and Gulavani, Bhargav S and Ramjee,
                   Ramachandran",
  abstract      = "Large Language Model (LLM) inference consists of two
                   distinct phases - prefill phase which processes the input
                   prompt and decode phase which generates output tokens
                   autoregressively. While the prefill phase effectively
                   saturates GPU compute at small batch sizes, the decode phase
                   results in low compute utilization as it generates one token
                   at a time per request. The varying prefill and decode times
                   also lead to imbalance across micro-batches when using
                   pipeline parallelism, resulting in further inefficiency due
                   to bubbles. We present SARATHI to address these challenges.
                   SARATHI employs chunked-prefills, which splits a prefill
                   request into equal sized chunks, and decode-maximal
                   batching, which constructs a batch using a single prefill
                   chunk and populates the remaining slots with decodes. During
                   inference, the prefill chunk saturates GPU compute, while
                   the decode requests 'piggyback' and cost up to an order of
                   magnitude less compared to a decode-only batch.
                   Chunked-prefills allows constructing multiple decode-maximal
                   batches from a single prefill request, maximizing coverage
                   of decodes that can piggyback. Furthermore, the uniform
                   compute design of these batches ameliorates the imbalance
                   between micro-batches, significantly reducing pipeline
                   bubbles. Our techniques yield significant improvements in
                   inference performance across models and hardware. For the
                   LLaMA-13B model on A6000 GPU, SARATHI improves decode
                   throughput by up to 10x, and accelerates end-to-end
                   throughput by up to 1.33x. For LLaMa-33B on A100 GPU, we
                   achieve 1.25x higher end-to-end-throughput and up to 4.25x
                   higher decode throughput. When used with pipeline
                   parallelism on GPT-3, SARATHI reduces bubbles by 6.29x,
                   resulting in an end-to-end throughput improvement of 1.91x.",
  month         =  aug,
  year          =  2023,
  keywords      = "Efficient",
  archivePrefix = "arXiv",
  eprint        = "2308.16369",
  primaryClass  = "cs.LG",
  arxivid       = "2308.16369"
}

@ARTICLE{Tarzanagh2023-li,
  title         = "Transformers as Support Vector Machines",
  author        = "Tarzanagh, Davoud Ataee and Li, Yingcong and Thrampoulidis,
                   Christos and Oymak, Samet",
  abstract      = "Since its inception in ``Attention Is All You Need'',
                   transformer architecture has led to revolutionary
                   advancements in NLP. The attention layer within the
                   transformer admits a sequence of input tokens $X$ and makes
                   them interact through pairwise similarities computed as
                   softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable
                   key-query parameters. In this work, we establish a formal
                   equivalence between the optimization geometry of
                   self-attention and a hard-margin SVM problem that separates
                   optimal input tokens from non-optimal tokens using linear
                   constraints on the outer-products of token pairs. This
                   formalism allows us to characterize the implicit bias of
                   1-layer transformers optimized with gradient descent: (1)
                   Optimizing the attention layer with vanishing
                   regularization, parameterized by $(K,Q)$, converges in
                   direction to an SVM solution minimizing the nuclear norm of
                   the combined parameter $W=KQ^\top$. Instead, directly
                   parameterizing by $W$ minimizes a Frobenius norm objective.
                   We characterize this convergence, highlighting that it can
                   occur toward locally-optimal directions rather than global
                   ones. (2) Complementing this, we prove the local/global
                   directional convergence of gradient descent under suitable
                   geometric conditions. Importantly, we show that
                   over-parameterization catalyzes global convergence by
                   ensuring the feasibility of the SVM problem and by
                   guaranteeing a benign optimization landscape devoid of
                   stationary points. (3) While our theory applies primarily to
                   linear prediction heads, we propose a more general SVM
                   equivalence that predicts the implicit bias with nonlinear
                   heads. Our findings are applicable to arbitrary datasets and
                   their validity is verified via experiments. We also
                   introduce several open problems and research directions. We
                   believe these findings inspire the interpretation of
                   transformers as a hierarchy of SVMs that separates and
                   selects optimal tokens.",
  month         =  aug,
  year          =  2023,
  keywords      = "Intriguing Properties",
  archivePrefix = "arXiv",
  eprint        = "2308.16898",
  primaryClass  = "cs.LG",
  arxivid       = "2308.16898"
}

@ARTICLE{Su2021-ne,
  title         = "{RoFormer}: Enhanced Transformer with Rotary Position
                   Embedding",
  author        = "Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha,
                   Ahmed and Wen, Bo and Liu, Yunfeng",
  abstract      = "Position encoding recently has shown effective in the
                   transformer architecture. It enables valuable supervision
                   for dependency modeling between elements at different
                   positions of the sequence. In this paper, we first
                   investigate various methods to integrate positional
                   information into the learning process of transformer-based
                   language models. Then, we propose a novel method named
                   Rotary Position Embedding(RoPE) to effectively leverage the
                   positional information. Specifically, the proposed RoPE
                   encodes the absolute position with a rotation matrix and
                   meanwhile incorporates the explicit relative position
                   dependency in self-attention formulation. Notably, RoPE
                   enables valuable properties, including the flexibility of
                   sequence length, decaying inter-token dependency with
                   increasing relative distances, and the capability of
                   equipping the linear self-attention with relative position
                   encoding. Finally, we evaluate the enhanced transformer with
                   rotary position embedding, also called RoFormer, on various
                   long text classification benchmark datasets. Our experiments
                   show that it consistently overcomes its alternatives.
                   Furthermore, we provide a theoretical analysis to explain
                   some experimental results. RoFormer is already integrated
                   into Huggingface:
                   \textbackslashurl\{https://huggingface.co/docs/transformers/model\_doc/roformer\}.",
  month         =  apr,
  year          =  2021,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2104.09864",
  primaryClass  = "cs.CL",
  arxivid       = "2104.09864"
}

@ARTICLE{Bulatov2023-it,
  title         = "Scaling Transformer to {1M} tokens and beyond with {RMT}",
  author        = "Bulatov, Aydar and Kuratov, Yuri and Burtsev, Mikhail S",
  abstract      = "This technical report presents the application of a
                   recurrent memory to extend the context length of BERT, one
                   of the most effective Transformer-based models in natural
                   language processing. By leveraging the Recurrent Memory
                   Transformer architecture, we have successfully increased the
                   model's effective context length to an unprecedented two
                   million tokens, while maintaining high memory retrieval
                   accuracy. Our method allows for the storage and processing
                   of both local and global information and enables information
                   flow between segments of the input sequence through the use
                   of recurrence. Our experiments demonstrate the effectiveness
                   of our approach, which holds significant potential to
                   enhance long-term dependency handling in natural language
                   understanding and generation tasks as well as enable
                   large-scale context processing for memory-intensive
                   applications.",
  month         =  apr,
  year          =  2023,
  keywords      = "LLMs;Models",
  archivePrefix = "arXiv",
  eprint        = "2304.11062",
  primaryClass  = "cs.CL",
  arxivid       = "2304.11062"
}

@INPROCEEDINGS{Guo2022-dg,
  title      = "{LongT5}: Efficient text-to-text transformer for long sequences",
  booktitle  = "Findings of the Association for Computational Linguistics:
                {NAACL} 2022",
  author     = "Guo, Mandy and Ainslie, Joshua and Uthus, David and Ontanon,
                Santiago and Ni, Jianmo and Sung, Yun-Hsuan and Yang, Yinfei",
  abstract   = "Recent work has shown that either (1) increasing the input
                length or (2) increasing model size can improve the performance
                of Transformer-based neural models. In this paper, we present
                LongT5, a new model that explores the effects of scaling both
                the input length and model size at the same time. Specifically,
                we integrate attention ideas from long-input transformers
                (ETC), and adopt pre-training strategies from summarization
                pre-training (PEGASUS) into the scalable T5 architecture. The
                result is a new attention mechanism we call Transient Global
                (TGlobal), which mimics ETC's local/global attention mechanism,
                but without requiring additional side-inputs. We are able to
                achieve state-of-the-art results on several summarization and
                question answering tasks, as well as outperform the original T5
                models on these tasks. We have open sourced our architecture
                and training code, as well as our pre-trained model
                checkpoints.",
  publisher  = "Association for Computational Linguistics",
  pages      = "724--736",
  month      =  jul,
  year       =  2022,
  address    = "Stroudsburg, PA, USA",
  keywords   = "LLMs",
  conference = "Findings of the Association for Computational Linguistics:
                NAACL 2022",
  location   = "Seattle, United States",
  doi        = "10.18653/v1/2022.findings-naacl.55"
}

@ARTICLE{Singh2023-ps,
  title         = "Beyond human data: Scaling self-training for problem-solving
                   with language models",
  author        = "Singh, Avi and Co-Reyes, John D and Agarwal, Rishabh and
                   Anand, Ankesh and Patil, Piyush and Liu, Peter J and
                   Harrison, James and Lee, Jaehoon and Xu, Kelvin and Parisi,
                   Aaron and Kumar, Abhishek and Alemi, Alex and Rizkowsky,
                   Alex and Nova, Azade and Adlam, Ben and Bohnet, Bernd and
                   Sedghi, Hanie and Mordatch, Igor and Simpson, Isabelle and
                   Gur, Izzeddin and Snoek, Jasper and Pennington, Jeffrey and
                   Hron, Jiri and Kenealy, Kathleen and Swersky, Kevin and
                   Mahajan, Kshiteej and Culp, Laura and Xiao, Lechao and
                   Bileschi, Maxwell L and Constant, Noah and Novak, Roman and
                   Liu, Rosanne and Warkentin, Tris and Qian, Yundi and Dyer,
                   Ethan and Neyshabur, Behnam and Sohl-Dickstein, Jascha and
                   Fiedel, Noah",
  abstract      = "Fine-tuning language models~(LMs) on human-generated data
                   remains a prevalent practice. However, the performance of
                   such models is often limited by the quantity and diversity
                   of high-quality human data. In this paper, we explore
                   whether we can go beyond human data on tasks where we have
                   access to scalar feedback, for example, on math problems
                   where one can verify correctness. To do so, we investigate a
                   simple self-training method based on
                   expectation-maximization, which we call ReST$^\{EM\}$, where
                   we (1) generate samples from the model and filter them using
                   binary feedback, (2) fine-tune the model on these samples,
                   and (3) repeat this process a few times. Testing on advanced
                   MATH reasoning and APPS coding benchmarks using PaLM-2
                   models, we find that ReST$^\{EM\}$ scales favorably with
                   model size and significantly surpasses fine-tuning only on
                   human data. Overall, our findings suggest self-training with
                   feedback can substantially reduce dependence on
                   human-generated data.",
  month         =  dec,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2312.06585",
  primaryClass  = "cs.LG",
  arxivid       = "2312.06585"
}

@MISC{noauthor_undated-fw,
  title        = "{Seq2Seq} Tutorial by Oriol Vinyals",
  booktitle    = "Google Docs",
  abstract     = "Seq2Seq ICML Tutorial Oriol Vinyals and Navdeep Jaitly
                  @OriolVinyalsML | @NavdeepLearning Site:
                  https://sites.google.com/view/seq2seq-icml17 Sydney,
                  Australia, 2017",
  howpublished = "\url{https://docs.google.com/presentation/d/1OFbeCiBJnHiCaa4rVc3OSUkFyQGCqi7ll0VmEXuo8JU/edit}",
  note         = "Accessed: 2017-8-8",
  keywords     = "Survey/Review Paper;Machine Translation"
}

@ARTICLE{Shi2023-sr,
  title         = "Detecting pretraining data from large language models",
  author        = "Shi, Weijia and Ajith, Anirudh and Xia, Mengzhou and Huang,
                   Yangsibo and Liu, Daogao and Blevins, Terra and Chen, Danqi
                   and Zettlemoyer, Luke",
  abstract      = "Although large language models (LLMs) are widely deployed,
                   the data used to train them is rarely disclosed. Given the
                   incredible scale of this data, up to trillions of tokens, it
                   is all but certain that it includes potentially problematic
                   text such as copyrighted materials, personally identifiable
                   information, and test data for widely reported reference
                   benchmarks. However, we currently have no way to know which
                   data of these types is included or in what proportions. In
                   this paper, we study the pretraining data detection problem:
                   given a piece of text and black-box access to an LLM without
                   knowing the pretraining data, can we determine if the model
                   was trained on the provided text? To facilitate this study,
                   we introduce a dynamic benchmark WIKIMIA that uses data
                   created before and after model training to support gold
                   truth detection. We also introduce a new detection method
                   Min-K\% Prob based on a simple hypothesis: an unseen example
                   is likely to contain a few outlier words with low
                   probabilities under the LLM, while a seen example is less
                   likely to have words with such low probabilities. Min-K\%
                   Prob can be applied without any knowledge about the
                   pretraining corpus or any additional training, departing
                   from previous detection methods that require training a
                   reference model on data that is similar to the pretraining
                   data. Moreover, our experiments demonstrate that Min-K\%
                   Prob achieves a 7.4\% improvement on WIKIMIA over these
                   previous methods. We apply Min-K\% Prob to two real-world
                   scenarios, copyrighted book detection, and contaminated
                   downstream example detection, and find it a consistently
                   effective solution.",
  month         =  oct,
  year          =  2023,
  keywords      = "Value of a sample;LLMs",
  archivePrefix = "arXiv",
  eprint        = "2310.16789",
  primaryClass  = "cs.CL",
  arxivid       = "2310.16789"
}

@MISC{noauthor_undated-ie,
  howpublished = "\url{https://arxiv.org/abs/2309.16588}",
  note         = "Accessed: 2023-10-3",
  keywords     = "Vision;Multimodal (Vision, speech, etc);LLMs"
}

@ARTICLE{Mukkavilli2023-ve,
  title         = "{AI} foundation models for weather and climate:
                   Applications, design, and implementation",
  author        = "Mukkavilli, S Karthik and Civitarese, Daniel Salles and
                   Schmude, Johannes and Jakubik, Johannes and Jones, Anne and
                   Nguyen, Nam and Phillips, Christopher and Roy, Sujit and
                   Singh, Shraddha and Watson, Campbell and Ganti, Raghu and
                   Hamann, Hendrik and Nair, Udaysankar and Ramachandran, Rahul
                   and Weldemariam, Kommy",
  abstract      = "Machine learning and deep learning methods have been widely
                   explored in understanding the chaotic behavior of the
                   atmosphere and furthering weather forecasting. There has
                   been increasing interest from technology companies,
                   government institutions, and meteorological agencies in
                   building digital twins of the Earth. Recent approaches using
                   transformers, physics-informed machine learning, and graph
                   neural networks have demonstrated state-of-the-art
                   performance on relatively narrow spatiotemporal scales and
                   specific tasks. With the recent success of generative
                   artificial intelligence (AI) using pre-trained transformers
                   for language modeling and vision with prompt engineering and
                   fine-tuning, we are now moving towards generalizable AI. In
                   particular, we are witnessing the rise of AI foundation
                   models that can perform competitively on multiple
                   domain-specific downstream tasks. Despite this progress, we
                   are still in the nascent stages of a generalizable AI model
                   for global Earth system models, regional climate models, and
                   mesoscale weather models. Here, we review current
                   state-of-the-art AI approaches, primarily from transformer
                   and operator learning literature in the context of
                   meteorology. We provide our perspective on criteria for
                   success towards a family of foundation models for nowcasting
                   and forecasting weather and climate predictions. We also
                   discuss how such models can perform competitively on
                   downstream tasks such as downscaling (super-resolution),
                   identifying conditions conducive to the occurrence of
                   wildfires, and predicting consequential meteorological
                   phenomena across various spatiotemporal scales such as
                   hurricanes and atmospheric rivers. In particular, we examine
                   current AI methodologies and contend they have matured
                   enough to design and implement a weather foundation model.",
  month         =  sep,
  year          =  2023,
  keywords      = "Applications",
  archivePrefix = "arXiv",
  eprint        = "2309.10808",
  primaryClass  = "cs.LG",
  arxivid       = "2309.10808"
}

@ARTICLE{Azaria2023-rt,
  title         = "The internal state of an {LLM} knows when its lying",
  author        = "Azaria, Amos and Mitchell, Tom",
  abstract      = "While Large Language Models (LLMs) have shown exceptional
                   performance in various tasks, their (arguably) most
                   prominent drawback is generating inaccurate or false
                   information with a confident tone. In this paper, we
                   hypothesize that the LLM's internal state can be used to
                   reveal the truthfulness of a statement. Therefore, we
                   introduce a simple yet effective method to detect the
                   truthfulness of LLM-generated statements, which utilizes the
                   LLM's hidden layer activations to determine the veracity of
                   statements. To train and evaluate our method, we compose a
                   dataset of true and false statements in six different
                   topics. A classifier is trained to detect which statement is
                   true or false based on an LLM's activation values.
                   Specifically, the classifier receives as input the
                   activation values from the LLM for each of the statements in
                   the dataset. Our experiments demonstrate that our method for
                   detecting statement veracity significantly outperforms even
                   few-shot prompting methods, highlighting its potential to
                   enhance the reliability of LLM-generated content and its
                   practical applicability in real-world scenarios.",
  month         =  apr,
  year          =  2023,
  keywords      = "Hallucination",
  archivePrefix = "arXiv",
  eprint        = "2304.13734",
  primaryClass  = "cs.CL",
  arxivid       = "2304.13734"
}

@ARTICLE{Todd2023-jc,
  title         = "Function Vectors in Large Language Models",
  author        = "Todd, Eric and Li, Millicent L and Sharma, Arnab Sen and
                   Mueller, Aaron and Wallace, Byron C and Bau, David",
  abstract      = "We report the presence of a simple neural mechanism that
                   represents an input-output function as a vector within
                   autoregressive transformer language models (LMs). Using
                   causal mediation analysis on a diverse range of
                   in-context-learning (ICL) tasks, we find that a small number
                   attention heads transport a compact representation of the
                   demonstrated task, which we call a function vector (FV). FVs
                   are robust to changes in context, i.e., they trigger
                   execution of the task on inputs such as zero-shot and
                   natural text settings that do not resemble the ICL contexts
                   from which they are collected. We test FVs across a range of
                   tasks, models, and layers and find strong causal effects
                   across settings in middle layers. We investigate the
                   internal structure of FVs and find while that they often
                   contain information that encodes the output space of the
                   function, this information alone is not sufficient to
                   reconstruct an FV. Finally, we test semantic vector
                   composition in FVs, and find that to some extent they can be
                   summed to create vectors that trigger new complex tasks.
                   Taken together, our findings suggest that LLMs contain
                   internal abstractions of general-purpose functions that can
                   be invoked in a variety of contexts.",
  month         =  oct,
  year          =  2023,
  keywords      = "Intriguing Properties",
  archivePrefix = "arXiv",
  eprint        = "2310.15213",
  primaryClass  = "cs.CL",
  arxivid       = "2310.15213"
}

@ARTICLE{Jain2023-uj,
  title         = "Mechanistically analyzing the effects of fine-tuning on
                   procedurally defined tasks",
  author        = "Jain, Samyak and Kirk, Robert and Lubana, Ekdeep Singh and
                   Dick, Robert P and Tanaka, Hidenori and Grefenstette, Edward
                   and Rockt{\"a}schel, Tim and Krueger, David Scott",
  abstract      = "Fine-tuning large pre-trained models has become the de facto
                   strategy for developing both task-specific and
                   general-purpose machine learning systems, including
                   developing models that are safe to deploy. Despite its clear
                   importance, there has been minimal work that explains how
                   fine-tuning alters the underlying capabilities learned by a
                   model during pretraining: does fine-tuning yield entirely
                   novel capabilities or does it just modulate existing ones?
                   We address this question empirically in synthetic,
                   controlled settings where we can use mechanistic
                   interpretability tools (e.g., network pruning and probing)
                   to understand how the model's underlying capabilities are
                   changing. We perform an extensive analysis of the effects of
                   fine-tuning in these settings, and show that: (i)
                   fine-tuning rarely alters the underlying model capabilities;
                   (ii) a minimal transformation, which we call a 'wrapper', is
                   typically learned on top of the underlying model
                   capabilities, creating the illusion that they have been
                   modified; and (iii) further fine-tuning on a task where such
                   hidden capabilities are relevant leads to sample-efficient
                   'revival' of the capability, i.e., the model begins reusing
                   these capability after only a few gradient steps. This
                   indicates that practitioners can unintentionally remove a
                   model's safety wrapper merely by fine-tuning it on a, e.g.,
                   superficially unrelated, downstream task. We additionally
                   perform analysis on language models trained on the
                   TinyStories dataset to support our claims in a more
                   realistic setup.",
  month         =  nov,
  year          =  2023,
  keywords      = "Interpretability",
  archivePrefix = "arXiv",
  eprint        = "2311.12786",
  primaryClass  = "cs.LG",
  arxivid       = "2311.12786"
}

@ARTICLE{Weston2023-uc,
  title         = "System 2 Attention (is something you might need too)",
  author        = "Weston, Jason and Sukhbaatar, Sainbayar",
  abstract      = "Soft attention in Transformer-based Large Language Models
                   (LLMs) is susceptible to incorporating irrelevant
                   information from the context into its latent
                   representations, which adversely affects next token
                   generations. To help rectify these issues, we introduce
                   System 2 Attention (S2A), which leverages the ability of
                   LLMs to reason in natural language and follow instructions
                   in order to decide what to attend to. S2A regenerates the
                   input context to only include the relevant portions, before
                   attending to the regenerated context to elicit the final
                   response. In experiments, S2A outperforms standard
                   attention-based LLMs on three tasks containing opinion or
                   irrelevant information, QA, math word problems and longform
                   generation, where S2A increases factuality and objectivity,
                   and decreases sycophancy.",
  month         =  nov,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2311.11829",
  primaryClass  = "cs.CL",
  arxivid       = "2311.11829"
}

@ARTICLE{Nori2023-ig,
  title         = "Can generalist foundation models outcompete special-purpose
                   tuning? Case study in medicine",
  author        = "Nori, Harsha and Lee, Yin Tat and Zhang, Sheng and Carignan,
                   Dean and Edgar, Richard and Fusi, Nicolo and King, Nicholas
                   and Larson, Jonathan and Li, Yuanzhi and Liu, Weishung and
                   Luo, Renqian and McKinney, Scott Mayer and Ness, Robert
                   Osazuwa and Poon, Hoifung and Qin, Tao and Usuyama, Naoto
                   and White, Chris and Horvitz, Eric",
  abstract      = "Generalist foundation models such as GPT-4 have displayed
                   surprising capabilities in a wide variety of domains and
                   tasks. Yet, there is a prevalent assumption that they cannot
                   match specialist capabilities of fine-tuned models. For
                   example, most explorations to date on medical competency
                   benchmarks have leveraged domain-specific training, as
                   exemplified by efforts on BioGPT and Med-PaLM. We build on a
                   prior study of GPT-4's capabilities on medical challenge
                   benchmarks in the absence of special training. Rather than
                   using simple prompting to highlight the model's
                   out-of-the-box capabilities, we perform a systematic
                   exploration of prompt engineering. We find that prompting
                   innovation can unlock deeper specialist capabilities and
                   show that GPT-4 easily tops prior leading results for
                   medical benchmarks. The prompting methods we explore are
                   general purpose, and make no specific use of domain
                   expertise, removing the need for expert-curated content. Our
                   experimental design carefully controls for overfitting
                   during the prompt engineering process. We introduce
                   Medprompt, based on a composition of several prompting
                   strategies. With Medprompt, GPT-4 achieves state-of-the-art
                   results on all nine of the benchmark datasets in the
                   MultiMedQA suite. The method outperforms leading specialist
                   models such as Med-PaLM 2 by a significant margin with an
                   order of magnitude fewer calls to the model. Steering GPT-4
                   with Medprompt achieves a 27\% reduction in error rate on
                   the MedQA dataset over the best methods to date achieved
                   with specialist models and surpasses a score of 90\% for the
                   first time. Beyond medical problems, we show the power of
                   Medprompt to generalize to other domains and provide
                   evidence for the broad applicability of the approach via
                   studies of the strategy on exams in electrical engineering,
                   machine learning, philosophy, accounting, law, nursing, and
                   clinical psychology.",
  month         =  nov,
  year          =  2023,
  keywords      = "In-Context;Applications;AI in Health;Health",
  archivePrefix = "arXiv",
  eprint        = "2311.16452",
  primaryClass  = "cs.CL",
  arxivid       = "2311.16452"
}

@ARTICLE{Bai2023-qf,
  title         = "Sequential modeling enables scalable learning for large
                   vision models",
  author        = "Bai, Yutong and Geng, Xinyang and Mangalam, Karttikeya and
                   Bar, Amir and Yuille, Alan and Darrell, Trevor and Malik,
                   Jitendra and Efros, Alexei A",
  abstract      = "We introduce a novel sequential modeling approach which
                   enables learning a Large Vision Model (LVM) without making
                   use of any linguistic data. To do this, we define a common
                   format, ``visual sentences'', in which we can represent raw
                   images and videos as well as annotated data sources such as
                   semantic segmentations and depth reconstructions without
                   needing any meta-knowledge beyond the pixels. Once this wide
                   variety of visual data (comprising 420 billion tokens) is
                   represented as sequences, the model can be trained to
                   minimize a cross-entropy loss for next token prediction. By
                   training across various scales of model architecture and
                   data diversity, we provide empirical evidence that our
                   models scale effectively. Many different vision tasks can be
                   solved by designing suitable visual prompts at test time.",
  month         =  dec,
  year          =  2023,
  keywords      = "Multimodal (Vision, speech, etc)",
  archivePrefix = "arXiv",
  eprint        = "2312.00785",
  primaryClass  = "cs.CV",
  arxivid       = "2312.00785"
}

@ARTICLE{Gurnee2023-mq,
  title         = "Language models represent space and time",
  author        = "Gurnee, Wes and Tegmark, Max",
  abstract      = "The capabilities of large language models (LLMs) have
                   sparked debate over whether such systems just learn an
                   enormous collection of superficial statistics or a coherent
                   model of the data generating process -- a world model. We
                   find evidence for the latter by analyzing the learned
                   representations of three spatial datasets (world, US, NYC
                   places) and three temporal datasets (historical figures,
                   artworks, news headlines) in the Llama-2 family of models.
                   We discover that LLMs learn linear representations of space
                   and time across multiple scales. These representations are
                   robust to prompting variations and unified across different
                   entity types (e.g. cities and landmarks). In addition, we
                   identify individual ``space neurons'' and ``time neurons''
                   that reliably encode spatial and temporal coordinates. Our
                   analysis demonstrates that modern LLMs acquire structured
                   knowledge about fundamental dimensions such as space and
                   time, supporting the view that they learn not merely
                   superficial statistics, but literal world models.",
  month         =  oct,
  year          =  2023,
  keywords      = "Understanding",
  archivePrefix = "arXiv",
  eprint        = "2310.02207",
  primaryClass  = "cs.LG",
  arxivid       = "2310.02207"
}

@ARTICLE{Donoho2023-ey,
  title         = "Data science at the Singularity",
  author        = "Donoho, David",
  abstract      = "A purported `AI Singularity' has been in the public eye
                   recently. Mass media and US national political attention
                   focused on `AI Doom' narratives hawked by social media
                   influencers. The European Commission is announcing
                   initiatives to forestall `AI Extinction'. In my opinion, `AI
                   Singularity' is the wrong narrative for what's happening
                   now; recent happenings signal something else entirely.
                   Something fundamental to computation-based research really
                   changed in the last ten years. In certain fields, progress
                   is dramatically more rapid than previously, as the fields
                   undergo a transition to frictionless reproducibility (FR).
                   This transition markedly changes the rate of spread of ideas
                   and practices, affects mindsets, and erases memories of much
                   that came before. The emergence of frictionless
                   reproducibility follows from the maturation of 3 data
                   science principles in the last decade. Those principles
                   involve data sharing, code sharing, and competitive
                   challenges, however implemented in the particularly strong
                   form of frictionless open services. Empirical Machine
                   Learning (EML) is todays leading adherent field, and its
                   consequent rapid changes are responsible for the AI progress
                   we see. Still, other fields can and do benefit when they
                   adhere to the same principles. Many rapid changes from this
                   maturation are misidentified. The advent of FR in EML
                   generates a steady flow of innovations; this flow stimulates
                   outsider intuitions that there's an emergent superpower
                   somewhere in AI. This opens the way for PR to push worrying
                   narratives: not only `AI Extinction', but also the supposed
                   monopoly of big tech on AI research. The helpful narrative
                   observes that the superpower of EML is adherence to
                   frictionless reproducibility practices; these practices are
                   responsible for the striking progress in AI that we see
                   everywhere.",
  month         =  oct,
  year          =  2023,
  keywords      = "Understanding",
  archivePrefix = "arXiv",
  eprint        = "2310.00865",
  primaryClass  = "stat.OT",
  arxivid       = "2310.00865"
}

@INPROCEEDINGS{Zhao2020-na,
  title           = "Hierarchical attention transfer networks for depression
                     assessment from speech",
  booktitle       = "{ICASSP} 2020 - 2020 {IEEE} International Conference on
                     Acoustics, Speech and Signal Processing ({ICASSP})",
  author          = "Zhao, Ziping and Bao, Zhongtian and Zhang, Zixing and
                     Cummins, Nicholas and Wang, Haishuai and Schuller, Bjorn",
  abstract        = "A growing area of mental health research is the search for
                     speech-based objective markers for conditions such as
                     depression. However, when combined with machine learning,
                     this search can be challenging due to a limited amount of
                     annotated training data. In this paper, we propose a novel
                     crosstask approach which transfers attention mechanisms
                     from speech recognition to aid depression severity
                     measurement. This transfer is applied in a two-level
                     hierarchical network which mirrors the natural
                     hierarchical structure of speech. Experiments based on the
                     Distress Analysis Interview Corpus - Wizard of Oz
                     (DAIC-WOZ) dataset, as used in the 2017 Audio/Visual
                     Emotion Challenge, demonstrate the effectiveness of our
                     Hierarchical Attention Transfer Network. On the
                     development set, the proposed approach achieves a root
                     mean square error (RMSE) of 3.85, and a mean absolute
                     error (MAE) of 2.99, on a Patient Health Questionnaire
                     (PHQ)-8 scale [0], [24], while on the test set, it
                     achieves an RMSE of 5.66 and an MAE of 4.28. To the best
                     of our knowledge, these scores represent the best-known
                     speech-only results to date on this corpus.",
  publisher       = "IEEE",
  month           =  may,
  year            =  2020,
  keywords        = "Audio as a Biomarker;LLMs",
  conference      = "ICASSP 2020 - 2020 IEEE International Conference on
                     Acoustics, Speech and Signal Processing (ICASSP)",
  location        = "Barcelona, Spain",
  isbn            = "9781509066315",
  doi             = "10.1109/icassp40776.2020.9053207"
}

@ARTICLE{Liang2023-at,
  title         = "Can large language models provide useful feedback on
                   research papers? A large-scale empirical analysis",
  author        = "Liang, Weixin and Zhang, Yuhui and Cao, Hancheng and Wang,
                   Binglu and Ding, Daisy and Yang, Xinyu and Vodrahalli,
                   Kailas and He, Siyu and Smith, Daniel and Yin, Yian and
                   McFarland, Daniel and Zou, James",
  abstract      = "Expert feedback lays the foundation of rigorous research.
                   However, the rapid growth of scholarly production and
                   intricate knowledge specialization challenge the
                   conventional scientific feedback mechanisms. High-quality
                   peer reviews are increasingly difficult to obtain.
                   Researchers who are more junior or from under-resourced
                   settings have especially hard times getting timely feedback.
                   With the breakthrough of large language models (LLM) such as
                   GPT-4, there is growing interest in using LLMs to generate
                   scientific feedback on research manuscripts. However, the
                   utility of LLM-generated feedback has not been
                   systematically studied. To address this gap, we created an
                   automated pipeline using GPT-4 to provide comments on the
                   full PDFs of scientific papers. We evaluated the quality of
                   GPT-4's feedback through two large-scale studies. We first
                   quantitatively compared GPT-4's generated feedback with
                   human peer reviewer feedback in 15 Nature family journals
                   (3,096 papers in total) and the ICLR machine learning
                   conference (1,709 papers). The overlap in the points raised
                   by GPT-4 and by human reviewers (average overlap 30.85\% for
                   Nature journals, 39.23\% for ICLR) is comparable to the
                   overlap between two human reviewers (average overlap 28.58\%
                   for Nature journals, 35.25\% for ICLR). The overlap between
                   GPT-4 and human reviewers is larger for the weaker papers.
                   We then conducted a prospective user study with 308
                   researchers from 110 US institutions in the field of AI and
                   computational biology to understand how researchers perceive
                   feedback generated by our GPT-4 system on their own papers.
                   Overall, more than half (57.4\%) of the users found GPT-4
                   generated feedback helpful/very helpful and 82.4\% found it
                   more beneficial than feedback from at least some human
                   reviewers. While our findings show that LLM-generated
                   feedback can help researchers, we also identify several
                   limitations.",
  month         =  oct,
  year          =  2023,
  keywords      = "Applications",
  archivePrefix = "arXiv",
  eprint        = "2310.01783",
  primaryClass  = "cs.LG",
  arxivid       = "2310.01783"
}

@ARTICLE{Adams2023-vm,
  title    = "A new research agenda for African generative {AI}",
  author   = "Adams, Rachel and Alayande, Ayantola and Brey, Zameer and
              Browning, Brantley and Gastrow, Michael and Kponyo, Jerry John
              and Mathew, Dona and Nkosi, Moremi and Nunoo-Mensah, Henry and
              Nyakundi, Diana and Odumuyiwa, Victor and Okunowo, Olubunmi and
              Olbrich, Philipp and Omar, Nawal and Omotubora, Kemi and
              Plantinga, Paul and Razzano, Gabriella and Schroeder, Zara and
              Agbemenu, Andrew Selasi and Sey, Araba and Shilongo, Kristophina
              and Shirude, Shreya and Smith, Matthew and Tchao, Eric Tutu and
              Uwizera, Davy K",
  journal  = "Nat Hum Behav",
  month    =  oct,
  year     =  2023,
  keywords = "Applications",
  language = "en",
  issn     = "2397-3374",
  pmid     = "37803130",
  doi      = "10.1038/s41562-023-01735-1"
}

@ARTICLE{Han2023-cx,
  title         = "{HyperAttention}: Long-context attention in near-linear time",
  author        = "Han, Insu and Jayaram, Rajesh and Karbasi, Amin and
                   Mirrokni, Vahab and Woodruff, David P and Zandieh, Amir",
  abstract      = "We present an approximate attention mechanism named
                   HyperAttention to address the computational challenges posed
                   by the growing complexity of long contexts used in Large
                   Language Models (LLMs). Recent work suggests that in the
                   worst-case scenario, quadratic time is necessary unless the
                   entries of the attention matrix are bounded or the matrix
                   has low stable rank. We introduce two parameters which
                   measure: (1) the max column norm in the normalized attention
                   matrix, and (2) the ratio of row norms in the unnormalized
                   attention matrix after detecting and removing large entries.
                   We use these fine-grained parameters to capture the hardness
                   of the problem. Despite previous lower bounds, we are able
                   to achieve a linear time sampling algorithm even when the
                   matrix has unbounded entries or a large stable rank,
                   provided the above parameters are small. HyperAttention
                   features a modular design that easily accommodates
                   integration of other fast low-level implementations,
                   particularly FlashAttention. Empirically, employing Locality
                   Sensitive Hashing (LSH) to identify large entries,
                   HyperAttention outperforms existing methods, giving
                   significant speed improvements compared to state-of-the-art
                   solutions like FlashAttention. We validate the empirical
                   performance of HyperAttention on a variety of different
                   long-context length datasets. For example, HyperAttention
                   makes the inference time of ChatGLM2 50\% faster on 32k
                   context length while perplexity increases from 5.6 to 6.3.
                   On larger context length, e.g., 131k, with causal masking,
                   HyperAttention offers 5-fold speedup on a single attention
                   layer.",
  month         =  oct,
  year          =  2023,
  keywords      = "Efficient",
  archivePrefix = "arXiv",
  eprint        = "2310.05869",
  primaryClass  = "cs.LG",
  arxivid       = "2310.05869"
}

@ARTICLE{Zhu2023-ne,
  title         = "Large Language Models can Learn Rules",
  author        = "Zhu, Zhaocheng and Xue, Yuan and Chen, Xinyun and Zhou,
                   Denny and Tang, Jian and Schuurmans, Dale and Dai, Hanjun",
  abstract      = "When prompted with a few examples and intermediate steps,
                   large language models (LLMs) have demonstrated impressive
                   performance in various reasoning tasks. However, prompting
                   methods that rely on implicit knowledge in an LLM often
                   hallucinate incorrect answers when the implicit knowledge is
                   wrong or inconsistent with the task. To tackle this problem,
                   we present Hypotheses-to-Theories (HtT), a framework that
                   learns a rule library for reasoning with LLMs. HtT contains
                   two stages, an induction stage and a deduction stage. In the
                   induction stage, an LLM is first asked to generate and
                   verify rules over a set of training examples. Rules that
                   appear and lead to correct answers sufficiently often are
                   collected to form a rule library. In the deduction stage,
                   the LLM is then prompted to employ the learned rule library
                   to perform reasoning to answer test questions. Experiments
                   on both numerical reasoning and relational reasoning
                   problems show that HtT improves existing prompting methods,
                   with an absolute gain of 11-27\% in accuracy. The learned
                   rules are also transferable to different models and to
                   different forms of the same problem.",
  month         =  oct,
  year          =  2023,
  keywords      = "In-Context",
  archivePrefix = "arXiv",
  eprint        = "2310.07064",
  primaryClass  = "cs.AI",
  arxivid       = "2310.07064"
}

@ARTICLE{Zakka2024-nx,
  title     = "Almanac --- retrieval-augmented language models for clinical
               medicine",
  author    = "Zakka, Cyril and Shad, Rohan and Chaurasia, Akash and Dalal,
               Alex R and Kim, Jennifer L and Moor, Michael and Fong, Robyn and
               Phillips, Curran and Alexander, Kevin and Ashley, Euan and Boyd,
               Jack and Boyd, Kathleen and Hirsch, Karen and Langlotz, Curt and
               Lee, Rita and Melia, Joanna and Nelson, Joanna and Sallam, Karim
               and Tullis, Stacey and Vogelsong, Melissa Ann and Cunningham,
               John Patrick and Hiesinger, William",
  journal   = "NEJM AI",
  publisher = "Massachusetts Medical Society",
  volume    =  1,
  number    =  2,
  month     =  jan,
  year      =  2024,
  keywords  = "Health",
  language  = "en",
  issn      = "2836-9386",
  doi       = "10.1056/aioa2300068"
}

@ARTICLE{Jakubik2023-kt,
  title         = "Foundation models for generalist geospatial Artificial
                   Intelligence",
  author        = "Jakubik, Johannes and Roy, Sujit and Phillips, C E and
                   Fraccaro, Paolo and Godwin, Denys and Zadrozny, Bianca and
                   Szwarcman, Daniela and Gomes, Carlos and Nyirjesy, Gabby and
                   Edwards, Blair and Kimura, Daiki and Simumba, Naomi and Chu,
                   Linsong and Mukkavilli, S Karthik and Lambhate, Devyani and
                   Das, Kamal and Bangalore, Ranjini and Oliveira, Dario and
                   Muszynski, Michal and Ankur, Kumar and Ramasubramanian,
                   Muthukumaran and Gurung, Iksha and Khallaghi, Sam and
                   {Hanxi} and {Li} and Cecil, Michael and Ahmadi, Maryam and
                   Kordi, Fatemeh and Alemohammad, Hamed and Maskey, Manil and
                   Ganti, Raghu and Weldemariam, Kommy and Ramachandran, Rahul",
  abstract      = "Significant progress in the development of highly adaptable
                   and reusable Artificial Intelligence (AI) models is expected
                   to have a significant impact on Earth science and remote
                   sensing. Foundation models are pre-trained on large
                   unlabeled datasets through self-supervision, and then
                   fine-tuned for various downstream tasks with small labeled
                   datasets. This paper introduces a first-of-a-kind framework
                   for the efficient pre-training and fine-tuning of
                   foundational models on extensive geospatial data. We have
                   utilized this framework to create Prithvi, a
                   transformer-based geospatial foundational model pre-trained
                   on more than 1TB of multispectral satellite imagery from the
                   Harmonized Landsat-Sentinel 2 (HLS) dataset. Our study
                   demonstrates the efficacy of our framework in successfully
                   fine-tuning Prithvi to a range of Earth observation tasks
                   that have not been tackled by previous work on foundation
                   models involving multi-temporal cloud gap imputation, flood
                   mapping, wildfire scar segmentation, and multi-temporal crop
                   segmentation. Our experiments show that the pre-trained
                   model accelerates the fine-tuning process compared to
                   leveraging randomly initialized weights. In addition,
                   pre-trained Prithvi compares well against the
                   state-of-the-art, e.g., outperforming a conditional GAN
                   model in multi-temporal cloud imputation by up to 5pp (or
                   5.7\%) in the structural similarity index. Finally, due to
                   the limited availability of labeled data in the field of
                   Earth observation, we gradually reduce the quantity of
                   available labeled data for refining the model to evaluate
                   data efficiency and demonstrate that data can be decreased
                   significantly without affecting the model's accuracy. The
                   pre-trained 100 million parameter model and corresponding
                   fine-tuning workflows have been released publicly as open
                   source contributions to the global Earth sciences community
                   through Hugging Face.",
  month         =  oct,
  year          =  2023,
  keywords      = "Applications",
  archivePrefix = "arXiv",
  eprint        = "2310.18660",
  primaryClass  = "cs.CV",
  arxivid       = "2310.18660"
}

@ARTICLE{Smith2023-vd,
  title         = "{ConvNets} match Vision Transformers at scale",
  author        = "Smith, Samuel L and Brock, Andrew and Berrada, Leonard and
                   De, Soham",
  abstract      = "Many researchers believe that ConvNets perform well on small
                   or moderately sized datasets, but are not competitive with
                   Vision Transformers when given access to datasets on the
                   web-scale. We challenge this belief by evaluating a
                   performant ConvNet architecture pre-trained on JFT-4B, a
                   large labelled dataset of images often used for training
                   foundation models. We consider pre-training compute budgets
                   between 0.4k and 110k TPU-v4 core compute hours, and train a
                   series of networks of increasing depth and width from the
                   NFNet model family. We observe a log-log scaling law between
                   held out loss and compute budget. After fine-tuning on
                   ImageNet, NFNets match the reported performance of Vision
                   Transformers with comparable compute budgets. Our strongest
                   fine-tuned model achieves a Top-1 accuracy of 90.4\%.",
  month         =  oct,
  year          =  2023,
  keywords      = "Vision;CNN - Classification;Multimodal (Vision, speech,
                   etc);Models",
  archivePrefix = "arXiv",
  eprint        = "2310.16764",
  primaryClass  = "cs.CV",
  arxivid       = "2310.16764"
}

@MISC{noauthor_undated-iu,
  howpublished = "\url{https://www.anthropic.com/index/evaluating-ai-systems}",
  note         = "Accessed: 2023-10-5",
  keywords     = "Evaluation"
}

@MISC{Weng_undated-hk,
  title        = "Generalized Visual Language Models",
  author       = "Weng, Lilian",
  abstract     = "Processing images to generate text, such as image captioning
                  and visual question-answering, has been studied for years.
                  Traditionally such systems rely on an object detection
                  network as a vision encoder to capture visual features and
                  then produce text via a text decoder. Given a large amount of
                  existing literature, in this post, I would like to only focus
                  on one approach for solving vision language tasks, which is
                  to extend pre-trained generalized language models to be
                  capable of consuming visual signals.",
  howpublished = "\url{https://lilianweng.github.io/posts/2022-06-09-vlm/}",
  note         = "Accessed: 2023-10-27",
  keywords     = "Multimodal (Vision, speech, etc)"
}

@MISC{Anthropic2023-oq,
  title        = "Challenges in evaluating {AI} systems",
  booktitle    = "Anthropic",
  author       = "Anthropic, P B C",
  abstract     = "Here, we outline challenges that we have encountered while
                  evaluating our own models to give readers a sense of what
                  developing, implementing, and interpreting model evaluations
                  looks like in practice.",
  month        =  oct,
  year         =  2023,
  howpublished = "\url{https://www.anthropic.com/index/evaluating-ai-systems}",
  note         = "Accessed: 2023-10-27",
  keywords     = "Evaluation",
  language     = "en"
}

@MISC{Lehman2023-nr,
  title    = "{Clinical-T5}: Large Language Models Built Using {MIMIC} Clinical
              Text",
  author   = "Lehman, Eric and Johnson, Alistair",
  abstract = "Recent advances in scaling large language models (LLMs) has
              resulted in significant improvements over a number of natural
              language processing benchmarks. There has been some work to
              pretrain these language models over clinical text. These works
              demonstrate that training a language model using masked language
              modeling (MLM) on clinical notes is an effective technique for
              boosting performance on downstream tasks. All of these previous
              works use decoder-only architectures. We train 4 different
              clinical T5 models on the union of MIMIC-III and IV notes. Two of
              the models are initialized from previous T5-models (T5-base and
              SciFive). We additionally train a T5-Base and T5-Large model from
              scratch. These models should not be distributed to
              non-credentialed users. Research has shown that these language
              models have the potential to leak sensitive information. Due to
              this potential risk, we release the model weights under PhysioNet
              credentialed access.",
  month    =  jan,
  year     =  2023,
  keywords = "Health",
  doi      = "10.13026/rj8x-v335"
}

@ARTICLE{Zakka2023-yg,
  title         = "Almanac: {Retrieval-Augmented} Language Models for Clinical
                   Medicine",
  author        = "Zakka, Cyril and Chaurasia, Akash and Shad, Rohan and Dalal,
                   Alex R and Kim, Jennifer L and Moor, Michael and Alexander,
                   Kevin and Ashley, Euan and Boyd, Jack and Boyd, Kathleen and
                   Hirsch, Karen and Langlotz, Curt and Nelson, Joanna and
                   Hiesinger, William",
  abstract      = "Large-language models have recently demonstrated impressive
                   zero-shot capabilities in a variety of natural language
                   tasks such as summarization, dialogue generation, and
                   question-answering. Despite many promising applications in
                   clinical medicine, adoption of these models in real-world
                   settings has been largely limited by their tendency to
                   generate incorrect and sometimes even toxic statements. In
                   this study, we develop Almanac, a large language model
                   framework augmented with retrieval capabilities for medical
                   guideline and treatment recommendations. Performance on a
                   novel dataset of clinical scenarios (n = 130) evaluated by a
                   panel of 5 board-certified and resident physicians
                   demonstrates significant increases in factuality (mean of
                   18\% at p-value < 0.05) across all specialties, with
                   improvements in completeness and safety. Our results
                   demonstrate the potential for large language models to be
                   effective tools in the clinical decision-making process,
                   while also emphasizing the importance of careful testing and
                   deployment to mitigate their shortcomings.",
  month         =  mar,
  year          =  2023,
  keywords      = "Health",
  archivePrefix = "arXiv",
  eprint        = "2303.01229",
  primaryClass  = "cs.CL",
  arxivid       = "2303.01229"
}

@MISC{noauthor_undated-qh,
  title       = "ragas: Evaluation framework for your Retrieval Augmented
                 Generation ({RAG}) pipelines",
  abstract    = "Evaluation framework for your Retrieval Augmented Generation
                 (RAG) pipelines - GitHub - explodinggradients/ragas:
                 Evaluation framework for your Retrieval Augmented Generation
                 (RAG) pipelines",
  institution = "Github",
  keywords    = "Evaluation",
  language    = "en"
}

@ARTICLE{Peng2023-iz,
  title         = "{YaRN}: Efficient context window extension of large language
                   models",
  author        = "Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and
                   Shippole, Enrico",
  abstract      = "Rotary Position Embeddings (RoPE) have been shown to
                   effectively encode positional information in
                   transformer-based language models. However, these models
                   fail to generalize past the sequence length they were
                   trained on. We present YaRN (Yet another RoPE extensioN
                   method), a compute-efficient method to extend the context
                   window of such models, requiring 10x less tokens and 2.5x
                   less training steps than previous methods. Using YaRN, we
                   show that LLaMA models can effectively utilize and
                   extrapolate to context lengths much longer than their
                   original pre-training would allow, while also surpassing
                   previous the state-of-the-art at context window extension.
                   In addition, we demonstrate that YaRN exhibits the
                   capability to extrapolate beyond the limited context of a
                   fine-tuning dataset. The models fine-tuned using YaRN has
                   been made available and reproduced online up to 128k context
                   length at https://github.com/jquesnelle/yarn",
  month         =  aug,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2309.00071",
  primaryClass  = "cs.CL",
  arxivid       = "2309.00071"
}

@ARTICLE{Gandhi2023-uq,
  title         = "{Distil-Whisper}: Robust knowledge distillation via
                   large-scale pseudo labelling",
  author        = "Gandhi, Sanchit and von Platen, Patrick and Rush, Alexander
                   M",
  abstract      = "As the size of pre-trained speech recognition models
                   increases, running these large models in low-latency or
                   resource-constrained environments becomes challenging. In
                   this work, we leverage pseudo-labelling to assemble a
                   large-scale open-source dataset which we use to distill the
                   Whisper model into a smaller variant, called Distil-Whisper.
                   Using a simple word error rate (WER) heuristic, we select
                   only the highest quality pseudo-labels for training. The
                   distilled model is 5.8 times faster with 51\% fewer
                   parameters, while performing to within 1\% WER on
                   out-of-distribution test data in a zero-shot transfer
                   setting. Distil-Whisper maintains the robustness of the
                   Whisper model to difficult acoustic conditions, while being
                   less prone to hallucination errors on long-form audio.
                   Distil-Whisper is designed to be paired with Whisper for
                   speculative decoding, yielding a 2 times speed-up while
                   mathematically ensuring the same outputs as the original
                   model. To facilitate further research in this domain, we
                   make our training code, inference code and models publicly
                   accessible.",
  month         =  nov,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2311.00430",
  primaryClass  = "cs.CL",
  arxivid       = "2311.00430"
}

@ARTICLE{Morris2023-oe,
  title         = "Levels of {AGI}: Operationalizing progress on the path to
                   {AGI}",
  author        = "Morris, Meredith Ringel and Sohl-dickstein, Jascha and
                   Fiedel, Noah and Warkentin, Tris and Dafoe, Allan and Faust,
                   Aleksandra and Farabet, Clement and Legg, Shane",
  abstract      = "We propose a framework for classifying the capabilities and
                   behavior of Artificial General Intelligence (AGI) models and
                   their precursors. This framework introduces levels of AGI
                   performance, generality, and autonomy. It is our hope that
                   this framework will be useful in an analogous way to the
                   levels of autonomous driving, by providing a common language
                   to compare models, assess risks, and measure progress along
                   the path to AGI. To develop our framework, we analyze
                   existing definitions of AGI, and distill six principles that
                   a useful ontology for AGI should satisfy. These principles
                   include focusing on capabilities rather than mechanisms;
                   separately evaluating generality and performance; and
                   defining stages along the path toward AGI, rather than
                   focusing on the endpoint. With these principles in mind, we
                   propose 'Levels of AGI' based on depth (performance) and
                   breadth (generality) of capabilities, and reflect on how
                   current systems fit into this ontology. We discuss the
                   challenging requirements for future benchmarks that quantify
                   the behavior and capabilities of AGI models against these
                   levels. Finally, we discuss how these levels of AGI interact
                   with deployment considerations such as autonomy and risk,
                   and emphasize the importance of carefully selecting Human-AI
                   Interaction paradigms for responsible and safe deployment of
                   highly capable AI systems.",
  month         =  nov,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2311.02462",
  primaryClass  = "cs.AI",
  arxivid       = "2311.02462"
}

@ARTICLE{Zhou2023-nu,
  title         = "A survey of large language models in medicine: Principles,
                   applications, and challenges",
  author        = "Zhou, Hongjian and Liu, Fenglin and Gu, Boyang and Zou,
                   Xinyu and Huang, Jinfa and Wu, Jinge and Li, Yiru and Chen,
                   Sam S and Zhou, Peilin and Liu, Junling and Hua, Yining and
                   Mao, Chengfeng and Wu, Xian and Zheng, Yefeng and Clifton,
                   Lei and Li, Zheng and Luo, Jiebo and Clifton, David A",
  abstract      = "Large language models (LLMs), such as ChatGPT, have received
                   substantial attention due to their impressive human language
                   understanding and generation capabilities. Therefore, the
                   application of LLMs in medicine to assist physicians and
                   patient care emerges as a promising research direction in
                   both artificial intelligence and clinical medicine. To
                   reflect this trend, this survey provides a comprehensive
                   overview of the principles, applications, and challenges
                   faced by LLMs in medicine. Specifically, we aim to address
                   the following questions: 1) How can medical LLMs be built?
                   2) What are the downstream performances of medical LLMs? 3)
                   How can medical LLMs be utilized in real-world clinical
                   practice? 4) What challenges arise from the use of medical
                   LLMs? and 5) How can we better construct and utilize medical
                   LLMs? As a result, this survey aims to provide insights into
                   the opportunities and challenges of LLMs in medicine and
                   serve as a valuable resource for constructing practical and
                   effective medical LLMs. A regularly updated list of
                   practical guides on medical LLMs can be found at
                   https://github.com/AI-in-Health/MedLLMsPracticalGuide.",
  month         =  nov,
  year          =  2023,
  keywords      = "Health",
  archivePrefix = "arXiv",
  eprint        = "2311.05112",
  primaryClass  = "cs.CL",
  arxivid       = "2311.05112"
}

@ARTICLE{Friedman2023-wa,
  title         = "Learning Transformer Programs",
  author        = "Friedman, Dan and Wettig, Alexander and Chen, Danqi",
  abstract      = "Recent research in mechanistic interpretability has
                   attempted to reverse-engineer Transformer models by
                   carefully inspecting network weights and activations.
                   However, these approaches require considerable manual effort
                   and still fall short of providing complete, faithful
                   descriptions of the underlying algorithms. In this work, we
                   introduce a procedure for training Transformers that are
                   mechanistically interpretable by design. We build on RASP
                   [Weiss et al., 2021], a programming language that can be
                   compiled into Transformer weights. Instead of compiling
                   human-written programs into Transformers, we design a
                   modified Transformer that can be trained using
                   gradient-based optimization and then automatically converted
                   into a discrete, human-readable program. We refer to these
                   models as Transformer Programs. To validate our approach, we
                   learn Transformer Programs for a variety of problems,
                   including an in-context learning task, a suite of
                   algorithmic problems (e.g. sorting, recognizing Dyck
                   languages), and NLP tasks including named entity recognition
                   and text classification. The Transformer Programs can
                   automatically find reasonable solutions, performing on par
                   with standard Transformers of comparable size; and, more
                   importantly, they are easy to interpret. To demonstrate
                   these advantages, we convert Transformers into Python
                   programs and use off-the-shelf code analysis tools to debug
                   model errors and identify the ``circuits'' used to solve
                   different sub-problems. We hope that Transformer Programs
                   open a new path toward the goal of intrinsically
                   interpretable machine learning.",
  month         =  jun,
  year          =  2023,
  keywords      = "Understanding",
  archivePrefix = "arXiv",
  eprint        = "2306.01128",
  primaryClass  = "cs.LG",
  arxivid       = "2306.01128"
}

@ARTICLE{Ye2023-vs,
  title         = "Cognitive Mirage: A Review of Hallucinations in Large
                   Language Models",
  author        = "Ye, Hongbin and Liu, Tong and Zhang, Aijia and Hua, Wei and
                   Jia, Weiqiang",
  abstract      = "As large language models continue to develop in the field of
                   AI, text generation systems are susceptible to a worrisome
                   phenomenon known as hallucination. In this study, we
                   summarize recent compelling insights into hallucinations in
                   LLMs. We present a novel taxonomy of hallucinations from
                   various text generation tasks, thus provide theoretical
                   insights, detection methods and improvement approaches.
                   Based on this, future research directions are proposed. Our
                   contribution are threefold: (1) We provide a detailed and
                   complete taxonomy for hallucinations appearing in text
                   generation tasks; (2) We provide theoretical analyses of
                   hallucinations in LLMs and provide existing detection and
                   improvement methods; (3) We propose several research
                   directions that can be developed in the future. As
                   hallucinations garner significant attention from the
                   community, we will maintain updates on relevant research
                   progress.",
  month         =  sep,
  year          =  2023,
  keywords      = "Hallucination",
  archivePrefix = "arXiv",
  eprint        = "2309.06794",
  primaryClass  = "cs.CL",
  arxivid       = "2309.06794"
}

@ARTICLE{Zhang2023-ic,
  title         = "Watermarks in the sand: Impossibility of strong watermarking
                   for generative models",
  author        = "Zhang, Hanlin and Edelman, Benjamin L and Francati, Danilo
                   and Venturi, Daniele and Ateniese, Giuseppe and Barak, Boaz",
  abstract      = "Watermarking generative models consists of planting a
                   statistical signal (watermark) in a model's output so that
                   it can be later verified that the output was generated by
                   the given model. A strong watermarking scheme satisfies the
                   property that a computationally bounded attacker cannot
                   erase the watermark without causing significant quality
                   degradation. In this paper, we study the (im)possibility of
                   strong watermarking schemes. We prove that, under
                   well-specified and natural assumptions, strong watermarking
                   is impossible to achieve. This holds even in the private
                   detection algorithm setting, where the watermark insertion
                   and detection algorithms share a secret key, unknown to the
                   attacker. To prove this result, we introduce a generic
                   efficient watermark attack; the attacker is not required to
                   know the private key of the scheme or even which scheme is
                   used. Our attack is based on two assumptions: (1) The
                   attacker has access to a ``quality oracle'' that can
                   evaluate whether a candidate output is a high-quality
                   response to a prompt, and (2) The attacker has access to a
                   ``perturbation oracle'' which can modify an output with a
                   nontrivial probability of maintaining quality, and which
                   induces an efficiently mixing random walk on high-quality
                   outputs. We argue that both assumptions can be satisfied in
                   practice by an attacker with weaker computational
                   capabilities than the watermarked model itself, to which the
                   attacker has only black-box access. Furthermore, our
                   assumptions will likely only be easier to satisfy over time
                   as models grow in capabilities and modalities. We
                   demonstrate the feasibility of our attack by instantiating
                   it to attack three existing watermarking schemes for large
                   language models: Kirchenbauer et al. (2023), Kuditipudi et
                   al. (2023), and Zhao et al. (2023). The same attack
                   successfully removes the watermarks planted by all three
                   schemes, with only minor quality degradation.",
  month         =  nov,
  year          =  2023,
  keywords      = "Watermarking",
  archivePrefix = "arXiv",
  eprint        = "2311.04378",
  primaryClass  = "cs.LG",
  arxivid       = "2311.04378"
}

@MISC{noauthor_undated-zp,
  howpublished = "\url{https://papers.nips.cc/paper_files/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html}",
  note         = "Accessed: 2023-11-10",
  keywords     = "LLMs"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{noauthor_undated-rl,
  title        = "Hacking Google Bard - From Prompt Injection to Data
                  Exfiltration ·",
  booktitle    = "Embrace The Red",
  howpublished = "\url{https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/}",
  note         = "Accessed: 2023-11-27",
  keywords     = "Jailbreak",
  language     = "en"
}

@ARTICLE{Scheurer2023-hy,
  title         = "Technical Report: Large Language Models can Strategically
                   Deceive their Users when Put Under Pressure",
  author        = "Scheurer, J{\'e}r{\'e}my and Balesni, Mikita and Hobbhahn,
                   Marius",
  abstract      = "We demonstrate a situation in which Large Language Models,
                   trained to be helpful, harmless, and honest, can display
                   misaligned behavior and strategically deceive their users
                   about this behavior without being instructed to do so.
                   Concretely, we deploy GPT-4 as an agent in a realistic,
                   simulated environment, where it assumes the role of an
                   autonomous stock trading agent. Within this environment, the
                   model obtains an insider tip about a lucrative stock trade
                   and acts upon it despite knowing that insider trading is
                   disapproved of by company management. When reporting to its
                   manager, the model consistently hides the genuine reasons
                   behind its trading decision. We perform a brief
                   investigation of how this behavior varies under changes to
                   the setting, such as removing model access to a reasoning
                   scratchpad, attempting to prevent the misaligned behavior by
                   changing system instructions, changing the amount of
                   pressure the model is under, varying the perceived risk of
                   getting caught, and making other simple changes to the
                   environment. To our knowledge, this is the first
                   demonstration of Large Language Models trained to be
                   helpful, harmless, and honest, strategically deceiving their
                   users in a realistic situation without direct instructions
                   or training for deception.",
  month         =  nov,
  year          =  2023,
  keywords      = "Hallucination",
  archivePrefix = "arXiv",
  eprint        = "2311.07590",
  primaryClass  = "cs.CL",
  arxivid       = "2311.07590"
}

@ARTICLE{Wang2023-cy,
  title         = "Data management for Large Language Models: A survey",
  author        = "Wang, Zige and Zhong, Wanjun and Wang, Yufei and Zhu, Qi and
                   Mi, Fei and Wang, Baojun and Shang, Lifeng and Jiang, Xin
                   and Liu, Qun",
  abstract      = "Data plays a fundamental role in the training of Large
                   Language Models (LLMs). Effective data management,
                   particularly in the formulation of a well-suited training
                   dataset, holds significance for enhancing model performance
                   and improving training efficiency during pretraining and
                   supervised fine-tuning phases. Despite the considerable
                   importance of data management, the current research
                   community still falls short in providing a systematic
                   analysis of the rationale behind management strategy
                   selection, its consequential effects, methodologies for
                   evaluating curated datasets, and the ongoing pursuit of
                   improved strategies. Consequently, the exploration of data
                   management has attracted more and more attention among the
                   research community. This survey provides a comprehensive
                   overview of current research in data management within both
                   the pretraining and supervised fine-tuning stages of LLMs,
                   covering various noteworthy aspects of data management
                   strategy design: data quantity, data quality, domain/task
                   composition, etc. Looking toward the future, we extrapolate
                   existing challenges and outline promising directions for
                   development in this field. Therefore, this survey serves as
                   a guiding resource for practitioners aspiring to construct
                   powerful LLMs through effective data management practices.
                   The collection of the latest papers is available at
                   https://github.com/ZigeW/data\_management\_LLM.",
  month         =  dec,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  eprint        = "2312.01700",
  primaryClass  = "cs.CL",
  arxivid       = "2312.01700"
}

@ARTICLE{Ding2023-qs,
  title         = "The efficiency spectrum of Large Language Models: An
                   algorithmic survey",
  author        = "Ding, Tianyu and Chen, Tianyi and Zhu, Haidong and Jiang,
                   Jiachen and Zhong, Yiqi and Zhou, Jinxin and Wang, Guangzhi
                   and Zhu, Zhihui and Zharkov, Ilya and Liang, Luming",
  abstract      = "The rapid growth of Large Language Models (LLMs) has been a
                   driving force in transforming various domains, reshaping the
                   artificial general intelligence landscape. However, the
                   increasing computational and memory demands of these models
                   present substantial challenges, hindering both academic
                   research and practical applications. To address these
                   issues, a wide array of methods, including both algorithmic
                   and hardware solutions, have been developed to enhance the
                   efficiency of LLMs. This survey delivers a comprehensive
                   review of algorithmic advancements aimed at improving LLM
                   efficiency. Unlike other surveys that typically focus on
                   specific areas such as training or model compression, this
                   paper examines the multi-faceted dimensions of efficiency
                   essential for the end-to-end algorithmic development of
                   LLMs. Specifically, it covers various topics related to
                   efficiency, including scaling laws, data utilization,
                   architectural innovations, training and tuning strategies,
                   and inference techniques. This paper aims to serve as a
                   valuable resource for researchers and practitioners, laying
                   the groundwork for future innovations in this critical
                   research area. Our repository of relevant references is
                   maintained at
                   url\{https://github.com/tding1/Efficient-LLM-Survey\}.",
  month         =  dec,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  eprint        = "2312.00678",
  primaryClass  = "cs.CL",
  arxivid       = "2312.00678"
}

@ARTICLE{Liu2023-wf,
  title         = "{LLM360}: Towards Fully Transparent {Open-Source} {LLMs}",
  author        = "Liu, Zhengzhong and Qiao, Aurick and Neiswanger, Willie and
                   Wang, Hongyi and Tan, Bowen and Tao, Tianhua and Li, Junbo
                   and Wang, Yuqi and Sun, Suqi and Pangarkar, Omkar and Fan,
                   Richard and Gu, Yi and Miller, Victor and Zhuang, Yonghao
                   and He, Guowei and Li, Haonan and Koto, Fajri and Tang,
                   Liping and Ranjan, Nikhil and Shen, Zhiqiang and Ren,
                   Xuguang and Iriondo, Roberto and Mu, Cun and Hu, Zhiting and
                   Schulze, Mark and Nakov, Preslav and Baldwin, Tim and Xing,
                   Eric P",
  abstract      = "The recent surge in open-source Large Language Models
                   (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse
                   options for AI practitioners and researchers. However, most
                   LLMs have only released partial artifacts, such as the final
                   model weights or inference code, and technical reports
                   increasingly limit their scope to high-level design choices
                   and surface statistics. These choices hinder progress in the
                   field by degrading transparency into the training of LLMs
                   and forcing teams to rediscover many details in the training
                   process. We present LLM360, an initiative to fully
                   open-source LLMs, which advocates for all training code and
                   data, model checkpoints, and intermediate results to be made
                   available to the community. The goal of LLM360 is to support
                   open and collaborative AI research by making the end-to-end
                   LLM training process transparent and reproducible by
                   everyone. As a first step of LLM360, we release two 7B
                   parameter LLMs pre-trained from scratch, Amber and
                   CrystalCoder, including their training code, data,
                   intermediate checkpoints, and analyses (at
                   https://www.llm360.ai). We are committed to continually
                   pushing the boundaries of LLMs through this open-source
                   effort. More large-scale and stronger models are underway
                   and will be released in the future.",
  month         =  dec,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2312.06550",
  primaryClass  = "cs.CL",
  arxivid       = "2312.06550"
}

@ARTICLE{Shankar2024-yi,
  title         = "{SPADE}: Synthesizing assertions for large language model
                   pipelines",
  author        = "Shankar, Shreya and Li, Haotian and Asawa, Parth and
                   Hulsebos, Madelon and Lin, Yiming and Zamfirescu-Pereira, J
                   D and Chase, Harrison and Fu-Hinthorn, Will and
                   Parameswaran, Aditya G and Wu, Eugene",
  abstract      = "Operationalizing large language models (LLMs) for custom,
                   repetitive data pipelines is challenging, particularly due
                   to their unpredictable and potentially catastrophic
                   failures. Acknowledging the inevitability of these errors,
                   we focus on identifying when LLMs may be generating
                   incorrect responses when used repeatedly as part of data
                   generation pipelines. We present SPADE, a method for
                   automatically synthesizing assertions that identify bad LLM
                   outputs. SPADE analyzes prompt version histories to create
                   candidate assertion functions and then selects a minimal set
                   that fulfills both coverage and accuracy requirements. In
                   testing across nine different real-world LLM pipelines,
                   SPADE efficiently reduces the number of assertions by 14\%
                   and decreases false failures by 21\% when compared to
                   simpler baselines.",
  month         =  jan,
  year          =  2024,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2401.03038",
  primaryClass  = "cs.DB",
  arxivid       = "2401.03038"
}

@ARTICLE{Ridnik2024-fc,
  title         = "Code generation with {AlphaCodium}: From prompt engineering
                   to flow engineering",
  author        = "Ridnik, Tal and Kredo, Dedy and Friedman, Itamar",
  abstract      = "Code generation problems differ from common natural language
                   problems - they require matching the exact syntax of the
                   target language, identifying happy paths and edge cases,
                   paying attention to numerous small details in the problem
                   spec, and addressing other code-specific issues and
                   requirements. Hence, many of the optimizations and tricks
                   that have been successful in natural language generation may
                   not be effective for code tasks. In this work, we propose a
                   new approach to code generation by LLMs, which we call
                   AlphaCodium - a test-based, multi-stage, code-oriented
                   iterative flow, that improves the performances of LLMs on
                   code problems. We tested AlphaCodium on a challenging code
                   generation dataset called CodeContests, which includes
                   competitive programming problems from platforms such as
                   Codeforces. The proposed flow consistently and significantly
                   improves results. On the validation set, for example, GPT-4
                   accuracy (pass@5) increased from 19\% with a single
                   well-designed direct prompt to 44\% with the AlphaCodium
                   flow. Many of the principles and best practices acquired in
                   this work, we believe, are broadly applicable to general
                   code generation tasks. Full implementation is available at:
                   https://github.com/Codium-ai/AlphaCodium",
  month         =  jan,
  year          =  2024,
  keywords      = "Code",
  archivePrefix = "arXiv",
  eprint        = "2401.08500",
  primaryClass  = "cs.LG",
  arxivid       = "2401.08500"
}

@MISC{noauthor_undated-ni,
  title        = "{HarmBench}",
  abstract     = "A Standardized Evaluation Framework for Automated Red Teaming
                  and Robust Refusal",
  howpublished = "\url{https://www.harmbench.org/}",
  note         = "Accessed: 2024-2-8",
  keywords     = "Evaluation",
  language     = "en"
}

@MISC{noauthor_undated-ef,
  title        = "In others' words: Using large language models to accurately
                  analyze doctors' notes",
  booktitle    = "Johns Hopkins Whiting School of Engineering",
  abstract     = "Johns Hopkins and Columbia University computer scientists
                  improve real-world AI applications by combating inaccurate
                  correlations.",
  howpublished = "\url{https://engineering.jhu.edu/news/in-others-words-using-large-language-models-to-accurately-analyze-doctors-notes/}",
  note         = "Accessed: 2024-1-22",
  keywords     = "Health",
  language     = "en"
}

@ARTICLE{Rosenfeld2023-yy,
  title         = "Outliers with opposing signals have an outsized effect on
                   neural network optimization",
  author        = "Rosenfeld, Elan and Risteski, Andrej",
  abstract      = "We identify a new phenomenon in neural network optimization
                   which arises from the interaction of depth and a particular
                   heavy-tailed structure in natural data. Our result offers
                   intuitive explanations for several previously reported
                   observations about network training dynamics. In particular,
                   it implies a conceptually new cause for progressive
                   sharpening and the edge of stability; we also highlight
                   connections to other concepts in optimization and
                   generalization including grokking, simplicity bias, and
                   Sharpness-Aware Minimization. Experimentally, we demonstrate
                   the significant influence of paired groups of outliers in
                   the training data with strong opposing signals: consistent,
                   large magnitude features which dominate the network output
                   throughout training and provide gradients which point in
                   opposite directions. Due to these outliers, early
                   optimization enters a narrow valley which carefully balances
                   the opposing groups; subsequent sharpening causes their loss
                   to rise rapidly, oscillating between high on one group and
                   then the other, until the overall loss spikes. We describe
                   how to identify these groups, explore what sets them apart,
                   and carefully study their effect on the network's
                   optimization and behavior. We complement these experiments
                   with a mechanistic explanation on a toy example of opposing
                   signals and a theoretical analysis of a two-layer linear
                   network on a simple model. Our finding enables new
                   qualitative predictions of training behavior which we
                   confirm experimentally. It also provides a new lens through
                   which to study and improve modern training practices for
                   stochastic optimization, which we highlight via a case study
                   of Adam versus SGD.",
  month         =  nov,
  year          =  2023,
  keywords      = "Intriguing Properties",
  archivePrefix = "arXiv",
  eprint        = "2311.04163",
  primaryClass  = "cs.LG",
  arxivid       = "2311.04163"
}

@ARTICLE{Amatriain2024-hs,
  title         = "Prompt Design and Engineering: Introduction and Advanced
                   Methods",
  author        = "Amatriain, Xavier",
  abstract      = "Prompt design and engineering has become an important
                   discipline in just the past few months. In this paper, we
                   provide an introduction to the main concepts and design
                   approaches. We also provide more advanced techniques all the
                   way to those needed to design LLM-based agents. We finish by
                   providing a list of existing tools for prompt engineering.",
  month         =  jan,
  year          =  2024,
  keywords      = "In-Context",
  archivePrefix = "arXiv",
  eprint        = "2401.14423",
  primaryClass  = "cs.SE",
  arxivid       = "2401.14423"
}

@ARTICLE{Liu2024-bh,
  title         = "A survey on hallucination in Large {Vision-Language} Models",
  author        = "Liu, Hanchao and Xue, Wenyuan and Chen, Yifei and Chen,
                   Dapeng and Zhao, Xiutian and Wang, Ke and Hou, Liping and
                   Li, Rongjun and Peng, Wei",
  abstract      = "Recent development of Large Vision-Language Models (LVLMs)
                   has attracted growing attention within the AI landscape for
                   its practical implementation potential. However,
                   ``hallucination'', or more specifically, the misalignment
                   between factual visual content and corresponding textual
                   generation, poses a significant challenge of utilizing
                   LVLMs. In this comprehensive survey, we dissect LVLM-related
                   hallucinations in an attempt to establish an overview and
                   facilitate future mitigation. Our scrutiny starts with a
                   clarification of the concept of hallucinations in LVLMs,
                   presenting a variety of hallucination symptoms and
                   highlighting the unique challenges inherent in LVLM
                   hallucinations. Subsequently, we outline the benchmarks and
                   methodologies tailored specifically for evaluating
                   hallucinations unique to LVLMs. Additionally, we delve into
                   an investigation of the root causes of these hallucinations,
                   encompassing insights from the training data and model
                   components. We also critically review existing methods for
                   mitigating hallucinations. The open questions and future
                   directions pertaining to hallucinations within LVLMs are
                   discussed to conclude this survey.",
  journal       = "arXiv [cs.CV]",
  month         =  feb,
  year          =  2024,
  keywords      = "Hallucination",
  archivePrefix = "arXiv",
  eprint        = "2402.00253",
  primaryClass  = "cs.CV",
  arxivid       = "2402.00253"
}

@ARTICLE{Wang2024-tr,
  title         = "Mobile-agent: Autonomous multi-modal mobile device agent
                   with visual perception",
  author        = "Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Mingshi
                   and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang,
                   Jitao",
  abstract      = "Mobile device agent based on Multimodal Large Language
                   Models (MLLM) is becoming a popular application. In this
                   paper, we introduce Mobile-Agent, an autonomous multi-modal
                   mobile device agent. Mobile-Agent first leverages visual
                   perception tools to accurately identify and locate both the
                   visual and textual elements within the app's front-end
                   interface. Based on the perceived vision context, it then
                   autonomously plans and decomposes the complex operation
                   task, and navigates the mobile Apps through operations step
                   by step. Different from previous solutions that rely on XML
                   files of Apps or mobile system metadata, Mobile-Agent allows
                   for greater adaptability across diverse mobile operating
                   environments in a vision-centric way, thereby eliminating
                   the necessity for system-specific customizations. To assess
                   the performance of Mobile-Agent, we introduced Mobile-Eval,
                   a benchmark for evaluating mobile device operations. Based
                   on Mobile-Eval, we conducted a comprehensive evaluation of
                   Mobile-Agent. The experimental results indicate that
                   Mobile-Agent achieved remarkable accuracy and completion
                   rates. Even with challenging instructions, such as multi-app
                   operations, Mobile-Agent can still complete the
                   requirements. Code and model will be open-sourced at
                   https://github.com/X-PLUG/MobileAgent.",
  journal       = "arXiv [cs.CL]",
  month         =  jan,
  year          =  2024,
  keywords      = "Multimodal (Vision, speech, etc)",
  archivePrefix = "arXiv",
  eprint        = "2401.16158",
  primaryClass  = "cs.CL",
  arxivid       = "2401.16158"
}

@ARTICLE{Liu2024-ok,
  title         = "{ChatQA}: Building {GPT-4} Level Conversational {QA} Models",
  author        = "Liu, Zihan and Ping, Wei and Roy, Rajarshi and Xu, Peng and
                   Lee, Chankyu and Shoeybi, Mohammad and Catanzaro, Bryan",
  abstract      = "In this work, we introduce ChatQA, a family of
                   conversational question answering (QA) models that obtain
                   GPT-4 level accuracies. Specifically, we propose a two-stage
                   instruction tuning method that can significantly improve the
                   zero-shot conversational QA results from large language
                   models (LLMs). To handle retrieval-augmented generation in
                   conversational QA, we fine-tune a dense retriever on a
                   multi-turn QA dataset, which provides comparable results to
                   using the state-of-the-art query rewriting model while
                   largely reducing deployment cost. Notably, our ChatQA-70B
                   can outperform GPT-4 in terms of average score on 10
                   conversational QA datasets (54.14 vs. 53.90), without
                   relying on any synthetic data from OpenAI GPT models.",
  month         =  jan,
  year          =  2024,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2401.10225",
  primaryClass  = "cs.CL",
  arxivid       = "2401.10225"
}

@ARTICLE{Xiao2022-uv,
  title         = "Masked visual pre-training for motor control",
  author        = "Xiao, Tete and Radosavovic, Ilija and Darrell, Trevor and
                   Malik, Jitendra",
  abstract      = "This paper shows that self-supervised visual pre-training
                   from real-world images is effective for learning motor
                   control tasks from pixels. We first train the visual
                   representations by masked modeling of natural images. We
                   then freeze the visual encoder and train neural network
                   controllers on top with reinforcement learning. We do not
                   perform any task-specific fine-tuning of the encoder; the
                   same visual representations are used for all motor control
                   tasks. To the best of our knowledge, this is the first
                   self-supervised model to exploit real-world images at scale
                   for motor control. To accelerate progress in learning from
                   pixels, we contribute a benchmark suite of hand-designed
                   tasks varying in movements, scenes, and robots. Without
                   relying on labels, state-estimation, or expert
                   demonstrations, we consistently outperform supervised
                   encoders by up to 80\% absolute success rate, sometimes even
                   matching the oracle state performance. We also find that
                   in-the-wild images, e.g., from YouTube or Egocentric videos,
                   lead to better visual representations for various
                   manipulation tasks than ImageNet images.",
  journal       = "arXiv [cs.CV]",
  publisher     = "arXiv",
  month         =  mar,
  year          =  2022,
  keywords      = "Multimodal (Vision, speech, etc)",
  archivePrefix = "arXiv",
  eprint        = "2203.06173",
  primaryClass  = "cs.CV",
  arxivid       = "2203.06173",
  doi           = "10.48550/ARXIV.2203.06173"
}

@ARTICLE{Wu2022-cj,
  title         = "Memorizing Transformers",
  author        = "Wu, Yuhuai and Rabe, Markus N and Hutchins, Delesley and
                   Szegedy, Christian",
  abstract      = "Language models typically need to be trained or finetuned in
                   order to acquire new knowledge, which involves updating
                   their weights. We instead envision language models that can
                   simply read and memorize new data at inference time, thus
                   acquiring new knowledge immediately. In this work, we extend
                   language models with the ability to memorize the internal
                   representations of past inputs. We demonstrate that an
                   approximate kNN lookup into a non-differentiable memory of
                   recent (key, value) pairs improves language modeling across
                   various benchmarks and tasks, including generic webtext
                   (C4), math papers (arXiv), books (PG-19), code (Github), as
                   well as formal theorems (Isabelle). We show that the
                   performance steadily improves when we increase the size of
                   memory up to 262K tokens. On benchmarks including code and
                   mathematics, we find that the model is capable of making use
                   of newly defined functions and theorems during test time.",
  month         =  mar,
  year          =  2022,
  keywords      = "Models",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2203.08913",
  primaryClass  = "cs.LG",
  arxivid       = "2203.08913"
}

@ARTICLE{Burns2022-oa,
  title         = "Discovering latent knowledge in language models without
                   supervision",
  author        = "Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt,
                   Jacob",
  abstract      = "Existing techniques for training language models can be
                   misaligned with the truth: if we train models with imitation
                   learning, they may reproduce errors that humans make; if we
                   train them to generate text that humans rate highly, they
                   may output errors that human evaluators can't detect. We
                   propose circumventing this issue by directly finding latent
                   knowledge inside the internal activations of a language
                   model in a purely unsupervised way. Specifically, we
                   introduce a method for accurately answering yes-no questions
                   given only unlabeled model activations. It works by finding
                   a direction in activation space that satisfies logical
                   consistency properties, such as that a statement and its
                   negation have opposite truth values. We show that despite
                   using no supervision and no model outputs, our method can
                   recover diverse knowledge represented in large language
                   models: across 6 models and 10 question-answering datasets,
                   it outperforms zero-shot accuracy by 4\% on average. We also
                   find that it cuts prompt sensitivity in half and continues
                   to maintain high accuracy even when models are prompted to
                   generate incorrect answers. Our results provide an initial
                   step toward discovering what language models know, distinct
                   from what they say, even when we don't have access to
                   explicit ground truth labels.",
  month         =  dec,
  year          =  2022,
  keywords      = "Intriguing Properties",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2212.03827",
  primaryClass  = "cs.CL",
  arxivid       = "2212.03827"
}

@ARTICLE{Xi2023-pd,
  title         = "The rise and potential of large language model based agents:
                   A survey",
  author        = "Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and
                   Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang,
                   Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and
                   Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Liu, Qin
                   and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and
                   Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou,
                   Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi
                   and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and
                   Huan, Xuanjing and Gui, Tao",
  abstract      = "For a long time, humanity has pursued artificial
                   intelligence (AI) equivalent to or surpassing the human
                   level, with AI agents considered a promising vehicle for
                   this pursuit. AI agents are artificial entities that sense
                   their environment, make decisions, and take actions. Many
                   efforts have been made to develop intelligent AI agents
                   since the mid-20th century. However, these efforts have
                   mainly focused on advancement in algorithms or training
                   strategies to enhance specific capabilities or performance
                   on particular tasks. Actually, what the community lacks is a
                   sufficiently general and powerful model to serve as a
                   starting point for designing AI agents that can adapt to
                   diverse scenarios. Due to the versatile and remarkable
                   capabilities they demonstrate, large language models (LLMs)
                   are regarded as potential sparks for Artificial General
                   Intelligence (AGI), offering hope for building general AI
                   agents. Many research efforts have leveraged LLMs as the
                   foundation to build AI agents and have achieved significant
                   progress. We start by tracing the concept of agents from its
                   philosophical origins to its development in AI, and explain
                   why LLMs are suitable foundations for AI agents. Building
                   upon this, we present a conceptual framework for LLM-based
                   agents, comprising three main components: brain, perception,
                   and action, and the framework can be tailored to suit
                   different applications. Subsequently, we explore the
                   extensive applications of LLM-based agents in three aspects:
                   single-agent scenarios, multi-agent scenarios, and
                   human-agent cooperation. Following this, we delve into agent
                   societies, exploring the behavior and personality of
                   LLM-based agents, the social phenomena that emerge when they
                   form societies, and the insights they offer for human
                   society. Finally, we discuss a range of key topics and open
                   problems within the field.",
  month         =  sep,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  eprint        = "2309.07864",
  primaryClass  = "cs.AI",
  arxivid       = "2309.07864"
}

@ARTICLE{Von_Oswald2022-mb,
  title         = "Transformers learn in-context by gradient descent",
  author        = "von Oswald, Johannes and Niklasson, Eyvind and Randazzo,
                   Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander
                   and Zhmoginov, Andrey and Vladymyrov, Max",
  abstract      = "Transformers have become the state-of-the-art neural network
                   architecture across numerous domains of machine learning.
                   This is partly due to their celebrated ability to transfer
                   and to learn in-context based on few examples. Nevertheless,
                   the mechanisms by which Transformers become in-context
                   learners are not well understood and remain mostly an
                   intuition. Here, we argue that training Transformers on
                   auto-regressive tasks can be closely related to well-known
                   gradient-based meta-learning formulations. We start by
                   providing a simple weight construction that shows the
                   equivalence of data transformations induced by 1) a single
                   linear self-attention layer and by 2) gradient-descent (GD)
                   on a regression loss. Motivated by that construction, we
                   show empirically that when training self-attention-only
                   Transformers on simple regression tasks either the models
                   learned by GD and Transformers show great similarity or,
                   remarkably, the weights found by optimization match the
                   construction. Thus we show how trained Transformers
                   implement gradient descent in their forward pass. This
                   allows us, at least in the domain of regression problems, to
                   mechanistically understand the inner workings of optimized
                   Transformers that learn in-context. Furthermore, we identify
                   how Transformers surpass plain gradient descent by an
                   iterative curvature correction and learn linear models on
                   deep data representations to solve non-linear regression
                   tasks. Finally, we discuss intriguing parallels to a
                   mechanism identified to be crucial for in-context learning
                   termed induction-head (Olsson et al., 2022) and show how it
                   could be understood as a specific case of in-context
                   learning by gradient descent learning within Transformers.",
  month         =  dec,
  year          =  2022,
  keywords      = "LLMs",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2212.07677",
  primaryClass  = "cs.LG",
  arxivid       = "2212.07677"
}

@ARTICLE{Kirchenbauer2023-eh,
  title         = "A watermark for large language models",
  author        = "Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and
                   Katz, Jonathan and Miers, Ian and Goldstein, Tom",
  abstract      = "Potential harms of large language models can be mitigated by
                   watermarking model output, i.e., embedding signals into
                   generated text that are invisible to humans but
                   algorithmically detectable from a short span of tokens. We
                   propose a watermarking framework for proprietary language
                   models. The watermark can be embedded with negligible impact
                   on text quality, and can be detected using an efficient
                   open-source algorithm without access to the language model
                   API or parameters. The watermark works by selecting a
                   randomized set of ``green'' tokens before a word is
                   generated, and then softly promoting use of green tokens
                   during sampling. We propose a statistical test for detecting
                   the watermark with interpretable p-values, and derive an
                   information-theoretic framework for analyzing the
                   sensitivity of the watermark. We test the watermark using a
                   multi-billion parameter model from the Open Pretrained
                   Transformer (OPT) family, and discuss robustness and
                   security.",
  journal       = "arXiv [cs.LG]",
  publisher     = "arXiv",
  month         =  jan,
  year          =  2023,
  keywords      = "Watermarking",
  archivePrefix = "arXiv",
  eprint        = "2301.10226",
  primaryClass  = "cs.LG",
  arxivid       = "2301.10226",
  doi           = "10.48550/ARXIV.2301.10226"
}

@MISC{noauthor_undated-kv,
  title        = "Transformers\_and\_language\_v0.2.pdf",
  booktitle    = "Google Docs",
  howpublished = "\url{https://drive.google.com/file/d/1Z4raW0RRzGIvHeXtCRb0xjRil_4KBkWE/view?usp=drivesdk}",
  note         = "Accessed: 2023-4-20",
  keywords     = "LLMs"
}

@ARTICLE{Wu2023-ol,
  title         = "{BloombergGPT}: A Large Language Model for Finance",
  author        = "Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski,
                   Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur,
                   Prabhanjan and Rosenberg, David and Mann, Gideon",
  abstract      = "The use of NLP in the realm of financial technology is broad
                   and complex, with applications ranging from sentiment
                   analysis and named entity recognition to question answering.
                   Large Language Models (LLMs) have been shown to be effective
                   on a variety of tasks; however, no LLM specialized for the
                   financial domain has been reported in literature. In this
                   work, we present BloombergGPT, a 50 billion parameter
                   language model that is trained on a wide range of financial
                   data. We construct a 363 billion token dataset based on
                   Bloomberg's extensive data sources, perhaps the largest
                   domain-specific dataset yet, augmented with 345 billion
                   tokens from general purpose datasets. We validate
                   BloombergGPT on standard LLM benchmarks, open financial
                   benchmarks, and a suite of internal benchmarks that most
                   accurately reflect our intended usage. Our mixed dataset
                   training leads to a model that outperforms existing models
                   on financial tasks by significant margins without
                   sacrificing performance on general LLM benchmarks.
                   Additionally, we explain our modeling choices, training
                   process, and evaluation methodology. As a next step, we plan
                   to release training logs (Chronicles) detailing our
                   experience in training BloombergGPT.",
  month         =  mar,
  year          =  2023,
  keywords      = "Applications",
  archivePrefix = "arXiv",
  eprint        = "2303.17564",
  primaryClass  = "cs.LG",
  arxivid       = "2303.17564"
}

@ARTICLE{Pan2023-hc,
  title         = "Do the rewards justify the means? Measuring trade-offs
                   between rewards and ethical behavior in the {MACHIAVELLI}
                   benchmark",
  author        = "Pan, Alexander and Shern, Chan Jun and Zou, Andy and Li,
                   Nathaniel and Basart, Steven and Woodside, Thomas and Ng,
                   Jonathan and Zhang, Hanlin and Emmons, Scott and Hendrycks,
                   Dan",
  abstract      = "Artificial agents have traditionally been trained to
                   maximize reward, which may incentivize power-seeking and
                   deception, analogous to how next-token prediction in
                   language models (LMs) may incentivize toxicity. So do agents
                   naturally learn to be Machiavellian? And how do we measure
                   these behaviors in general-purpose models such as GPT-4?
                   Towards answering these questions, we introduce MACHIAVELLI,
                   a benchmark of 134 Choose-Your-Own-Adventure games
                   containing over half a million rich, diverse scenarios that
                   center on social decision-making. Scenario labeling is
                   automated with LMs, which are more performant than human
                   annotators. We mathematize dozens of harmful behaviors and
                   use our annotations to evaluate agents' tendencies to be
                   power-seeking, cause disutility, and commit ethical
                   violations. We observe some tension between maximizing
                   reward and behaving ethically. To improve this trade-off, we
                   investigate LM-based methods to steer agents' towards less
                   harmful behaviors. Our results show that agents can both act
                   competently and morally, so concrete progress can currently
                   be made in machine ethics--designing agents that are Pareto
                   improvements in both safety and capabilities.",
  month         =  apr,
  year          =  2023,
  keywords      = "Safety/Risks",
  archivePrefix = "arXiv",
  eprint        = "2304.03279",
  primaryClass  = "cs.LG",
  arxivid       = "2304.03279"
}

@ARTICLE{Olsson2018-ia,
  title         = "Skill rating for generative models",
  author        = "Olsson, Catherine and Bhupatiraju, Surya and Brown, Tom and
                   Odena, Augustus and Goodfellow, Ian",
  abstract      = "We explore a new way to evaluate generative models using
                   insights from evaluation of competitive games between human
                   players. We show experimentally that tournaments between
                   generators and discriminators provide an effective way to
                   evaluate generative models. We introduce two methods for
                   summarizing tournament outcomes: tournament win rate and
                   skill rating. Evaluations are useful in different contexts,
                   including monitoring the progress of a single model as it
                   learns during the training process, and comparing the
                   capabilities of two different fully trained models. We show
                   that a tournament consisting of a single model playing
                   against past and future versions of itself produces a useful
                   measure of training progress. A tournament containing
                   multiple separate models (using different seeds,
                   hyperparameters, and architectures) provides a useful
                   relative comparison between different trained GANs.
                   Tournament-based rating methods are conceptually distinct
                   from numerous previous categories of approaches to
                   evaluation of generative models, and have complementary
                   advantages and disadvantages.",
  month         =  aug,
  year          =  2018,
  keywords      = "Evaluation",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "1808.04888",
  primaryClass  = "stat.ML",
  arxivid       = "1808.04888"
}

@ARTICLE{Mu2023-sf,
  title         = "Learning to compress prompts with gist tokens",
  author        = "Mu, Jesse and Li, Xiang Lisa and Goodman, Noah",
  abstract      = "Prompting is now the primary way to utilize the multitask
                   capabilities of language models (LMs), but prompts occupy
                   valuable space in the input context window, and re-encoding
                   the same prompt is computationally inefficient. Finetuning
                   and distillation methods allow for specialization of LMs
                   without prompting, but require retraining the model for each
                   task. To avoid this trade-off entirely, we present gisting,
                   which trains an LM to compress prompts into smaller sets of
                   ``gist'' tokens which can be reused for compute efficiency.
                   Gist models can be easily trained as part of instruction
                   finetuning via a restricted attention mask that encourages
                   prompt compression. On decoder (LLaMA-7B) and
                   encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x
                   compression of prompts, resulting in up to 40\% FLOPs
                   reductions, 4.2\% wall time speedups, storage savings, and
                   minimal loss in output quality.",
  month         =  apr,
  year          =  2023,
  keywords      = "In-Context",
  archivePrefix = "arXiv",
  eprint        = "2304.08467",
  primaryClass  = "cs.CL",
  arxivid       = "2304.08467"
}

@ARTICLE{Hebenstreit2023-ek,
  title         = "An automatically discovered chain-of-thought prompt
                   generalizes to novel models and datasets",
  author        = "Hebenstreit, Konstantin and Praas, Robert and Kiesewetter,
                   Louis P and Samwald, Matthias",
  abstract      = "Emergent chain-of-thought (CoT) reasoning capabilities
                   promise to improve performance and explainability of large
                   language models (LLMs). However, uncertainties remain about
                   how prompting strategies formulated for previous model
                   generations generalize to new model generations and
                   different datasets. In this small-scale study we compare the
                   performance of a range of zero-shot prompts for inducing CoT
                   reasoning across six recently released LLMs (davinci-002,
                   davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere
                   command-xlarge) on a mixture of six question-answering
                   datasets, including datasets from scientific and medical
                   domains. We find that a CoT prompt that was previously
                   discovered through automated prompt discovery shows robust
                   performance across experimental conditions and produces best
                   results when applied to the state-of-the-art model GPT-4.",
  month         =  may,
  year          =  2023,
  keywords      = "Readling List;LLMs",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2305.02897",
  primaryClass  = "cs.CL",
  arxivid       = "2305.02897"
}

@ARTICLE{Nair2023-xq,
  title         = "{DERA}: Enhancing large language model completions with
                   dialog-enabled resolving agents",
  author        = "Nair, Varun and Schumacher, Elliot and Tso, Geoffrey and
                   Kannan, Anitha",
  abstract      = "Large language models (LLMs) have emerged as valuable tools
                   for many natural language understanding tasks. In
                   safety-critical applications such as healthcare, the utility
                   of these models is governed by their ability to generate
                   outputs that are factually accurate and complete. In this
                   work, we present dialog-enabled resolving agents (DERA).
                   DERA is a paradigm made possible by the increased
                   conversational abilities of LLMs, namely GPT-4. It provides
                   a simple, interpretable forum for models to communicate
                   feedback and iteratively improve output. We frame our dialog
                   as a discussion between two agent types - a Researcher, who
                   processes information and identifies crucial problem
                   components, and a Decider, who has the autonomy to integrate
                   the Researcher's information and makes judgments on the
                   final output. We test DERA against three clinically-focused
                   tasks. For medical conversation summarization and care plan
                   generation, DERA shows significant improvement over the base
                   GPT-4 performance in both human expert preference
                   evaluations and quantitative metrics. In a new finding, we
                   also show that GPT-4's performance (70\%) on an open-ended
                   version of the MedQA question-answering (QA) dataset (Jin et
                   al. 2021, USMLE) is well above the passing level (60\%),
                   with DERA showing similar performance. We release the
                   open-ended MEDQA dataset at
                   https://github.com/curai/curai-research/tree/main/DERA.",
  month         =  mar,
  year          =  2023,
  keywords      = "Readling List;LLMs",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2303.17071",
  primaryClass  = "cs.CL",
  arxivid       = "2303.17071"
}

@ARTICLE{Jiang2023-jq,
  title         = "Active retrieval augmented generation",
  author        = "Jiang, Zhengbao and Xu, Frank F and Gao, Luyu and Sun,
                   Zhiqing and Liu, Qian and Dwivedi-Yu, Jane and Yang, Yiming
                   and Callan, Jamie and Neubig, Graham",
  abstract      = "Despite the remarkable ability of large language models
                   (LMs) to comprehend and generate language, they have a
                   tendency to hallucinate and create factually inaccurate
                   output. Augmenting LMs by retrieving information from
                   external knowledge resources is one promising solution. Most
                   existing retrieval-augmented LMs employ a
                   retrieve-and-generate setup that only retrieves information
                   once based on the input. This is limiting, however, in more
                   general scenarios involving generation of long texts, where
                   continually gathering information throughout the generation
                   process is essential. There have been some past efforts to
                   retrieve information multiple times while generating
                   outputs, which mostly retrieve documents at fixed intervals
                   using the previous context as queries. In this work, we
                   provide a generalized view of active retrieval augmented
                   generation, methods that actively decide when and what to
                   retrieve across the course of the generation. We propose
                   Forward-Looking Active REtrieval augmented generation
                   (FLARE), a generic retrieval-augmented generation method
                   which iteratively uses a prediction of the upcoming sentence
                   to anticipate future content, which is then utilized as a
                   query to retrieve relevant documents to regenerate the
                   sentence if it contains low-confidence tokens. We test FLARE
                   along with baselines comprehensively over 4 long-form
                   knowledge-intensive generation tasks/datasets. FLARE
                   achieves superior or competitive performance on all tasks,
                   demonstrating the effectiveness of our method. Code and
                   datasets are available at https://github.com/jzbjyb/FLARE.",
  month         =  may,
  year          =  2023,
  keywords      = "NLP;LLMs;RAG",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2305.06983",
  primaryClass  = "cs.CL",
  arxivid       = "2305.06983"
}

@ARTICLE{Ahmadian2023-gi,
  title         = "Intriguing properties of quantization at scale",
  author        = "Ahmadian, Arash and Dash, Saurabh and Chen, Hongyu and
                   Venkitesh, Bharat and Gou, Stephen and Blunsom, Phil and
                   {\"U}st{\"u}n, Ahmet and Hooker, Sara",
  abstract      = "Emergent properties have been widely adopted as a term to
                   describe behavior not present in smaller models but observed
                   in larger models. Recent work suggests that the trade-off
                   incurred by quantization is also an emergent property, with
                   sharp drops in performance in models over 6B parameters. In
                   this work, we ask ``are quantization cliffs in performance
                   solely a factor of scale?'' Against a backdrop of increased
                   research focus on why certain emergent properties surface at
                   scale, this work provides a useful counter-example. We posit
                   that it is possible to optimize for a quantization friendly
                   training recipe that suppresses large activation magnitude
                   outliers. Here, we find that outlier dimensions are not an
                   inherent product of scale, but rather sensitive to the
                   optimization conditions present during pre-training. This
                   both opens up directions for more efficient quantization,
                   and poses the question of whether other emergent properties
                   are inherent or can be altered and conditioned by
                   optimization and architecture design choices. We
                   successfully quantize models ranging in size from 410M to
                   52B with minimal degradation in performance.",
  month         =  may,
  year          =  2023,
  keywords      = "Efficient;Intriguing Properties",
  archivePrefix = "arXiv",
  eprint        = "2305.19268",
  primaryClass  = "cs.LG",
  arxivid       = "2305.19268"
}

@ARTICLE{Dziri2023-ex,
  title         = "Faith and fate: Limits of Transformers on compositionality",
  author        = "Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang
                   Lorraine and Jiang, Liwei and Lin, Bill Yuchen and West,
                   Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang,
                   Jena D and Sanyal, Soumya and Welleck, Sean and Ren, Xiang
                   and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin",
  abstract      = "Transformer large language models (LLMs) have sparked
                   admiration for their exceptional performance on tasks that
                   demand intricate multi-step reasoning. Yet, these models
                   simultaneously show failures on surprisingly trivial
                   problems. This begs the question: Are these errors
                   incidental, or do they signal more substantial limitations?
                   In an attempt to demystify Transformers, we investigate the
                   limits of these models across three representative
                   compositional tasks -- multi-digit multiplication, logic
                   grid puzzles, and a classic dynamic programming problem.
                   These tasks require breaking problems down into sub-steps
                   and synthesizing these steps into a precise answer. We
                   formulate compositional tasks as computation graphs to
                   systematically quantify the level of complexity, and break
                   down reasoning steps into intermediate sub-procedures. Our
                   empirical findings suggest that Transformers solve
                   compositional tasks by reducing multi-step compositional
                   reasoning into linearized subgraph matching, without
                   necessarily developing systematic problem-solving skills. To
                   round off our empirical study, we provide theoretical
                   arguments on abstract multi-step reasoning problems that
                   highlight how Transformers' performance will rapidly decay
                   with increased task complexity.",
  month         =  may,
  year          =  2023,
  keywords      = "LLMs;Understanding",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2305.18654",
  primaryClass  = "cs.CL",
  arxivid       = "2305.18654"
}

@MISC{noauthor_undated-mf,
  title        = "Universal and transferable attacks on aligned language models",
  howpublished = "\url{https://llm-attacks.org/}",
  note         = "Accessed: 2024-1-13",
  keywords     = "Jailbreak",
  language     = "en"
}

@ARTICLE{Zhang2023-nm,
  title         = "Exploring the {MIT} Mathematics and {EECS} curriculum using
                   large language models",
  author        = "Zhang, Sarah J and Florin, Samuel and Lee, Ariel N and
                   Niknafs, Eamon and Marginean, Andrei and Wang, Annie and
                   Tyser, Keith and Chin, Zad and Hicke, Yann and Singh, Nikhil
                   and Udell, Madeleine and Kim, Yoon and Buonassisi, Tonio and
                   Solar-Lezama, Armando and Drori, Iddo",
  abstract      = "We curate a comprehensive dataset of 4,550 questions and
                   solutions from problem sets, midterm exams, and final exams
                   across all MIT Mathematics and Electrical Engineering and
                   Computer Science (EECS) courses required for obtaining a
                   degree. We evaluate the ability of large language models to
                   fulfill the graduation requirements for any MIT major in
                   Mathematics and EECS. Our results demonstrate that GPT-3.5
                   successfully solves a third of the entire MIT curriculum,
                   while GPT-4, with prompt engineering, achieves a perfect
                   solve rate on a test set excluding questions based on
                   images. We fine-tune an open-source large language model on
                   this dataset. We employ GPT-4 to automatically grade model
                   responses, providing a detailed performance breakdown by
                   course, question, and answer type. By embedding questions in
                   a low-dimensional space, we explore the relationships
                   between questions, topics, and classes and discover which
                   questions and classes are required for solving other
                   questions and classes through few-shot learning. Our
                   analysis offers valuable insights into course prerequisites
                   and curriculum design, highlighting language models'
                   potential for learning and improving Mathematics and EECS
                   education.",
  month         =  jun,
  year          =  2023,
  keywords      = "Models",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2306.08997",
  primaryClass  = "cs.CL",
  arxivid       = "2306.08997"
}

@ARTICLE{Mirchandani2023-kp,
  title         = "Large language models as general pattern machines",
  author        = "Mirchandani, Suvir and Xia, Fei and Florence, Pete and
                   Ichter, Brian and Driess, Danny and Arenas, Montserrat
                   Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy",
  abstract      = "We observe that pre-trained large language models (LLMs) are
                   capable of autoregressively completing complex token
                   sequences -- from arbitrary ones procedurally generated by
                   probabilistic context-free grammars (PCFG), to more rich
                   spatial patterns found in the Abstract Reasoning Corpus
                   (ARC), a general AI benchmark, prompted in the style of
                   ASCII art. Surprisingly, pattern completion proficiency can
                   be partially retained even when the sequences are expressed
                   using tokens randomly sampled from the vocabulary. These
                   results suggest that without any additional training, LLMs
                   can serve as general sequence modelers, driven by in-context
                   learning. In this work, we investigate how these zero-shot
                   capabilities may be applied to problems in robotics -- from
                   extrapolating sequences of numbers that represent states
                   over time to complete simple motions, to least-to-most
                   prompting of reward-conditioned trajectories that can
                   discover and represent closed-loop policies (e.g., a
                   stabilizing controller for CartPole). While difficult to
                   deploy today for real systems due to latency, context size
                   limitations, and compute costs, the approach of using LLMs
                   to drive low-level control may provide an exciting glimpse
                   into how the patterns among words could be transferred to
                   actions.",
  month         =  jul,
  year          =  2023,
  keywords      = "LLMs",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2307.04721",
  primaryClass  = "cs.AI",
  arxivid       = "2307.04721"
}

@ARTICLE{Guo2024-oo,
  title         = "Large language model based multi-agents: A survey of
                   progress and challenges",
  author        = "Guo, Taicheng and Chen, Xiuying and Wang, Yaqi and Chang,
                   Ruidi and Pei, Shichao and Chawla, Nitesh V and Wiest, Olaf
                   and Zhang, Xiangliang",
  abstract      = "Large Language Models (LLMs) have achieved remarkable
                   success across a wide array of tasks. Due to the impressive
                   planning and reasoning abilities of LLMs, they have been
                   used as autonomous agents to do many tasks automatically.
                   Recently, based on the development of using one LLM as a
                   single planning or decision-making agent, LLM-based
                   multi-agent systems have achieved considerable progress in
                   complex problem-solving and world simulation. To provide the
                   community with an overview of this dynamic field, we present
                   this survey to offer an in-depth discussion on the essential
                   aspects of multi-agent systems based on LLMs, as well as the
                   challenges. Our goal is for readers to gain substantial
                   insights on the following questions: What domains and
                   environments do LLM-based multi-agents simulate? How are
                   these agents profiled and how do they communicate? What
                   mechanisms contribute to the growth of agents' capacities?
                   For those interested in delving into this field of study, we
                   also summarize the commonly used datasets or benchmarks for
                   them to have convenient access. To keep researchers updated
                   on the latest studies, we maintain an open-source GitHub
                   repository, dedicated to outlining the research on LLM-based
                   multi-agent systems.",
  month         =  jan,
  year          =  2024,
  keywords      = "Survey/Review Paper;LLMs",
  archivePrefix = "arXiv",
  eprint        = "2402.01680",
  primaryClass  = "cs.CL",
  arxivid       = "2402.01680"
}

@ARTICLE{Habicht2024-qa,
  title     = "Closing the accessibility gap to mental health treatment with a
               personalized self-referral chatbot",
  author    = "Habicht, Johanna and Viswanathan, Sruthi and Carrington, Ben and
               Hauser, Tobias U and Harper, Ross and Rollwage, Max",
  abstract  = "Inequality in treatment access is a pressing issue in most
               healthcare systems across many medical disciplines. In mental
               healthcare, reduced treatment access for minorities is
               ubiquitous but remedies are sparse. Here we demonstrate that
               digital tools can reduce the accessibility gap by addressing
               several key barriers. In a multisite observational study of
               129,400 patients within England's NHS services, we evaluated the
               impact of a personalized artificial intelligence-enabled
               self-referral chatbot on patient referral volume and diversity
               in ethnicity, gender and sexual orientation. We found that
               services that used this digital solution identified
               substantially increased referrals (15\% increase versus 6\%
               increase in control services). Critically, this increase was
               particularly pronounced in minorities, such as nonbinary (179\%
               increase) and ethnic minority individuals (29\% increase). Using
               natural language processing to analyze qualitative feedback from
               42,332 individuals, we found that the chatbot's human-free
               nature and the patients' self-realization of their need for
               treatment were potential drivers for the observed improvement in
               the diversity of access. This provides strong evidence that
               digital tools may help overcome the pervasive inequality in
               mental healthcare. Implementation of a self-referral chatbot for
               mental healthcare services increased access substantially in a
               large multicenter study, with a much larger effect on minority
               individuals.",
  journal   = "Nat. Med.",
  publisher = "Springer Science and Business Media LLC",
  pages     = "1--8",
  month     =  feb,
  year      =  2024,
  keywords  = "Applications",
  copyright = "https://www.springernature.com/gp/researchers/text-and-data-mining",
  language  = "en",
  issn      = "1078-8956, 1546-170X",
  doi       = "10.1038/s41591-023-02766-x"
}

@ARTICLE{Khaddaj2023-hv,
  title         = "Rethinking backdoor attacks",
  author        = "Khaddaj, Alaa and Leclerc, Guillaume and Makelov, Aleksandar
                   and Georgiev, Kristian and Salman, Hadi and Ilyas, Andrew
                   and Madry, Aleksander",
  abstract      = "In a backdoor attack, an adversary inserts maliciously
                   constructed backdoor examples into a training set to make
                   the resulting model vulnerable to manipulation. Defending
                   against such attacks typically involves viewing these
                   inserted examples as outliers in the training set and using
                   techniques from robust statistics to detect and remove them.
                   In this work, we present a different approach to the
                   backdoor attack problem. Specifically, we show that without
                   structural information about the training data distribution,
                   backdoor attacks are indistinguishable from
                   naturally-occurring features in the data--and thus
                   impossible to ``detect'' in a general sense. Then, guided by
                   this observation, we revisit existing defenses against
                   backdoor attacks and characterize the (often latent)
                   assumptions they make and on which they depend. Finally, we
                   explore an alternative perspective on backdoor attacks: one
                   that assumes these attacks correspond to the strongest
                   feature in the training data. Under this assumption (which
                   we make formal) we develop a new primitive for detecting
                   backdoor attacks. Our primitive naturally gives rise to a
                   detection algorithm that comes with theoretical guarantees
                   and is effective in practice.",
  month         =  jul,
  year          =  2023,
  keywords      = "Safety/Risks",
  archivePrefix = "arXiv",
  eprint        = "2307.10163",
  primaryClass  = "cs.CR",
  arxivid       = "2307.10163"
}

@ARTICLE{Gu2023-ga,
  title         = "A systematic survey of prompt engineering on vision-language
                   foundation models",
  author        = "Gu, Jindong and Han, Zhen and Chen, Shuo and Beirami, Ahmad
                   and He, Bailan and Zhang, Gengyuan and Liao, Ruotong and
                   Qin, Yao and Tresp, Volker and Torr, Philip",
  abstract      = "Prompt engineering is a technique that involves augmenting a
                   large pre-trained model with task-specific hints, known as
                   prompts, to adapt the model to new tasks. Prompts can be
                   created manually as natural language instructions or
                   generated automatically as either natural language
                   instructions or vector representations. Prompt engineering
                   enables the ability to perform predictions based solely on
                   prompts without updating model parameters, and the easier
                   application of large pre-trained models in real-world tasks.
                   In past years, Prompt engineering has been well-studied in
                   natural language processing. Recently, it has also been
                   intensively studied in vision-language modeling. However,
                   there is currently a lack of a systematic overview of prompt
                   engineering on pre-trained vision-language models. This
                   paper aims to provide a comprehensive survey of cutting-edge
                   research in prompt engineering on three types of
                   vision-language models: multimodal-to-text generation models
                   (e.g. Flamingo), image-text matching models (e.g. CLIP), and
                   text-to-image generation models (e.g. Stable Diffusion). For
                   each type of model, a brief model summary, prompting
                   methods, prompting-based applications, and the corresponding
                   responsibility and integrity issues are summarized and
                   discussed. Furthermore, the commonalities and differences
                   between prompting on vision-language models, language
                   models, and vision models are also discussed. The
                   challenges, future directions, and research opportunities
                   are summarized to foster future research on this topic.",
  month         =  jul,
  year          =  2023,
  keywords      = "In-Context",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2307.12980",
  primaryClass  = "cs.CV",
  arxivid       = "2307.12980"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{noauthor_undated-ms,
  title       = "peft: 🤗 {PEFT}: State-of-the-art {Parameter-Efficient}
                 {Fine-Tuning}",
  abstract    = "🤗 PEFT: State-of-the-art Parameter-Efficient Fine-Tuning. -
                 huggingface/peft: 🤗 PEFT: State-of-the-art
                 Parameter-Efficient Fine-Tuning.",
  institution = "Github",
  keywords    = "Public Code;Survey/Review Paper;Fine-tuning",
  language    = "en"
}

@MISC{noauthor_undated-im,
  title       = "{RL4LMs}: A modular {RL} library to fine-tune language models
                 to human preferences",
  abstract    = "A modular RL library to fine-tune language models to human
                 preferences - allenai/RL4LMs: A modular RL library to
                 fine-tune language models to human preferences",
  institution = "Github",
  keywords    = "Public Code;Fine-tuning;Alignment (RLHF, etc)",
  language    = "en"
}

@MISC{Seita_undated-pe,
  title        = "Koala: A Dialogue Model for Academic Research",
  author       = "Seita, Daniel and {Xinyang Geng, Arnav Gudibande, Hao Liu,
                  Eric Wallace, Pieter Abbeel, Sergey Levine, Dawn Song}",
  abstract     = "The BAIR Blog",
  howpublished = "\url{https://bair.berkeley.edu/blog/2023/04/03/koala/}",
  note         = "Accessed: 2023-7-26",
  keywords     = "LLMs"
}

@MISC{Chiang2023-pe,
  title    = "Vicuna: An {Open-Source} Chatbot Impressing {GPT-4} with 90\%*
              {ChatGPT} Quality",
  author   = "Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and
              Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan
              and Zhuang, Yonghao and Gonzalez, Joseph E and Stoica, Ion and
              Xing, Eric P",
  month    =  mar,
  year     =  2023,
  keywords = "LLMs"
}

@ARTICLE{Lin2023-wx,
  title         = "Learning to Model the World with Language",
  author        = "Lin, Jessy and Du, Yuqing and Watkins, Olivia and Hafner,
                   Danijar and Abbeel, Pieter and Klein, Dan and Dragan, Anca",
  abstract      = "To interact with humans in the world, agents need to
                   understand the diverse types of language that people use,
                   relate them to the visual world, and act based on them.
                   While current agents learn to execute simple language
                   instructions from task rewards, we aim to build agents that
                   leverage diverse language that conveys general knowledge,
                   describes the state of the world, provides interactive
                   feedback, and more. Our key idea is that language helps
                   agents predict the future: what will be observed, how the
                   world will behave, and which situations will be rewarded.
                   This perspective unifies language understanding with future
                   prediction as a powerful self-supervised learning objective.
                   We present Dynalang, an agent that learns a multimodal world
                   model that predicts future text and image representations
                   and learns to act from imagined model rollouts. Unlike
                   traditional agents that use language only to predict
                   actions, Dynalang acquires rich language understanding by
                   using past language also to predict future language, video,
                   and rewards. In addition to learning from online interaction
                   in an environment, Dynalang can be pretrained on datasets of
                   text, video, or both without actions or rewards. From using
                   language hints in grid worlds to navigating photorealistic
                   scans of homes, Dynalang utilizes diverse types of language
                   to improve task performance, including environment
                   descriptions, game rules, and instructions.",
  month         =  jul,
  year          =  2023,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2308.01399",
  primaryClass  = "cs.CL",
  arxivid       = "2308.01399"
}

@ARTICLE{Carlini2023-do,
  title         = "Poisoning {Web-Scale} Training Datasets is Practical",
  author        = "Carlini, Nicholas and Jagielski, Matthew and Choquette-Choo,
                   Christopher A and Paleka, Daniel and Pearce, Will and
                   Anderson, Hyrum and Terzis, Andreas and Thomas, Kurt and
                   Tram{\`e}r, Florian",
  abstract      = "Deep learning models are often trained on distributed,
                   webscale datasets crawled from the internet. In this paper,
                   we introduce two new dataset poisoning attacks that
                   intentionally introduce malicious examples to a model's
                   performance. Our attacks are immediately practical and
                   could, today, poison 10 popular datasets. Our first attack,
                   split-view poisoning, exploits the mutable nature of
                   internet content to ensure a dataset annotator's initial
                   view of the dataset differs from the view downloaded by
                   subsequent clients. By exploiting specific invalid trust
                   assumptions, we show how we could have poisoned 0.01\% of
                   the LAION-400M or COYO-700M datasets for just \$60 USD. Our
                   second attack, frontrunning poisoning, targets web-scale
                   datasets that periodically snapshot crowd-sourced content --
                   such as Wikipedia -- where an attacker only needs a
                   time-limited window to inject malicious examples. In light
                   of both attacks, we notify the maintainers of each affected
                   dataset and recommended several low-overhead defenses.",
  month         =  feb,
  year          =  2023,
  keywords      = "Data Poison",
  archivePrefix = "arXiv",
  eprint        = "2302.10149",
  primaryClass  = "cs.CR",
  arxivid       = "2302.10149"
}

@ARTICLE{Peng2023-qt,
  title         = "{RWKV}: Reinventing {RNNs} for the Transformer Era",
  author        = "Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak,
                   Alon and Arcadinho, Samuel and Cao, Huanqi and Cheng, Xin
                   and Chung, Michael and Grella, Matteo and Gv, Kranthi Kiran
                   and He, Xuzheng and Hou, Haowen and Kazienko, Przemyslaw and
                   Kocon, Jan and Kong, Jiaming and Koptyra, Bartlomiej and
                   Lau, Hayden and Mantri, Krishna Sri Ipsit and Mom, Ferdinand
                   and Saito, Atsushi and Tang, Xiangru and Wang, Bolun and
                   Wind, Johan S and Wozniak, Stansilaw and Zhang, Ruichong and
                   Zhang, Zhenyuan and Zhao, Qihang and Zhou, Peng and Zhu,
                   Jian and Zhu, Rui-Jie",
  abstract      = "Transformers have revolutionized almost all natural language
                   processing (NLP) tasks but suffer from memory and
                   computational complexity that scales quadratically with
                   sequence length. In contrast, recurrent neural networks
                   (RNNs) exhibit linear scaling in memory and computational
                   requirements but struggle to match the same performance as
                   Transformers due to limitations in parallelization and
                   scalability. We propose a novel model architecture,
                   Receptance Weighted Key Value (RWKV), that combines the
                   efficient parallelizable training of Transformers with the
                   efficient inference of RNNs. Our approach leverages a linear
                   attention mechanism and allows us to formulate the model as
                   either a Transformer or an RNN, which parallelizes
                   computations during training and maintains constant
                   computational and memory complexity during inference,
                   leading to the first non-transformer architecture to be
                   scaled to tens of billions of parameters. Our experiments
                   reveal that RWKV performs on par with similarly sized
                   Transformers, suggesting that future work can leverage this
                   architecture to create more efficient models. This work
                   presents a significant step towards reconciling the
                   trade-offs between computational efficiency and model
                   performance in sequence processing tasks.",
  month         =  may,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2305.13048",
  primaryClass  = "cs.CL",
  arxivid       = "2305.13048"
}

@ARTICLE{Rafailov2023-tj,
  title         = "Direct Preference Optimization: Your Language Model is
                   Secretly a Reward Model",
  author        = "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and
                   Ermon, Stefano and Manning, Christopher D and Finn, Chelsea",
  abstract      = "While large-scale unsupervised language models (LMs) learn
                   broad world knowledge and some reasoning skills, achieving
                   precise control of their behavior is difficult due to the
                   completely unsupervised nature of their training. Existing
                   methods for gaining such steerability collect human labels
                   of the relative quality of model generations and fine-tune
                   the unsupervised LM to align with these preferences, often
                   with reinforcement learning from human feedback (RLHF).
                   However, RLHF is a complex and often unstable procedure,
                   first fitting a reward model that reflects the human
                   preferences, and then fine-tuning the large unsupervised LM
                   using reinforcement learning to maximize this estimated
                   reward without drifting too far from the original model. In
                   this paper, we leverage a mapping between reward functions
                   and optimal policies to show that this constrained reward
                   maximization problem can be optimized exactly with a single
                   stage of policy training, essentially solving a
                   classification problem on the human preference data. The
                   resulting algorithm, which we call Direct Preference
                   Optimization (DPO), is stable, performant and
                   computationally lightweight, eliminating the need for
                   fitting a reward model, sampling from the LM during
                   fine-tuning, or performing significant hyperparameter
                   tuning. Our experiments show that DPO can fine-tune LMs to
                   align with human preferences as well as or better than
                   existing methods. Notably, fine-tuning with DPO exceeds
                   RLHF's ability to control sentiment of generations and
                   improves response quality in summarization and single-turn
                   dialogue while being substantially simpler to implement and
                   train.",
  month         =  may,
  year          =  2023,
  keywords      = "Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "2305.18290",
  primaryClass  = "cs.LG",
  arxivid       = "2305.18290"
}

@ARTICLE{Sun2023-xy,
  title         = "{Principle-Driven} {Self-Alignment} of Language Models from
                   Scratch with Minimal Human Supervision",
  author        = "Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang,
                   Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming
                   and Gan, Chuang",
  abstract      = "Recent AI-assistant agents, such as ChatGPT, predominantly
                   rely on supervised fine-tuning (SFT) with human annotations
                   and reinforcement learning from human feedback (RLHF) to
                   align the output of large language models (LLMs) with human
                   intentions, ensuring they are helpful, ethical, and
                   reliable. However, this dependence can significantly
                   constrain the true potential of AI-assistant agents due to
                   the high cost of obtaining human supervision and the related
                   issues on quality, reliability, diversity, self-consistency,
                   and undesirable biases. To address these challenges, we
                   propose a novel approach called SELF-ALIGN, which combines
                   principle-driven reasoning and the generative power of LLMs
                   for the self-alignment of AI agents with minimal human
                   supervision. Our approach encompasses four stages: first, we
                   use an LLM to generate synthetic prompts, and a topic-guided
                   method to augment the prompt diversity; second, we use a
                   small set of human-written principles for AI models to
                   follow, and guide the LLM through in-context learning from
                   demonstrations (of principles application) to produce
                   helpful, ethical, and reliable responses to user's queries;
                   third, we fine-tune the original LLM with the high-quality
                   self-aligned responses so that the resulting model can
                   generate desirable responses for each query directly without
                   the principle set and the demonstrations anymore; and
                   finally, we offer a refinement step to address the issues of
                   overly-brief or indirect responses. Applying SELF-ALIGN to
                   the LLaMA-65b base language model, we develop an AI
                   assistant named Dromedary. With fewer than 300 lines of
                   human annotations (including < 200 seed prompts, 16 generic
                   principles, and 5 exemplars for in-context learning).
                   Dromedary significantly surpasses the performance of several
                   state-of-the-art AI systems, including Text-Davinci-003 and
                   Alpaca, on benchmark datasets with various settings.",
  month         =  may,
  year          =  2023,
  keywords      = "Safety/Risks",
  archivePrefix = "arXiv",
  eprint        = "2305.03047",
  primaryClass  = "cs.LG",
  arxivid       = "2305.03047"
}

@ARTICLE{Biderman2023-rz,
  title         = "Pythia: A Suite for Analyzing Large Language Models Across
                   Training and Scaling",
  author        = "Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin
                   and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and
                   Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth,
                   Usvsn Sai and Raff, Edward and Skowron, Aviya and Sutawika,
                   Lintang and van der Wal, Oskar",
  abstract      = "How do large language models (LLMs) develop and evolve over
                   the course of training? How do these patterns change as
                   models scale? To answer these questions, we introduce
                   \textbackslashtextit\{Pythia\}, a suite of 16 LLMs all
                   trained on public data seen in the exact same order and
                   ranging in size from 70M to 12B parameters. We provide
                   public access to 154 checkpoints for each one of the 16
                   models, alongside tools to download and reconstruct their
                   exact training dataloaders for further study. We intend
                   \textbackslashtextit\{Pythia\} to facilitate research in
                   many areas, and we present several case studies including
                   novel results in memorization, term frequency effects on
                   few-shot performance, and reducing gender bias. We
                   demonstrate that this highly controlled setup can be used to
                   yield novel insights toward LLMs and their training
                   dynamics. Trained models, analysis code, training code, and
                   training data can be found at
                   \textbackslashurl\{https://github.com/EleutherAI/pythia\}.",
  month         =  apr,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2304.01373",
  primaryClass  = "cs.CL",
  arxivid       = "2304.01373"
}

@ARTICLE{Huang2023-ju,
  title         = "Language Is Not All You Need: Aligning Perception with
                   Language Models",
  author        = "Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru
                   and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and
                   Cui, Lei and Mohammed, Owais Khan and Patra, Barun and Liu,
                   Qiang and Aggarwal, Kriti and Chi, Zewen and Bjorck, Johan
                   and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and
                   Wei, Furu",
  abstract      = "A big convergence of language, multimodal perception,
                   action, and world modeling is a key step toward artificial
                   general intelligence. In this work, we introduce Kosmos-1, a
                   Multimodal Large Language Model (MLLM) that can perceive
                   general modalities, learn in context (i.e., few-shot), and
                   follow instructions (i.e., zero-shot). Specifically, we
                   train Kosmos-1 from scratch on web-scale multimodal corpora,
                   including arbitrarily interleaved text and images,
                   image-caption pairs, and text data. We evaluate various
                   settings, including zero-shot, few-shot, and multimodal
                   chain-of-thought prompting, on a wide range of tasks without
                   any gradient updates or finetuning. Experimental results
                   show that Kosmos-1 achieves impressive performance on (i)
                   language understanding, generation, and even OCR-free NLP
                   (directly fed with document images), (ii)
                   perception-language tasks, including multimodal dialogue,
                   image captioning, visual question answering, and (iii)
                   vision tasks, such as image recognition with descriptions
                   (specifying classification via text instructions). We also
                   show that MLLMs can benefit from cross-modal transfer, i.e.,
                   transfer knowledge from language to multimodal, and from
                   multimodal to language. In addition, we introduce a dataset
                   of Raven IQ test, which diagnoses the nonverbal reasoning
                   capability of MLLMs.",
  month         =  feb,
  year          =  2023,
  keywords      = "Multimodal (Vision, speech, etc)",
  archivePrefix = "arXiv",
  eprint        = "2302.14045",
  primaryClass  = "cs.CL",
  arxivid       = "2302.14045"
}

@ARTICLE{Longpre2023-nc,
  title         = "The Flan Collection: Designing data and methods for
                   effective instruction tuning",
  author        = "Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert
                   and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le,
                   Quoc V and Zoph, Barret and Wei, Jason and Roberts, Adam",
  abstract      = "We study the design decisions of publicly available
                   instruction tuning methods, and break down the development
                   of Flan 2022 (Chung et al., 2022). Through careful ablation
                   studies on the Flan Collection of tasks and methods, we
                   tease apart the effect of design decisions which enable
                   Flan-T5 to outperform prior work by 3-17\%+ across
                   evaluation settings. We find task balancing and enrichment
                   techniques are overlooked but critical to effective
                   instruction tuning, and in particular, training with mixed
                   prompt settings (zero-shot, few-shot, and chain-of-thought)
                   actually yields stronger (2\%+) performance in all settings.
                   In further experiments, we show Flan-T5 requires less
                   finetuning to converge higher and faster than T5 on single
                   downstream tasks, motivating instruction-tuned models as
                   more computationally-efficient starting checkpoints for new
                   tasks. Finally, to accelerate research on instruction
                   tuning, we make the Flan 2022 collection of datasets,
                   templates, and methods publicly available at
                   https://github.com/google-research/FLAN/tree/main/flan/v2.",
  month         =  jan,
  year          =  2023,
  keywords      = "Fine-tuning",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2301.13688",
  primaryClass  = "cs.AI",
  arxivid       = "2301.13688"
}

@ARTICLE{Goyal2022-ay,
  title         = "News Summarization and Evaluation in the Era of {GPT-3}",
  author        = "Goyal, Tanya and Li, Junyi Jessy and Durrett, Greg",
  abstract      = "The recent success of prompting large language models like
                   GPT-3 has led to a paradigm shift in NLP research. In this
                   paper, we study its impact on text summarization, focusing
                   on the classic benchmark domain of news summarization.
                   First, we investigate how GPT-3 compares against fine-tuned
                   models trained on large summarization datasets. We show that
                   not only do humans overwhelmingly prefer GPT-3 summaries,
                   prompted using only a task description, but these also do
                   not suffer from common dataset-specific issues such as poor
                   factuality. Next, we study what this means for evaluation,
                   particularly the role of gold standard test sets. Our
                   experiments show that both reference-based and
                   reference-free automatic metrics cannot reliably evaluate
                   GPT-3 summaries. Finally, we evaluate models on a setting
                   beyond generic summarization, specifically keyword-based
                   summarization, and show how dominant fine-tuning approaches
                   compare to prompting. To support further research, we
                   release: (a) a corpus of 10K generated summaries from
                   fine-tuned and prompt-based models across 4 standard
                   summarization benchmarks, (b) 1K human preference judgments
                   comparing different systems for generic- and keyword-based
                   summarization.",
  month         =  sep,
  year          =  2022,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2209.12356",
  primaryClass  = "cs.CL",
  arxivid       = "2209.12356"
}

@ARTICLE{Guo2023-rj,
  title         = "How Close is {ChatGPT} to Human Experts? Comparison Corpus,
                   Evaluation, and Detection",
  author        = "Guo, Biyang and Zhang, Xin and Wang, Ziyuan and Jiang, Minqi
                   and Nie, Jinran and Ding, Yuxuan and Yue, Jianwei and Wu,
                   Yupeng",
  abstract      = "The introduction of ChatGPT has garnered widespread
                   attention in both academic and industrial communities.
                   ChatGPT is able to respond effectively to a wide range of
                   human questions, providing fluent and comprehensive answers
                   that significantly surpass previous public chatbots in terms
                   of security and usefulness. On one hand, people are curious
                   about how ChatGPT is able to achieve such strength and how
                   far it is from human experts. On the other hand, people are
                   starting to worry about the potential negative impacts that
                   large language models (LLMs) like ChatGPT could have on
                   society, such as fake news, plagiarism, and social security
                   issues. In this work, we collected tens of thousands of
                   comparison responses from both human experts and ChatGPT,
                   with questions ranging from open-domain, financial, medical,
                   legal, and psychological areas. We call the collected
                   dataset the Human ChatGPT Comparison Corpus (HC3). Based on
                   the HC3 dataset, we study the characteristics of ChatGPT's
                   responses, the differences and gaps from human experts, and
                   future directions for LLMs. We conducted comprehensive human
                   evaluations and linguistic analyses of ChatGPT-generated
                   content compared with that of humans, where many interesting
                   results are revealed. After that, we conduct extensive
                   experiments on how to effectively detect whether a certain
                   text is generated by ChatGPT or humans. We build three
                   different detection systems, explore several key factors
                   that influence their effectiveness, and evaluate them in
                   different scenarios. The dataset, code, and models are all
                   publicly available at
                   https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.",
  month         =  jan,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2301.07597",
  primaryClass  = "cs.CL",
  arxivid       = "2301.07597"
}

@ARTICLE{Jiao2023-sg,
  title         = "Is {ChatGPT} A Good Translator? Yes With {GPT-4} As The
                   Engine",
  author        = "Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-Tse and
                   Wang, Xing and Tu, Zhaopeng",
  abstract      = "This report provides a preliminary evaluation of ChatGPT for
                   machine translation, including translation prompt,
                   multilingual translation, and translation robustness. We
                   adopt the prompts advised by ChatGPT to trigger its
                   translation ability and find that the candidate prompts
                   generally work well and show minor performance differences.
                   By evaluating on a number of benchmark test sets, we find
                   that ChatGPT performs competitively with commercial
                   translation products (e.g., Google Translate) on
                   high-resource European languages but lags behind
                   significantly on low-resource or distant languages. For
                   distant languages, we explore an interesting strategy named
                   $\mathbf\{pivot~prompting\}$ that asks ChatGPT to translate
                   the source sentence into a high-resource pivot language
                   before into the target language, which improves the
                   translation performance significantly. As for the
                   translation robustness, ChatGPT does not perform as well as
                   the commercial systems on biomedical abstracts or Reddit
                   comments but exhibits good results on spoken language. With
                   the launch of the GPT-4 engine, the translation performance
                   of ChatGPT is significantly boosted, becoming comparable to
                   commercial translation products, even for distant languages.
                   In other words,
                   $\mathbf\{ChatGPT~has~already~become~a~good~translator!\}$
                   Scripts and data:
                   https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator",
  month         =  jan,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2301.08745",
  primaryClass  = "cs.CL",
  arxivid       = "2301.08745"
}

@ARTICLE{Zhang2023-zf,
  title         = "Benchmarking Large Language Models for News Summarization",
  author        = "Zhang, Tianyi and Ladhak, Faisal and Durmus, Esin and Liang,
                   Percy and McKeown, Kathleen and Hashimoto, Tatsunori B",
  abstract      = "Large language models (LLMs) have shown promise for
                   automatic summarization but the reasons behind their
                   successes are poorly understood. By conducting a human
                   evaluation on ten LLMs across different pretraining methods,
                   prompts, and model scales, we make two important
                   observations. First, we find instruction tuning, and not
                   model size, is the key to the LLM's zero-shot summarization
                   capability. Second, existing studies have been limited by
                   low-quality references, leading to underestimates of human
                   performance and lower few-shot and finetuning performance.
                   To better evaluate LLMs, we perform human evaluation over
                   high-quality summaries we collect from freelance writers.
                   Despite major stylistic differences such as the amount of
                   paraphrasing, we find that LMM summaries are judged to be on
                   par with human written summaries.",
  month         =  jan,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2301.13848",
  primaryClass  = "cs.CL",
  arxivid       = "2301.13848"
}

@ARTICLE{Qin2023-am,
  title         = "Is {ChatGPT} a {General-Purpose} Natural Language Processing
                   Task Solver?",
  author        = "Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and
                   Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi",
  abstract      = "Spurred by advancements in scale, large language models
                   (LLMs) have demonstrated the ability to perform a variety of
                   natural language processing (NLP) tasks zero-shot -- i.e.,
                   without adaptation on downstream data. Recently, the debut
                   of ChatGPT has drawn a great deal of attention from the
                   natural language processing (NLP) community due to the fact
                   that it can generate high-quality responses to human input
                   and self-correct previous mistakes based on subsequent
                   conversations. However, it is not yet known whether ChatGPT
                   can serve as a generalist model that can perform many NLP
                   tasks zero-shot. In this work, we empirically analyze the
                   zero-shot learning ability of ChatGPT by evaluating it on 20
                   popular NLP datasets covering 7 representative task
                   categories. With extensive empirical studies, we demonstrate
                   both the effectiveness and limitations of the current
                   version of ChatGPT. We find that ChatGPT performs well on
                   many tasks favoring reasoning capabilities (e.g., arithmetic
                   reasoning) while it still faces challenges when solving
                   specific tasks such as sequence tagging. We additionally
                   provide in-depth analysis through qualitative case studies.",
  month         =  feb,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2302.06476",
  primaryClass  = "cs.CL",
  arxivid       = "2302.06476"
}

@ARTICLE{Kocon2023-oe,
  title         = "{ChatGPT}: Jack of all trades, master of none",
  author        = "Koco{\'n}, Jan and Cichecki, Igor and Kaszyca, Oliwier and
                   Kochanek, Mateusz and Szyd{\l}o, Dominika and Baran, Joanna
                   and Bielaniewicz, Julita and Gruza, Marcin and Janz,
                   Arkadiusz and Kanclerz, Kamil and Koco{\'n}, Anna and
                   Koptyra, Bart{\l}omiej and Mieleszczenko-Kowszewicz,
                   Wiktoria and Mi{\l}kowski, Piotr and Oleksy, Marcin and
                   Piasecki, Maciej and Radli{\'n}ski, {\L}ukasz and Wojtasik,
                   Konrad and Wo{\'z}niak, Stanis{\l}aw and Kazienko,
                   Przemys{\l}aw",
  abstract      = "OpenAI has released the Chat Generative Pre-trained
                   Transformer (ChatGPT) and revolutionized the approach in
                   artificial intelligence to human-model interaction. Several
                   publications on ChatGPT evaluation test its effectiveness on
                   well-known natural language processing (NLP) tasks. However,
                   the existing studies are mostly non-automated and tested on
                   a very limited scale. In this work, we examined ChatGPT's
                   capabilities on 25 diverse analytical NLP tasks, most of
                   them subjective even to humans, such as sentiment analysis,
                   emotion recognition, offensiveness, and stance detection. In
                   contrast, the other tasks require more objective reasoning
                   like word sense disambiguation, linguistic acceptability,
                   and question answering. We also evaluated GPT-4 model on
                   five selected subsets of NLP tasks. We automated ChatGPT and
                   GPT-4 prompting process and analyzed more than 49k
                   responses. Our comparison of its results with available
                   State-of-the-Art (SOTA) solutions showed that the average
                   loss in quality of the ChatGPT model was about 25\% for
                   zero-shot and few-shot evaluation. For GPT-4 model, a loss
                   for semantic tasks is significantly lower than for ChatGPT.
                   We showed that the more difficult the task (lower SOTA
                   performance), the higher the ChatGPT loss. It especially
                   refers to pragmatic NLP problems like emotion recognition.
                   We also tested the ability to personalize ChatGPT responses
                   for selected subjective tasks via Random Contextual Few-Shot
                   Personalization, and we obtained significantly better
                   user-based predictions. Additional qualitative analysis
                   revealed a ChatGPT bias, most likely due to the rules
                   imposed on human trainers by OpenAI. Our results provide the
                   basis for a fundamental discussion of whether the high
                   quality of recent predictive NLP models can indicate a
                   tool's usefulness to society and how the learning and
                   validation procedures for such systems should be
                   established.",
  month         =  feb,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2302.10724",
  primaryClass  = "cs.CL",
  arxivid       = "2302.10724"
}

@ARTICLE{Zhong2023-oe,
  title         = "Can {ChatGPT} Understand Too? A Comparative Study on
                   {ChatGPT} and Fine-tuned {BERT}",
  author        = "Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and
                   Tao, Dacheng",
  abstract      = "Recently, ChatGPT has attracted great attention, as it can
                   generate fluent and high-quality responses to human
                   inquiries. Several prior studies have shown that ChatGPT
                   attains remarkable generation ability compared with existing
                   models. However, the quantitative analysis of ChatGPT's
                   understanding ability has been given little attention. In
                   this report, we explore the understanding ability of ChatGPT
                   by evaluating it on the most popular GLUE benchmark, and
                   comparing it with 4 representative fine-tuned BERT-style
                   models. We find that: 1) ChatGPT falls short in handling
                   paraphrase and similarity tasks; 2) ChatGPT outperforms all
                   BERT models on inference tasks by a large margin; 3) ChatGPT
                   achieves comparable performance compared with BERT on
                   sentiment analysis and question-answering tasks.
                   Additionally, by combining some advanced prompting
                   strategies, we show that the understanding ability of
                   ChatGPT can be further improved.",
  month         =  feb,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2302.10198",
  primaryClass  = "cs.CL",
  arxivid       = "2302.10198"
}

@ARTICLE{Yang2023-ei,
  title         = "Exploring the Limits of {ChatGPT} for Query or Aspect-based
                   Text Summarization",
  author        = "Yang, Xianjun and Li, Yan and Zhang, Xinlu and Chen, Haifeng
                   and Cheng, Wei",
  abstract      = "Text summarization has been a crucial problem in natural
                   language processing (NLP) for several decades. It aims to
                   condense lengthy documents into shorter versions while
                   retaining the most critical information. Various methods
                   have been proposed for text summarization, including
                   extractive and abstractive summarization. The emergence of
                   large language models (LLMs) like GPT3 and ChatGPT has
                   recently created significant interest in using these models
                   for text summarization tasks. Recent studies
                   \textbackslashcite\{goyal2022news, zhang2023benchmarking\}
                   have shown that LLMs-generated news summaries are already on
                   par with humans. However, the performance of LLMs for more
                   practical applications like aspect or query-based summaries
                   is underexplored. To fill this gap, we conducted an
                   evaluation of ChatGPT's performance on four widely used
                   benchmark datasets, encompassing diverse summaries from
                   Reddit posts, news articles, dialogue meetings, and stories.
                   Our experiments reveal that ChatGPT's performance is
                   comparable to traditional fine-tuning methods in terms of
                   Rouge scores. Moreover, we highlight some unique differences
                   between ChatGPT-generated summaries and human references,
                   providing valuable insights into the superpower of ChatGPT
                   for diverse text summarization tasks. Our findings call for
                   new directions in this area, and we plan to conduct further
                   research to systematically examine the characteristics of
                   ChatGPT-generated summaries through extensive human
                   evaluation.",
  month         =  feb,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2302.08081",
  primaryClass  = "cs.CL",
  arxivid       = "2302.08081"
}

@ARTICLE{Chen2023-uf,
  title         = "How Robust is {GPT-3.5} to Predecessors? A Comprehensive
                   Study on Language Understanding Tasks",
  author        = "Chen, Xuanting and Ye, Junjie and Zu, Can and Xu, Nuo and
                   Zheng, Rui and Peng, Minlong and Zhou, Jie and Gui, Tao and
                   Zhang, Qi and Huang, Xuanjing",
  abstract      = "The GPT-3.5 models have demonstrated impressive performance
                   in various Natural Language Processing (NLP) tasks,
                   showcasing their strong understanding and reasoning
                   capabilities. However, their robustness and abilities to
                   handle various complexities of the open world have yet to be
                   explored, which is especially crucial in assessing the
                   stability of models and is a key aspect of trustworthy AI.
                   In this study, we perform a comprehensive experimental
                   analysis of GPT-3.5, exploring its robustness using 21
                   datasets (about 116K test samples) with 66 text
                   transformations from TextFlint that cover 9 popular Natural
                   Language Understanding (NLU) tasks. Our findings indicate
                   that while GPT-3.5 outperforms existing fine-tuned models on
                   some tasks, it still encounters significant robustness
                   degradation, such as its average performance dropping by up
                   to 35.74\% and 43.59\% in natural language inference and
                   sentiment analysis tasks, respectively. We also show that
                   GPT-3.5 faces some specific robustness challenges, including
                   robustness instability, prompt sensitivity, and number
                   sensitivity. These insights are valuable for understanding
                   its limitations and guiding future research in addressing
                   these challenges to enhance GPT-3.5's overall performance
                   and generalization abilities.",
  month         =  mar,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2303.00293",
  primaryClass  = "cs.CL",
  arxivid       = "2303.00293"
}

@ARTICLE{noauthor_undated-sw,
  title         = "{ChatGPT} Outperforms {Crowd-Workers} for {Text-Annotation}
                   Tasks",
  author        = "Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l",
  abstract      = "Many NLP applications require manual data annotations for a
                   variety of tasks, notably to train classifiers or evaluate
                   the performance of unsupervised models. Depending on the
                   size and degree of complexity, the tasks may be conducted by
                   crowd-workers on platforms such as MTurk as well as trained
                   annotators, such as research assistants. Using a sample of
                   2,382 tweets, we demonstrate that ChatGPT outperforms
                   crowd-workers for several annotation tasks, including
                   relevance, stance, topics, and frames detection.
                   Specifically, the zero-shot accuracy of ChatGPT exceeds that
                   of crowd-workers for four out of five tasks, while ChatGPT's
                   intercoder agreement exceeds that of both crowd-workers and
                   trained annotators for all tasks. Moreover, the
                   per-annotation cost of ChatGPT is less than \$0.003 -- about
                   twenty times cheaper than MTurk. These results show the
                   potential of large language models to drastically increase
                   the efficiency of text classification.",
  journal       = "arXiv [cs.CL]",
  month         =  mar,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2303.15056",
  primaryClass  = "cs.CL",
  arxivid       = "2303.15056"
}

@ARTICLE{Ye2023-ab,
  title         = "A Comprehensive Capability Analysis of {GPT-3} and {GPT-3.5}
                   Series Models",
  author        = "Ye, Junjie and Chen, Xuanting and Xu, Nuo and Zu, Can and
                   Shao, Zekai and Liu, Shichun and Cui, Yuhan and Zhou, Zeyang
                   and Gong, Chao and Shen, Yang and Zhou, Jie and Chen, Siming
                   and Gui, Tao and Zhang, Qi and Huang, Xuanjing",
  abstract      = "GPT series models, such as GPT-3, CodeX, InstructGPT,
                   ChatGPT, and so on, have gained considerable attention due
                   to their exceptional natural language processing
                   capabilities. However, despite the abundance of research on
                   the difference in capabilities between GPT series models and
                   fine-tuned models, there has been limited attention given to
                   the evolution of GPT series models' capabilities over time.
                   To conduct a comprehensive analysis of the capabilities of
                   GPT series models, we select six representative models,
                   comprising two GPT-3 series models (i.e., davinci and
                   text-davinci-001) and four GPT-3.5 series models (i.e.,
                   code-davinci-002, text-davinci-002, text-davinci-003, and
                   gpt-3.5-turbo). We evaluate their performance on nine
                   natural language understanding (NLU) tasks using 21
                   datasets. In particular, we compare the performance and
                   robustness of different models for each task under zero-shot
                   and few-shot scenarios. Our extensive experiments reveal
                   that the overall ability of GPT series models on NLU tasks
                   does not increase gradually as the models evolve, especially
                   with the introduction of the RLHF training strategy. While
                   this strategy enhances the models' ability to generate
                   human-like responses, it also compromises their ability to
                   solve some tasks. Furthermore, our findings indicate that
                   there is still room for improvement in areas such as model
                   robustness.",
  month         =  mar,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2303.10420",
  primaryClass  = "cs.CL",
  arxivid       = "2303.10420"
}

@ARTICLE{Li2023-bu,
  title         = "Evaluating {ChatGPT's} Information Extraction Capabilities:
                   An Assessment of Performance, Explainability, Calibration,
                   and Faithfulness",
  author        = "Li, Bo and Fang, Gexiang and Yang, Yang and Wang, Quansen
                   and Ye, Wei and Zhao, Wen and Zhang, Shikun",
  abstract      = "The capability of Large Language Models (LLMs) like ChatGPT
                   to comprehend user intent and provide reasonable responses
                   has made them extremely popular lately. In this paper, we
                   focus on assessing the overall ability of ChatGPT using 7
                   fine-grained information extraction (IE) tasks. Specially,
                   we present the systematically analysis by measuring
                   ChatGPT's performance, explainability, calibration, and
                   faithfulness, and resulting in 15 keys from either the
                   ChatGPT or domain experts. Our findings reveal that
                   ChatGPT's performance in Standard-IE setting is poor, but it
                   surprisingly exhibits excellent performance in the OpenIE
                   setting, as evidenced by human evaluation. In addition, our
                   research indicates that ChatGPT provides high-quality and
                   trustworthy explanations for its decisions. However, there
                   is an issue of ChatGPT being overconfident in its
                   predictions, which resulting in low calibration.
                   Furthermore, ChatGPT demonstrates a high level of
                   faithfulness to the original text in the majority of cases.
                   We manually annotate and release the test sets of 7
                   fine-grained IE tasks contains 14 datasets to further
                   promote the research. The datasets and code are available at
                   https://github.com/pkuserc/ChatGPT\_for\_IE.",
  month         =  apr,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2304.11633",
  primaryClass  = "cs.CL",
  arxivid       = "2304.11633"
}

@ARTICLE{Schaeffer2023-qt,
  title         = "Are Emergent Abilities of Large Language Models a Mirage?",
  author        = "Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi",
  abstract      = "Recent work claims that large language models display
                   emergent abilities, abilities not present in smaller-scale
                   models that are present in larger-scale models. What makes
                   emergent abilities intriguing is two-fold: their sharpness,
                   transitioning seemingly instantaneously from not present to
                   present, and their unpredictability, appearing at seemingly
                   unforeseeable model scales. Here, we present an alternative
                   explanation for emergent abilities: that for a particular
                   task and model family, when analyzing fixed model outputs,
                   emergent abilities appear due to the researcher's choice of
                   metric rather than due to fundamental changes in model
                   behavior with scale. Specifically, nonlinear or
                   discontinuous metrics produce apparent emergent abilities,
                   whereas linear or continuous metrics produce smooth,
                   continuous predictable changes in model performance. We
                   present our alternative explanation in a simple mathematical
                   model, then test it in three complementary ways: we (1)
                   make, test and confirm three predictions on the effect of
                   metric choice using the InstructGPT/GPT-3 family on tasks
                   with claimed emergent abilities; (2) make, test and confirm
                   two predictions about metric choices in a meta-analysis of
                   emergent abilities on BIG-Bench; and (3) show to choose
                   metrics to produce never-before-seen seemingly emergent
                   abilities in multiple vision tasks across diverse deep
                   networks. Via all three analyses, we provide evidence that
                   alleged emergent abilities evaporate with different metrics
                   or with better statistics, and may not be a fundamental
                   property of scaling AI models.",
  month         =  apr,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2304.15004",
  primaryClass  = "cs.AI",
  arxivid       = "2304.15004"
}

@ARTICLE{Wei2022-ow,
  title         = "Emergent Abilities of Large Language Models",
  author        = "Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel,
                   Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama,
                   Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald
                   and Chi, Ed H and Hashimoto, Tatsunori and Vinyals, Oriol
                   and Liang, Percy and Dean, Jeff and Fedus, William",
  abstract      = "Scaling up language models has been shown to predictably
                   improve performance and sample efficiency on a wide range of
                   downstream tasks. This paper instead discusses an
                   unpredictable phenomenon that we refer to as emergent
                   abilities of large language models. We consider an ability
                   to be emergent if it is not present in smaller models but is
                   present in larger models. Thus, emergent abilities cannot be
                   predicted simply by extrapolating the performance of smaller
                   models. The existence of such emergence implies that
                   additional scaling could further expand the range of
                   capabilities of language models.",
  month         =  jun,
  year          =  2022,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2206.07682",
  primaryClass  = "cs.CL",
  arxivid       = "2206.07682"
}

@ARTICLE{Jung2022-bq,
  title         = "Maieutic Prompting: Logically Consistent Reasoning with
                   Recursive Explanations",
  author        = "Jung, Jaehun and Qin, Lianhui and Welleck, Sean and Brahman,
                   Faeze and Bhagavatula, Chandra and Le Bras, Ronan and Choi,
                   Yejin",
  abstract      = "Despite their impressive capabilities, large pre-trained
                   language models (LMs) struggle with consistent reasoning;
                   recently, prompting LMs to generate explanations that
                   self-guide the inference has emerged as a promising
                   direction to amend this. However, these approaches are
                   fundamentally bounded by the correctness of explanations,
                   which themselves are often noisy and inconsistent. In this
                   work, we develop Maieutic Prompting, which infers a correct
                   answer to a question even from the noisy and inconsistent
                   generations of LM. Maieutic Prompting induces a tree of
                   explanations abductively (e.g. X is true, because ...) and
                   recursively, then frames the inference as a satisfiability
                   problem over these explanations and their logical relations.
                   We test Maieutic Prompting for true/false QA on three
                   challenging benchmarks that require complex commonsense
                   reasoning. Maieutic Prompting achieves up to 20\% better
                   accuracy than state-of-the-art prompting methods, and as a
                   fully unsupervised approach, performs competitively with
                   supervised models. We also show that Maieutic Prompting
                   improves robustness in inference while providing
                   interpretable rationales.",
  month         =  may,
  year          =  2022,
  keywords      = "In-Context",
  archivePrefix = "arXiv",
  eprint        = "2205.11822",
  primaryClass  = "cs.CL",
  arxivid       = "2205.11822"
}

@ARTICLE{West2021-vl,
  title         = "Symbolic Knowledge Distillation: from General Language
                   Models to Commonsense Models",
  author        = "West, Peter and Bhagavatula, Chandra and Hessel, Jack and
                   Hwang, Jena D and Jiang, Liwei and Le Bras, Ronan and Lu,
                   Ximing and Welleck, Sean and Choi, Yejin",
  abstract      = "The common practice for training commonsense models has gone
                   from-human-to-corpus-to-machine: humans author commonsense
                   knowledge graphs in order to train commonsense models. In
                   this work, we investigate an alternative,
                   from-machine-to-corpus-to-machine: general language models
                   author these commonsense knowledge graphs to train
                   commonsense models. Our study leads to a new framework,
                   Symbolic Knowledge Distillation. As with prior art in
                   Knowledge Distillation (Hinton et al., 2015), our approach
                   uses larger models to teach smaller models. A key difference
                   is that we distill knowledge symbolically-as text-in
                   addition to the neural model. We also distill only one
                   aspect-the commonsense of a general language model teacher,
                   allowing the student to be a different type, a commonsense
                   model. Altogether, we show that careful prompt engineering
                   and a separately trained critic model allow us to
                   selectively distill high-quality causal commonsense from
                   GPT-3, a general language model. Empirical results
                   demonstrate that, for the first time, a human-authored
                   commonsense knowledge graph is surpassed by our
                   automatically distilled variant in all three criteria:
                   quantity, quality, and diversity. In addition, it results in
                   a neural commonsense model that surpasses the teacher
                   model's commonsense capabilities despite its 100x smaller
                   size. We apply this to the ATOMIC resource, and share our
                   new symbolic knowledge graph and commonsense models.",
  month         =  oct,
  year          =  2021,
  keywords      = "Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "2110.07178",
  primaryClass  = "cs.CL",
  arxivid       = "2110.07178"
}

@ARTICLE{Jiang2021-um,
  title         = "Can Machines Learn Morality? The Delphi Experiment",
  author        = "Jiang, Liwei and Hwang, Jena D and Bhagavatula, Chandra and
                   Le Bras, Ronan and Liang, Jenny and Dodge, Jesse and
                   Sakaguchi, Keisuke and Forbes, Maxwell and Borchardt, Jon
                   and Gabriel, Saadia and Tsvetkov, Yulia and Etzioni, Oren
                   and Sap, Maarten and Rini, Regina and Choi, Yejin",
  abstract      = "As AI systems become increasingly powerful and pervasive,
                   there are growing concerns about machines' morality or a
                   lack thereof. Yet, teaching morality to machines is a
                   formidable task, as morality remains among the most
                   intensely debated questions in humanity, let alone for AI.
                   Existing AI systems deployed to millions of users, however,
                   are already making decisions loaded with moral implications,
                   which poses a seemingly impossible challenge: teaching
                   machines moral sense, while humanity continues to grapple
                   with it. To explore this challenge, we introduce Delphi, an
                   experimental framework based on deep neural networks trained
                   directly to reason about descriptive ethical judgments,
                   e.g., ``helping a friend'' is generally good, while
                   ``helping a friend spread fake news'' is not. Empirical
                   results shed novel insights on the promises and limits of
                   machine ethics; Delphi demonstrates strong generalization
                   capabilities in the face of novel ethical situations, while
                   off-the-shelf neural network models exhibit markedly poor
                   judgment including unjust biases, confirming the need for
                   explicitly teaching machines moral sense. Yet, Delphi is not
                   perfect, exhibiting susceptibility to pervasive biases and
                   inconsistencies. Despite that, we demonstrate positive use
                   cases of imperfect Delphi, including using it as a component
                   model within other imperfect AI systems. Importantly, we
                   interpret the operationalization of Delphi in light of
                   prominent ethical theories, which leads us to important
                   future research questions.",
  month         =  oct,
  year          =  2021,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2110.07574",
  primaryClass  = "cs.CL",
  arxivid       = "2110.07574"
}

@ARTICLE{Creswell2022-zx,
  title         = "Faithful Reasoning Using Large Language Models",
  author        = "Creswell, Antonia and Shanahan, Murray",
  abstract      = "Although contemporary large language models (LMs)
                   demonstrate impressive question-answering capabilities,
                   their answers are typically the product of a single call to
                   the model. This entails an unwelcome degree of opacity and
                   compromises performance, especially on problems that are
                   inherently multi-step. To address these limitations, we show
                   how LMs can be made to perform faithful multi-step reasoning
                   via a process whose causal structure mirrors the underlying
                   logical structure of the problem. Our approach works by
                   chaining together reasoning steps, where each step results
                   from calls to two fine-tuned LMs, one for selection and one
                   for inference, to produce a valid reasoning trace. Our
                   method carries out a beam search through the space of
                   reasoning traces to improve reasoning quality. We
                   demonstrate the effectiveness of our model on multi-step
                   logical deduction and scientific question-answering, showing
                   that it outperforms baselines on final answer accuracy, and
                   generates humanly interpretable reasoning traces whose
                   validity can be checked by the user.",
  month         =  aug,
  year          =  2022,
  keywords      = "In-Context",
  archivePrefix = "arXiv",
  eprint        = "2208.14271",
  primaryClass  = "cs.AI",
  arxivid       = "2208.14271"
}

@ARTICLE{Dasgupta2022-wz,
  title         = "Language models show human-like content effects on reasoning",
  author        = "Dasgupta, Ishita and Lampinen, Andrew K and Chan, Stephanie
                   C Y and Creswell, Antonia and Kumaran, Dharshan and
                   McClelland, James L and Hill, Felix",
  abstract      = "Abstract reasoning is a key ability for an intelligent
                   system. Large language models achieve above-chance
                   performance on abstract reasoning tasks, but exhibit many
                   imperfections. However, human abstract reasoning is also
                   imperfect, and depends on our knowledge and beliefs about
                   the content of the reasoning problem. For example, humans
                   reason much more reliably about logical rules that are
                   grounded in everyday situations than arbitrary rules about
                   abstract attributes. The training experiences of language
                   models similarly endow them with prior expectations that
                   reflect human knowledge and beliefs. We therefore
                   hypothesized that language models would show human-like
                   content effects on abstract reasoning problems. We explored
                   this hypothesis across three logical reasoning tasks:
                   natural language inference, judging the logical validity of
                   syllogisms, and the Wason selection task (Wason, 1968). We
                   find that state of the art large language models (with 7 or
                   70 billion parameters; Hoffman et al., 2022) reflect many of
                   the same patterns observed in humans across these tasks --
                   like humans, models reason more effectively about believable
                   situations than unrealistic or abstract ones. Our findings
                   have implications for understanding both these cognitive
                   effects, and the factors that contribute to language model
                   performance.",
  month         =  jul,
  year          =  2022,
  keywords      = "In-Context",
  archivePrefix = "arXiv",
  eprint        = "2207.07051",
  primaryClass  = "cs.CL",
  arxivid       = "2207.07051"
}

@INPROCEEDINGS{Blodgett2020-mg,
  title     = "Language (Technology) is Power: A Critical Survey of
               {``}Bias{''} in {NLP}",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  author    = "Blodgett, Su Lin and Barocas, Solon and Daum{\'e}, III, Hal and
               Wallach, Hanna",
  abstract  = "We survey 146 papers analyzing ``bias'' in NLP systems, finding
               that their motivations are often vague, inconsistent, and
               lacking in normative reasoning, despite the fact that analyzing
               ``bias'' is an inherently normative process. We further find
               that these papers' proposed quantitative techniques for
               measuring or mitigating ``bias'' are poorly matched to their
               motivations and do not engage with the relevant literature
               outside of NLP. Based on these findings, we describe the
               beginnings of a path forward by proposing three recommendations
               that should guide work analyzing ``bias'' in NLP systems. These
               recommendations rest on a greater recognition of the
               relationships between language and social hierarchies,
               encouraging researchers and practitioners to articulate their
               conceptualizations of ``bias''---i.e., what kinds of system
               behaviors are harmful, in what ways, to whom, and why, as well
               as the normative reasoning underlying these statements---and to
               center work around the lived experiences of members of
               communities affected by NLP systems, while interrogating and
               reimagining the power relations between technologists and such
               communities.",
  publisher = "Association for Computational Linguistics",
  pages     = "5454--5476",
  month     =  jul,
  year      =  2020,
  address   = "Online",
  keywords  = "Fairness, Bias, Toxicity",
  doi       = "10.18653/v1/2020.acl-main.485"
}

@ARTICLE{Weidinger2021-aj,
  title         = "Ethical and social risks of harm from Language Models",
  author        = "Weidinger, Laura and Mellor, John and Rauh, Maribeth and
                   Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and
                   Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh,
                   Atoosa and Kenton, Zac and Brown, Sasha and Hawkins, Will
                   and Stepleton, Tom and Biles, Courtney and Birhane, Abeba
                   and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne
                   and Isaac, William and Legassick, Sean and Irving, Geoffrey
                   and Gabriel, Iason",
  abstract      = "This paper aims to help structure the risk landscape
                   associated with large-scale Language Models (LMs). In order
                   to foster advances in responsible innovation, an in-depth
                   understanding of the potential risks posed by these models
                   is needed. A wide range of established and anticipated risks
                   are analysed in detail, drawing on multidisciplinary
                   expertise and literature from computer science, linguistics,
                   and social sciences. We outline six specific risk areas: I.
                   Discrimination, Exclusion and Toxicity, II. Information
                   Hazards, III. Misinformation Harms, V. Malicious Uses, V.
                   Human-Computer Interaction Harms, VI. Automation, Access,
                   and Environmental Harms. The first area concerns the
                   perpetuation of stereotypes, unfair discrimination,
                   exclusionary norms, toxic language, and lower performance by
                   social group for LMs. The second focuses on risks from
                   private data leaks or LMs correctly inferring sensitive
                   information. The third addresses risks arising from poor,
                   false or misleading information including in sensitive
                   domains, and knock-on risks such as the erosion of trust in
                   shared information. The fourth considers risks from actors
                   who try to use LMs to cause harm. The fifth focuses on risks
                   specific to LLMs used to underpin conversational agents that
                   interact with human users, including unsafe use,
                   manipulation or deception. The sixth discusses the risk of
                   environmental harm, job automation, and other challenges
                   that may have a disparate effect on different social groups
                   or communities. In total, we review 21 risks in-depth. We
                   discuss the points of origin of different risks and point to
                   potential mitigation approaches. Lastly, we discuss
                   organisational responsibilities in implementing mitigations,
                   and the role of collaboration and participation. We
                   highlight directions for further research, particularly on
                   expanding the toolkit for assessing and evaluating the
                   outlined risks in LMs.",
  month         =  dec,
  year          =  2021,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2112.04359",
  primaryClass  = "cs.CL",
  arxivid       = "2112.04359"
}

@ARTICLE{Rae2021-zm,
  title         = "Scaling Language Models: Methods, Analysis \& Insights from
                   Training Gopher",
  author        = "Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and
                   Millican, Katie and Hoffmann, Jordan and Song, Francis and
                   Aslanides, John and Henderson, Sarah and Ring, Roman and
                   Young, Susannah and Rutherford, Eliza and Hennigan, Tom and
                   Menick, Jacob and Cassirer, Albin and Powell, Richard and
                   van den Driessche, George and Hendricks, Lisa Anne and Rauh,
                   Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl,
                   Johannes and Dathathri, Sumanth and Huang, Saffron and
                   Uesato, Jonathan and Mellor, John and Higgins, Irina and
                   Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen,
                   Erich and Jayakumar, Siddhant and Buchatskaya, Elena and
                   Budden, David and Sutherland, Esme and Simonyan, Karen and
                   Paganini, Michela and Sifre, Laurent and Martens, Lena and
                   Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh,
                   Aida and Gribovskaya, Elena and Donato, Domenic and
                   Lazaridou, Angeliki and Mensch, Arthur and Lespiau,
                   Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai
                   and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas
                   and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and de
                   Masson d'Autume, Cyprien and Li, Yujia and Terzi, Tayfun and
                   Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and
                   de Las Casas, Diego and Guy, Aurelia and Jones, Chris and
                   Bradbury, James and Johnson, Matthew and Hechtman, Blake and
                   Weidinger, Laura and Gabriel, Iason and Isaac, William and
                   Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer,
                   Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff
                   and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu,
                   Koray and Irving, Geoffrey",
  abstract      = "Language modelling provides a step towards intelligent
                   communication systems by harnessing large repositories of
                   written human knowledge to better predict and understand the
                   world. In this paper, we present an analysis of
                   Transformer-based language model performance across a wide
                   range of model scales -- from models with tens of millions
                   of parameters up to a 280 billion parameter model called
                   Gopher. These models are evaluated on 152 diverse tasks,
                   achieving state-of-the-art performance across the majority.
                   Gains from scale are largest in areas such as reading
                   comprehension, fact-checking, and the identification of
                   toxic language, but logical and mathematical reasoning see
                   less benefit. We provide a holistic analysis of the training
                   dataset and model's behaviour, covering the intersection of
                   model scale with bias and toxicity. Finally we discuss the
                   application of language models to AI safety and the
                   mitigation of downstream harms.",
  month         =  dec,
  year          =  2021,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2112.11446",
  primaryClass  = "cs.CL",
  arxivid       = "2112.11446"
}

@ARTICLE{Min2022-aa,
  title         = "Rethinking the Role of Demonstrations: What Makes
                   {In-Context} Learning Work?",
  author        = "Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe,
                   Mikel and Lewis, Mike and Hajishirzi, Hannaneh and
                   Zettlemoyer, Luke",
  abstract      = "Large language models (LMs) are able to in-context learn --
                   perform a new task via inference alone by conditioning on a
                   few input-label pairs (demonstrations) and making
                   predictions for new inputs. However, there has been little
                   understanding of how the model learns and which aspects of
                   the demonstrations contribute to end task performance. In
                   this paper, we show that ground truth demonstrations are in
                   fact not required -- randomly replacing labels in the
                   demonstrations barely hurts performance on a range of
                   classification and multi-choce tasks, consistently over 12
                   different models including GPT-3. Instead, we find that
                   other aspects of the demonstrations are the key drivers of
                   end task performance, including the fact that they provide a
                   few examples of (1) the label space, (2) the distribution of
                   the input text, and (3) the overall format of the sequence.
                   Together, our analysis provides a new way of understanding
                   how and why in-context learning works, while opening up new
                   questions about how much can be learned from large language
                   models through inference alone.",
  month         =  feb,
  year          =  2022,
  keywords      = "In-Context",
  archivePrefix = "arXiv",
  eprint        = "2202.12837",
  primaryClass  = "cs.CL",
  arxivid       = "2202.12837"
}

@ARTICLE{Xie2021-nh,
  title         = "An Explanation of In-context Learning as Implicit Bayesian
                   Inference",
  author        = "Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy
                   and Ma, Tengyu",
  abstract      = "Large language models (LMs) such as GPT-3 have the
                   surprising ability to do in-context learning, where the
                   model learns to do a downstream task simply by conditioning
                   on a prompt consisting of input-output examples. The LM
                   learns from these examples without being explicitly
                   pretrained to learn. Thus, it is unclear what enables
                   in-context learning. In this paper, we study how in-context
                   learning can emerge when pretraining documents have
                   long-range coherence. Here, the LM must infer a latent
                   document-level concept to generate coherent next tokens
                   during pretraining. At test time, in-context learning occurs
                   when the LM also infers a shared latent concept between
                   examples in a prompt. We prove when this occurs despite a
                   distribution mismatch between prompts and pretraining data
                   in a setting where the pretraining distribution is a mixture
                   of HMMs. In contrast to messy large-scale datasets used to
                   train LMs capable of in-context learning, we generate a
                   small-scale synthetic dataset (GINC) where Transformers and
                   LSTMs both exhibit in-context learning. Beyond the theory,
                   experiments on GINC exhibit large-scale real-world phenomena
                   including improved in-context performance with model scaling
                   (despite the same pretraining loss), sensitivity to example
                   order, and instances where zero-shot is better than few-shot
                   in-context learning.",
  month         =  nov,
  year          =  2021,
  keywords      = "In-Context",
  archivePrefix = "arXiv",
  eprint        = "2111.02080",
  primaryClass  = "cs.CL",
  arxivid       = "2111.02080"
}

@ARTICLE{Zhu2023-fm,
  title         = "A Survey on Model Compression for Large Language Models",
  author        = "Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang,
                   Weiping",
  abstract      = "Large Language Models (LLMs) have revolutionized natural
                   language processing tasks with remarkable success. However,
                   their formidable size and computational demands present
                   significant challenges for practical deployment, especially
                   in resource-constrained environments. As these challenges
                   become increasingly pertinent, the field of model
                   compression has emerged as a pivotal research area to
                   alleviate these limitations. This paper presents a
                   comprehensive survey that navigates the landscape of model
                   compression techniques tailored specifically for LLMs.
                   Addressing the imperative need for efficient deployment, we
                   delve into various methodologies, encompassing quantization,
                   pruning, knowledge distillation, and more. Within each of
                   these techniques, we highlight recent advancements and
                   innovative approaches that contribute to the evolving
                   landscape of LLM research. Furthermore, we explore
                   benchmarking strategies and evaluation metrics that are
                   essential for assessing the effectiveness of compressed
                   LLMs. By providing insights into the latest developments and
                   practical implications, this survey serves as an invaluable
                   resource for both researchers and practitioners. As LLMs
                   continue to evolve, this survey aims to facilitate enhanced
                   efficiency and real-world applicability, establishing a
                   foundation for future advancements in the field.",
  month         =  aug,
  year          =  2023,
  keywords      = "Efficient;Survey",
  archivePrefix = "arXiv",
  eprint        = "2308.07633",
  primaryClass  = "cs.CL",
  arxivid       = "2308.07633"
}

@MISC{Tsipras_undated-xp,
  title       = "in-context-learning",
  author      = "Tsipras, Dimitris",
  abstract    = "Contribute to dtsip/in-context-learning development by
                 creating an account on GitHub.",
  institution = "Github",
  keywords    = "In-Context",
  language    = "en"
}

@ARTICLE{Khandelwal2019-bx,
  title         = "Generalization through Memorization: Nearest Neighbor
                   Language Models",
  author        = "Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and
                   Zettlemoyer, Luke and Lewis, Mike",
  abstract      = "We introduce $k$NN-LMs, which extend a pre-trained neural
                   language model (LM) by linearly interpolating it with a
                   $k$-nearest neighbors ($k$NN) model. The nearest neighbors
                   are computed according to distance in the pre-trained LM
                   embedding space, and can be drawn from any text collection,
                   including the original LM training data. Applying this
                   augmentation to a strong Wikitext-103 LM, with neighbors
                   drawn from the original training set, our $k$NN-LM achieves
                   a new state-of-the-art perplexity of 15.79 - a 2.9 point
                   improvement with no additional training. We also show that
                   this approach has implications for efficiently scaling up to
                   larger training sets and allows for effective domain
                   adaptation, by simply varying the nearest neighbor
                   datastore, again without further training. Qualitatively,
                   the model is particularly helpful in predicting rare
                   patterns, such as factual knowledge. Together, these results
                   strongly suggest that learning similarity between sequences
                   of text is easier than predicting the next word, and that
                   nearest neighbor search is an effective approach for
                   language modeling in the long tail.",
  month         =  nov,
  year          =  2019,
  keywords      = "RAG",
  archivePrefix = "arXiv",
  eprint        = "1911.00172",
  primaryClass  = "cs.CL",
  arxivid       = "1911.00172"
}

@ARTICLE{Izacard2022-ut,
  title         = "Atlas: Few-shot Learning with Retrieval Augmented Language
                   Models",
  author        = "Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and
                   Hosseini, Lucas and Petroni, Fabio and Schick, Timo and
                   Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian
                   and Grave, Edouard",
  abstract      = "Large language models have shown impressive few-shot results
                   on a wide range of tasks. However, when knowledge is key for
                   such results, as is the case for tasks such as question
                   answering and fact checking, massive parameter counts to
                   store knowledge seem to be needed. Retrieval augmented
                   models are known to excel at knowledge intensive tasks
                   without the need for as many parameters, but it is unclear
                   whether they work in few-shot settings. In this work we
                   present Atlas, a carefully designed and pre-trained
                   retrieval augmented language model able to learn knowledge
                   intensive tasks with very few training examples. We perform
                   evaluations on a wide range of tasks, including MMLU, KILT
                   and NaturalQuestions, and study the impact of the content of
                   the document index, showing that it can easily be updated.
                   Notably, Atlas reaches over 42\% accuracy on Natural
                   Questions using only 64 examples, outperforming a 540B
                   parameters model by 3\% despite having 50x fewer parameters.",
  month         =  aug,
  year          =  2022,
  keywords      = "RAG",
  archivePrefix = "arXiv",
  eprint        = "2208.03299",
  primaryClass  = "cs.CL",
  arxivid       = "2208.03299"
}

@ARTICLE{Gulcehre2023-ue,
  title         = "Reinforced Self-training ({ReST}) for language modeling",
  author        = "Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan
                   and Konyushkova, Ksenia and Weerts, Lotte and Sharma,
                   Abhishek and Siddhant, Aditya and Ahern, Alex and Wang,
                   Miaosen and Gu, Chenjie and Macherey, Wolfgang and Doucet,
                   Arnaud and Firat, Orhan and de Freitas, Nando",
  abstract      = "Reinforcement learning from human feedback (RLHF) can
                   improve the quality of large language model's (LLM) outputs
                   by aligning them with human preferences. We propose a simple
                   algorithm for aligning LLMs with human preferences inspired
                   by growing batch reinforcement learning (RL), which we call
                   Reinforced Self-Training (ReST). Given an initial LLM
                   policy, ReST produces a dataset by generating samples from
                   the policy, which are then used to improve the LLM policy
                   using offline RL algorithms. ReST is more efficient than
                   typical online RLHF methods because the training dataset is
                   produced offline, which allows data reuse. While ReST is a
                   general approach applicable to all generative learning
                   settings, we focus on its application to machine
                   translation. Our results show that ReST can substantially
                   improve translation quality, as measured by automated
                   metrics and human evaluation on machine translation
                   benchmarks in a compute and sample-efficient manner.",
  month         =  aug,
  year          =  2023,
  keywords      = "Alignment (RLHF, etc)",
  archivePrefix = "arXiv",
  eprint        = "2308.08998",
  primaryClass  = "cs.CL",
  arxivid       = "2308.08998"
}

@ARTICLE{Rein2023-gx,
  title         = "{GPQA}: A {Graduate-Level} {Google-Proof} {Q\&A} Benchmark",
  author        = "Rein, David and Hou, Betty Li and Stickland, Asa Cooper and
                   Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien
                   and Michael, Julian and Bowman, Samuel R",
  abstract      = "We present GPQA, a challenging dataset of 448
                   multiple-choice questions written by domain experts in
                   biology, physics, and chemistry. We ensure that the
                   questions are high-quality and extremely difficult: experts
                   who have or are pursuing PhDs in the corresponding domains
                   reach 65\% accuracy (74\% when discounting clear mistakes
                   the experts identified in retrospect), while highly skilled
                   non-expert validators only reach 34\% accuracy, despite
                   spending on average over 30 minutes with unrestricted access
                   to the web (i.e., the questions are ``Google-proof''). The
                   questions are also difficult for state-of-the-art AI
                   systems, with our strongest GPT-4 based baseline achieving
                   39\% accuracy. If we are to use future AI systems to help us
                   answer very hard questions, for example, when developing new
                   scientific knowledge, we need to develop scalable oversight
                   methods that enable humans to supervise their outputs, which
                   may be difficult even if the supervisors are themselves
                   skilled and knowledgeable. The difficulty of GPQA both for
                   skilled non-experts and frontier AI systems should enable
                   realistic scalable oversight experiments, which we hope can
                   help devise ways for human experts to reliably get truthful
                   information from AI systems that surpass human capabilities.",
  month         =  nov,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2311.12022",
  primaryClass  = "cs.AI",
  arxivid       = "2311.12022"
}

@ARTICLE{Greshake2023-np,
  title         = "Not what you've signed up for: Compromising {Real-World}
                   {LLM-Integrated} Applications with Indirect Prompt Injection",
  author        = "Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and
                   Endres, Christoph and Holz, Thorsten and Fritz, Mario",
  abstract      = "Large Language Models (LLMs) are increasingly being
                   integrated into various applications. The functionalities of
                   recent LLMs can be flexibly modulated via natural language
                   prompts. This renders them susceptible to targeted
                   adversarial prompting, e.g., Prompt Injection (PI) attacks
                   enable attackers to override original instructions and
                   employed controls. So far, it was assumed that the user is
                   directly prompting the LLM. But, what if it is not the user
                   prompting? We argue that LLM-Integrated Applications blur
                   the line between data and instructions. We reveal new attack
                   vectors, using Indirect Prompt Injection, that enable
                   adversaries to remotely (without a direct interface) exploit
                   LLM-integrated applications by strategically injecting
                   prompts into data likely to be retrieved. We derive a
                   comprehensive taxonomy from a computer security perspective
                   to systematically investigate impacts and vulnerabilities,
                   including data theft, worming, information ecosystem
                   contamination, and other novel security risks. We
                   demonstrate our attacks' practical viability against both
                   real-world systems, such as Bing's GPT-4 powered Chat and
                   code-completion engines, and synthetic applications built on
                   GPT-4. We show how processing retrieved prompts can act as
                   arbitrary code execution, manipulate the application's
                   functionality, and control how and if other APIs are called.
                   Despite the increasing integration and reliance on LLMs,
                   effective mitigations of these emerging threats are
                   currently lacking. By raising awareness of these
                   vulnerabilities and providing key insights into their
                   implications, we aim to promote the safe and responsible
                   deployment of these powerful models and the development of
                   robust defenses that protect users and systems from
                   potential attacks.",
  month         =  feb,
  year          =  2023,
  keywords      = "Jailbreak",
  archivePrefix = "arXiv",
  eprint        = "2302.12173",
  primaryClass  = "cs.CR",
  arxivid       = "2302.12173"
}

@ARTICLE{noauthor_undated-xd,
  title         = "{GAIA}: a benchmark for General {AI} Assistants",
  author        = "Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Swift,
                   Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas",
  abstract      = "We introduce GAIA, a benchmark for General AI Assistants
                   that, if solved, would represent a milestone in AI research.
                   GAIA proposes real-world questions that require a set of
                   fundamental abilities such as reasoning, multi-modality
                   handling, web browsing, and generally tool-use proficiency.
                   GAIA questions are conceptually simple for humans yet
                   challenging for most advanced AIs: we show that human
                   respondents obtain 92\% vs. 15\% for GPT-4 equipped with
                   plugins. This notable performance disparity contrasts with
                   the recent trend of LLMs outperforming humans on tasks
                   requiring professional skills in e.g. law or chemistry.
                   GAIA's philosophy departs from the current trend in AI
                   benchmarks suggesting to target tasks that are ever more
                   difficult for humans. We posit that the advent of Artificial
                   General Intelligence (AGI) hinges on a system's capability
                   to exhibit similar robustness as the average human does on
                   such questions. Using GAIA's methodology, we devise 466
                   questions and their answer. We release our questions while
                   retaining answers to 300 of them to power a leader-board
                   available at https://huggingface.co/gaia-benchmark.",
  journal       = "arXiv [cs.CL]",
  month         =  nov,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2311.12983",
  primaryClass  = "cs.CL",
  arxivid       = "2311.12983"
}

@ARTICLE{Roziere2023-jd,
  title         = "Code Llama: Open foundation models for code",
  author        = "Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle,
                   Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing
                   Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and
                   Rapin, J{\'e}r{\'e}my and Kozhevnikov, Artyom and Evtimov,
                   Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer,
                   Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and
                   D{\'e}fossez, Alexandre and Copet, Jade and Azhar, Faisal
                   and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and
                   Scialom, Thomas and Synnaeve, Gabriel",
  abstract      = "We release Code Llama, a family of large language models for
                   code based on Llama 2 providing state-of-the-art performance
                   among open models, infilling capabilities, support for large
                   input contexts, and zero-shot instruction following ability
                   for programming tasks. We provide multiple flavors to cover
                   a wide range of applications: foundation models (Code
                   Llama), Python specializations (Code Llama - Python), and
                   instruction-following models (Code Llama - Instruct) with
                   7B, 13B and 34B parameters each. All models are trained on
                   sequences of 16k tokens and show improvements on inputs with
                   up to 100k tokens. 7B and 13B Code Llama and Code Llama -
                   Instruct variants support infilling based on surrounding
                   content. Code Llama reaches state-of-the-art performance
                   among open models on several code benchmarks, with scores of
                   up to 53\% and 55\% on HumanEval and MBPP, respectively.
                   Notably, Code Llama - Python 7B outperforms Llama 2 70B on
                   HumanEval and MBPP, and all our models outperform every
                   other publicly available model on MultiPL-E. We release Code
                   Llama under a permissive license that allows for both
                   research and commercial use.",
  month         =  aug,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2308.12950",
  primaryClass  = "cs.CL",
  arxivid       = "2308.12950"
}

@ARTICLE{Van_Veen2023-wn,
  title         = "Clinical text summarization: Adapting large language models
                   can outperform human experts",
  author        = "Van Veen, Dave and Van Uden, Cara and Blankemeier, Louis and
                   Delbrouck, Jean-Benoit and Aali, Asad and Bluethgen,
                   Christian and Pareek, Anuj and Polacin, Malgorzata and
                   Collins, William and Ahuja, Neera and Langlotz, Curtis P and
                   Hom, Jason and Gatidis, Sergios and Pauly, John and
                   Chaudhari, Akshay S",
  abstract      = "Sifting through vast textual data and summarizing key
                   information imposes a substantial burden on how clinicians
                   allocate their time. Although large language models (LLMs)
                   have shown immense promise in natural language processing
                   (NLP) tasks, their efficacy across diverse clinical
                   summarization tasks has not yet been rigorously examined. In
                   this work, we employ domain adaptation methods on eight
                   LLMs, spanning six datasets and four distinct summarization
                   tasks: radiology reports, patient questions, progress notes,
                   and doctor-patient dialogue. Our thorough quantitative
                   assessment reveals trade-offs between models and adaptation
                   methods in addition to instances where recent advances in
                   LLMs may not lead to improved results. Further, in a
                   clinical reader study with six physicians, we depict that
                   summaries from the best adapted LLM are preferable to human
                   summaries in terms of completeness and correctness. Our
                   ensuing qualitative analysis delineates mutual challenges
                   faced by both LLMs and human experts. Lastly, we correlate
                   traditional quantitative NLP metrics with reader study
                   scores to enhance our understanding of how these metrics
                   align with physician preferences. Our research marks the
                   first evidence of LLMs outperforming human experts in
                   clinical text summarization across multiple tasks. This
                   implies that integrating LLMs into clinical workflows could
                   alleviate documentation burden, empowering clinicians to
                   focus more on personalized patient care and other
                   irreplaceable human aspects of medicine.",
  month         =  sep,
  year          =  2023,
  keywords      = "Health",
  archivePrefix = "arXiv",
  eprint        = "2309.07430",
  primaryClass  = "cs.CL",
  arxivid       = "2309.07430"
}

@ARTICLE{Ramanathan2015-hw,
  title         = "Detecting events and key actors in multi-person videos",
  author        = "Ramanathan, Vignesh and Huang, Jonathan and Abu-El-Haija,
                   Sami and Gorban, Alexander and Murphy, Kevin and Fei-Fei, Li",
  abstract      = "Multi-person event recognition is a challenging task, often
                   with many people active in the scene but only a small subset
                   contributing to an actual event. In this paper, we propose a
                   model which learns to detect events in such videos while
                   automatically ``attending'' to the people responsible for
                   the event. Our model does not use explicit annotations
                   regarding who or where those people are during training and
                   testing. In particular, we track people in videos and use a
                   recurrent neural network (RNN) to represent the track
                   features. We learn time-varying attention weights to combine
                   these features at each time-instant. The attended features
                   are then processed using another RNN for event
                   detection/classification. Since most video datasets with
                   multiple people are restricted to a small number of videos,
                   we also collected a new basketball dataset comprising 257
                   basketball games with 14K event annotations corresponding to
                   11 event classes. Our model outperforms state-of-the-art
                   methods for both event classification and detection on this
                   new dataset. Additionally, we show that the attention
                   mechanism is able to consistently localize the relevant
                   players.",
  month         =  nov,
  year          =  2015,
  keywords      = "CNN - Classification;Models;Spatio-Temporal Data",
  archivePrefix = "arXiv",
  eprint        = "1511.02917",
  primaryClass  = "cs.CV",
  arxivid       = "1511.02917"
}

@INCOLLECTION{Mnih2014-pz,
  title     = "Recurrent Models of Visual Attention",
  booktitle = "Advances in Neural Information Processing Systems 27",
  author    = "Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and
               Kavukcuoglu, Koray",
  editor    = "Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N D and
               Weinberger, K Q",
  publisher = "Curran Associates, Inc.",
  pages     = "2204--2212",
  year      =  2014,
  keywords  = "Spatio-Temporal Data ;Reinforcement Learning;Models"
}

@ARTICLE{Denil2012-lj,
  title    = "Learning where to attend with deep architectures for image
              tracking",
  author   = "Denil, Misha and Bazzani, Loris and Larochelle, Hugo and de
              Freitas, Nando",
  abstract = "We discuss an attentional model for simultaneous object tracking
              and recognition that is driven by gaze data. Motivated by
              theories of perception, the model consists of two interacting
              pathways, identity and control, intended to mirror the what and
              where pathways in neuroscience models. The identity pathway
              models object appearance and performs classification using deep
              (factored)-restricted Boltzmann machines. At each point in time,
              the observations consist of foveated images, with decaying
              resolution toward the periphery of the gaze. The control pathway
              models the location, orientation, scale, and speed of the
              attended object. The posterior distribution of these states is
              estimated with particle filtering. Deeper in the control pathway,
              we encounter an attentional mechanism that learns to select gazes
              so as to minimize tracking uncertainty. Unlike in our previous
              work, we introduce gaze selection strategies that operate in the
              presence of partial information and on a continuous action space.
              We show that a straightforward extension of the existing approach
              to the partial information setting results in poor performance,
              and we propose an alternative method based on modeling the reward
              surface as a gaussian process. This approach gives good
              performance in the presence of partial information and allows us
              to expand the action space from a small, discrete set of fixation
              points to a continuous domain.",
  journal  = "Neural Comput.",
  volume   =  24,
  number   =  8,
  pages    = "2151--2184",
  month    =  aug,
  year     =  2012,
  keywords = "Models;NLP",
  issn     = "0899-7667, 1530-888X",
  pmid     = "22509964",
  doi      = "10.1162/NECO\_a\_00312"
}

@ARTICLE{Xu2015-yk,
  title         = "Show, Attend and Tell: Neural Image Caption Generation with
                   Visual Attention",
  author        = "Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun
                   and Courville, Aaron and Salakhutdinov, Ruslan and Zemel,
                   Richard and Bengio, Yoshua",
  abstract      = "Inspired by recent work in machine translation and object
                   detection, we introduce an attention based model that
                   automatically learns to describe the content of images. We
                   describe how we can train this model in a deterministic
                   manner using standard backpropagation techniques and
                   stochastically by maximizing a variational lower bound. We
                   also show through visualization how the model is able to
                   automatically learn to fix its gaze on salient objects while
                   generating the corresponding words in the output sequence.
                   We validate the use of attention with state-of-the-art
                   performance on three benchmark datasets: Flickr8k, Flickr30k
                   and MS COCO.",
  month         =  feb,
  year          =  2015,
  keywords      = "Models;NLP",
  archivePrefix = "arXiv",
  eprint        = "1502.03044",
  primaryClass  = "cs.LG",
  arxivid       = "1502.03044"
}

@ARTICLE{Rush2015-sr,
  title         = "A Neural Attention Model for Abstractive Sentence
                   Summarization",
  author        = "Rush, Alexander M and Chopra, Sumit and Weston, Jason",
  abstract      = "Summarization based on text extraction is inherently
                   limited, but generation-style abstractive methods have
                   proven challenging to build. In this work, we propose a
                   fully data-driven approach to abstractive sentence
                   summarization. Our method utilizes a local attention-based
                   model that generates each word of the summary conditioned on
                   the input sentence. While the model is structurally simple,
                   it can easily be trained end-to-end and scales to a large
                   amount of training data. The model shows significant
                   performance gains on the DUC-2004 shared task compared with
                   several strong baselines.",
  month         =  sep,
  year          =  2015,
  keywords      = "Models;NLP",
  archivePrefix = "arXiv",
  eprint        = "1509.00685",
  primaryClass  = "cs.CL",
  arxivid       = "1509.00685"
}

@ARTICLE{Xu2015-yk,
  title         = "Ask, Attend and Answer: Exploring {Question-Guided} Spatial
                   Attention for Visual Question Answering",
  author        = "Xu, Huijuan and Saenko, Kate",
  abstract      = "The problem of Visual Question Answering (VQA) requires
                   joint image and language understanding to answer a question
                   about a given photograph. Recent approaches have applied
                   deep image captioning methods based on recurrent LSTM
                   networks to this problem, but have failed to model spatial
                   inference. In this paper, we propose a memory network with
                   spatial attention for the VQA task. Memory networks are
                   recurrent neural networks with an explicit attention
                   mechanism that selects certain parts of the information
                   stored in memory. We store neuron activations from different
                   spatial receptive fields in the memory, and use the question
                   to choose relevant regions for computing the answer. We
                   experiment with spatial attention architectures that use
                   different question representations to choose regions, and
                   also show that two attention steps (hops) obtain improved
                   results compared to a single step. To understand the
                   inference process learned by the network, we design
                   synthetic questions that specifically require spatial
                   inference and visualize the attention weights. We evaluate
                   our model on two published visual question answering
                   datasets, DAQUAR and VQA, and obtain promising results.",
  month         =  nov,
  year          =  2015,
  keywords      = "Models;VisualQA/Captioning",
  archivePrefix = "arXiv",
  eprint        = "1511.05234",
  primaryClass  = "cs.CV",
  arxivid       = "1511.05234"
}

@ARTICLE{Mnih2014-vp,
  title         = "Recurrent Models of Visual Attention",
  author        = "Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and
                   Kavukcuoglu, Koray",
  abstract      = "Applying convolutional neural networks to large images is
                   computationally expensive because the amount of computation
                   scales linearly with the number of image pixels. We present
                   a novel recurrent neural network model that is capable of
                   extracting information from an image or video by adaptively
                   selecting a sequence of regions or locations and only
                   processing the selected regions at high resolution. Like
                   convolutional neural networks, the proposed model has a
                   degree of translation invariance built-in, but the amount of
                   computation it performs can be controlled independently of
                   the input image size. While the model is non-differentiable,
                   it can be trained using reinforcement learning methods to
                   learn task-specific policies. We evaluate our model on
                   several image classification tasks, where it significantly
                   outperforms a convolutional neural network baseline on
                   cluttered images, and on a dynamic visual control problem,
                   where it learns to track a simple object without an explicit
                   training signal for doing so.",
  month         =  jun,
  year          =  2014,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "1406.6247",
  primaryClass  = "cs.LG",
  arxivid       = "1406.6247"
}

@ARTICLE{Lindsay2015-cl,
  title         = "Feature-based Attention in Convolutional Neural Networks",
  author        = "Lindsay, Grace W",
  abstract      = "Convolutional neural networks (CNNs) have proven effective
                   for image processing tasks, such as object recognition and
                   classification. Recently, CNNs have been enhanced with
                   concepts of attention, similar to those found in biology.
                   Much of this work on attention has focused on effective
                   serial spatial processing. In this paper, I introduce a
                   simple procedure for applying feature-based attention (FBA)
                   to CNNs and compare multiple implementation options. FBA is
                   a top-down signal applied globally to an input image which
                   aides in detecting chosen objects in cluttered or noisy
                   settings. The concept of FBA and the implementation details
                   tested here were derived from what is known (and debated)
                   about biological object- and feature-based attention. The
                   implementations of FBA described here increase performance
                   on challenging object detection tasks using a procedure that
                   is simple, fast, and does not require additional iterative
                   training. Furthermore, the comparisons performed here
                   suggest that a proposed model of biological FBA (the
                   ``feature similarity gain model'') is effective in
                   increasing performance.",
  month         =  nov,
  year          =  2015,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "1511.06408",
  primaryClass  = "cs.CV",
  arxivid       = "1511.06408"
}

@ARTICLE{Ballas2015-jk,
  title         = "Delving Deeper into Convolutional Networks for Learning
                   Video Representations",
  author        = "Ballas, Nicolas and Yao, Li and Pal, Chris and Courville,
                   Aaron",
  abstract      = "We propose an approach to learn spatio-temporal features in
                   videos from intermediate visual representations we call
                   ``percepts'' using Gated-Recurrent-Unit Recurrent Networks
                   (GRUs).Our method relies on percepts that are extracted from
                   all level of a deep convolutional network trained on the
                   large ImageNet dataset. While high-level percepts contain
                   highly discriminative information, they tend to have a
                   low-spatial resolution. Low-level percepts, on the other
                   hand, preserve a higher spatial resolution from which we can
                   model finer motion patterns. Using low-level percepts can
                   leads to high-dimensionality video representations. To
                   mitigate this effect and control the model number of
                   parameters, we introduce a variant of the GRU model that
                   leverages the convolution operations to enforce sparse
                   connectivity of the model units and share parameters across
                   the input spatial locations. We empirically validate our
                   approach on both Human Action Recognition and Video
                   Captioning tasks. In particular, we achieve results
                   equivalent to state-of-art on the YouTube2Text dataset using
                   a simpler text-decoder model and without extra 3D CNN
                   features.",
  month         =  nov,
  year          =  2015,
  keywords      = "Spatio-Temporal Data ;Models;Detection/Recognition",
  archivePrefix = "arXiv",
  eprint        = "1511.06432",
  primaryClass  = "cs.CV",
  arxivid       = "1511.06432"
}

@ARTICLE{Moczulski2015-tw,
  title         = "A Controller Recognizer Framework: How necessary is
                   recognition for control?",
  author        = "Moczulski, Marcin and Xu, Kelvin and Courville, Aaron and
                   Cho, Kyunghyun",
  abstract      = "Recently there has been growing interest in building active
                   visual object recognizers, as opposed to the usual passive
                   recognizers which classifies a given static image into a
                   predefined set of object categories. In this paper we
                   propose to generalize these recently proposed end-to-end
                   active visual recognizers into a controller-recognizer
                   framework. A model in the controller-recognizer framework
                   consists of a controller, which interfaces with an external
                   manipulator, and a recognizer which classifies the visual
                   input adjusted by the manipulator. We describe two most
                   recently proposed controller-recognizer models--recurrent
                   attention model and spatial transformer network-- as
                   representative examples of controller-recognizer models.
                   Based on this description we observe that most existing
                   end-to-end controller-recognizers tightly, or completely,
                   couple a controller and recognizer. We ask a question
                   whether this tight coupling is necessary, and try to answer
                   this empirically by building a controller-recognizer model
                   with a decoupled controller and recognizer. Our experiments
                   revealed that it is not always necessary to tightly couple
                   them and that by decoupling a controller and recognizer,
                   there is a possibility of building a generic controller that
                   is pretrained and works together with any subsequent
                   recognizer.",
  month         =  nov,
  year          =  2015,
  keywords      = "Models;Detection/Recognition",
  archivePrefix = "arXiv",
  eprint        = "1511.06428",
  primaryClass  = "cs.LG",
  arxivid       = "1511.06428"
}

@ARTICLE{Johnson2015-pt,
  title         = "{DenseCap}: Fully Convolutional Localization Networks for
                   Dense Captioning",
  author        = "Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li",
  abstract      = "We introduce the dense captioning task, which requires a
                   computer vision system to both localize and describe salient
                   regions in images in natural language. The dense captioning
                   task generalizes object detection when the descriptions
                   consist of a single word, and Image Captioning when one
                   predicted region covers the full image. To address the
                   localization and description task jointly we propose a Fully
                   Convolutional Localization Network (FCLN) architecture that
                   processes an image with a single, efficient forward pass,
                   requires no external regions proposals, and can be trained
                   end-to-end with a single round of optimization. The
                   architecture is composed of a Convolutional Network, a novel
                   dense localization layer, and Recurrent Neural Network
                   language model that generates the label sequences. We
                   evaluate our network on the Visual Genome dataset, which
                   comprises 94,000 images and 4,100,000 region-grounded
                   captions. We observe both speed and accuracy improvements
                   over baselines based on current state of the art approaches
                   in both generation and retrieval settings.",
  month         =  nov,
  year          =  2015,
  keywords      = "Detection/Recognition;VisualQA/Captioning;Models",
  archivePrefix = "arXiv",
  eprint        = "1511.07571",
  primaryClass  = "cs.CV",
  arxivid       = "1511.07571"
}

@ARTICLE{Itti2015-jg,
  title         = "Computational models of attention",
  author        = "Itti, Laurent and Borji, Ali",
  abstract      = "This chapter reviews recent computational models of visual
                   attention. We begin with models for the bottom-up or
                   stimulus-driven guidance of attention to salient visual
                   items, which we examine in seven different broad categories.
                   We then examine more complex models which address the
                   top-down or goal-oriented guidance of attention towards
                   items that are more relevant to the task at hand.",
  month         =  oct,
  year          =  2015,
  keywords      = "Survey/Review Paper;Models",
  archivePrefix = "arXiv",
  eprint        = "1510.07182",
  primaryClass  = "cs.CV",
  arxivid       = "1510.07182"
}

@ARTICLE{Luong2015-ep,
  title         = "Effective Approaches to Attention-based Neural Machine
                   Translation",
  author        = "Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D",
  abstract      = "An attentional mechanism has lately been used to improve
                   neural machine translation (NMT) by selectively focusing on
                   parts of the source sentence during translation. However,
                   there has been little work exploring useful architectures
                   for attention-based NMT. This paper examines two simple and
                   effective classes of attentional mechanism: a global
                   approach which always attends to all source words and a
                   local one that only looks at a subset of source words at a
                   time. We demonstrate the effectiveness of both approaches
                   over the WMT translation tasks between English and German in
                   both directions. With local attention, we achieve a
                   significant gain of 5.0 BLEU points over non-attentional
                   systems which already incorporate known techniques such as
                   dropout. Our ensemble model using different attention
                   architectures has established a new state-of-the-art result
                   in the WMT'15 English to German translation task with 25.9
                   BLEU points, an improvement of 1.0 BLEU points over the
                   existing best system backed by NMT and an n-gram reranker.",
  month         =  aug,
  year          =  2015,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "1508.04025",
  primaryClass  = "cs.CL",
  arxivid       = "1508.04025"
}

@ARTICLE{Chen2015-dg,
  title         = "{ABC-CNN}: An Attention Based Convolutional Neural Network
                   for Visual Question Answering",
  author        = "Chen, Kan and Wang, Jiang and Chen, Liang-Chieh and Gao,
                   Haoyuan and Xu, Wei and Nevatia, Ram",
  abstract      = "We propose a novel attention based deep learning
                   architecture for visual question answering task (VQA). Given
                   an image and an image related natural language question, VQA
                   generates the natural language answer for the question.
                   Generating the correct answers requires the model's
                   attention to focus on the regions corresponding to the
                   question, because different questions inquire about the
                   attributes of different image regions. We introduce an
                   attention based configurable convolutional neural network
                   (ABC-CNN) to learn such question-guided attention. ABC-CNN
                   determines an attention map for an image-question pair by
                   convolving the image feature map with configurable
                   convolutional kernels derived from the question's semantics.
                   We evaluate the ABC-CNN architecture on three benchmark VQA
                   datasets: Toronto COCO-QA, DAQUAR, and VQA dataset. ABC-CNN
                   model achieves significant improvements over
                   state-of-the-art methods on these datasets. The
                   question-guided attention generated by ABC-CNN is also shown
                   to reflect the regions that are highly relevant to the
                   questions.",
  month         =  nov,
  year          =  2015,
  keywords      = "Models;VisualQA/Captioning",
  archivePrefix = "arXiv",
  eprint        = "1511.05960",
  primaryClass  = "cs.CV",
  arxivid       = "1511.05960"
}

@ARTICLE{Rocktaschel2015-qf,
  title         = "Reasoning about Entailment with Neural Attention",
  author        = "Rockt{\"a}schel, Tim and Grefenstette, Edward and Hermann,
                   Karl Moritz and Ko{\v c}isk{\'y}, Tom{\'a}{\v s} and
                   Blunsom, Phil",
  abstract      = "Automatically recognizing entailment relations between pairs
                   of natural language sentences has so far been the dominion
                   of classifiers employing hand engineered features derived
                   from natural language processing pipelines. End-to-end
                   differentiable neural architectures have failed to approach
                   state-of-the-art performance until very recently. In this
                   paper, we propose a neural model that reads two sentences to
                   determine entailment using long short-term memory units. We
                   extend this model with a word-by-word neural attention
                   mechanism that encourages reasoning over entailments of
                   pairs of words and phrases. Furthermore, we present a
                   qualitative analysis of attention weights produced by this
                   model, demonstrating such reasoning capabilities. On a large
                   entailment dataset this model outperforms the previous best
                   neural model and a classifier with engineered features by a
                   substantial margin. It is the first generic end-to-end
                   differentiable system that achieves state-of-the-art
                   accuracy on a textual entailment dataset.",
  month         =  sep,
  year          =  2015,
  keywords      = "Models;NLP",
  archivePrefix = "arXiv",
  eprint        = "1509.06664",
  primaryClass  = "cs.CL",
  arxivid       = "1509.06664"
}

@ARTICLE{Chorowski2015-ly,
  title         = "{Attention-Based} Models for Speech Recognition",
  author        = "Chorowski, Jan and Bahdanau, Dzmitry and Serdyuk, Dmitriy
                   and Cho, Kyunghyun and Bengio, Yoshua",
  abstract      = "Recurrent sequence generators conditioned on input data
                   through an attention mechanism have recently shown very good
                   performance on a range of tasks in- cluding machine
                   translation, handwriting synthesis and image caption gen-
                   eration. We extend the attention-mechanism with features
                   needed for speech recognition. We show that while an
                   adaptation of the model used for machine translation in
                   reaches a competitive 18.7\% phoneme error rate (PER) on the
                   TIMIT phoneme recognition task, it can only be applied to
                   utterances which are roughly as long as the ones it was
                   trained on. We offer a qualitative explanation of this
                   failure and propose a novel and generic method of adding
                   location-awareness to the attention mechanism to alleviate
                   this issue. The new method yields a model that is robust to
                   long inputs and achieves 18\% PER in single utterances and
                   20\% in 10-times longer (repeated) utterances. Finally, we
                   propose a change to the at- tention mechanism that prevents
                   it from concentrating too much on single frames, which
                   further reduces PER to 17.6\% level.",
  month         =  jun,
  year          =  2015,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "1506.07503",
  primaryClass  = "cs.CL",
  arxivid       = "1506.07503"
}

@ARTICLE{Ba2014-zf,
  title         = "Multiple Object Recognition with Visual Attention",
  author        = "Ba, Jimmy and Mnih, Volodymyr and Kavukcuoglu, Koray",
  abstract      = "We present an attention-based model for recognizing multiple
                   objects in images. The proposed model is a deep recurrent
                   neural network trained with reinforcement learning to attend
                   to the most relevant regions of the input image. We show
                   that the model learns to both localize and recognize
                   multiple objects despite being given only class labels
                   during training. We evaluate the model on the challenging
                   task of transcribing house number sequences from Google
                   Street View images and show that it is both more accurate
                   than the state-of-the-art convolutional networks and uses
                   fewer parameters and less computation.",
  month         =  dec,
  year          =  2014,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "1412.7755",
  primaryClass  = "cs.LG",
  arxivid       = "1412.7755"
}

@INCOLLECTION{Ba2015-uj,
  title     = "Learning {Wake-Sleep} Recurrent Attention Models",
  booktitle = "Advances in Neural Information Processing Systems 28",
  author    = "Ba, Jimmy and Salakhutdinov, Ruslan R and Grosse, Roger B and
               Frey, Brendan J",
  editor    = "Cortes, C and Lawrence, N D and Lee, D D and Sugiyama, M and
               Garnett, R and Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "2575--2583",
  year      =  2015,
  keywords  = "Models"
}

@ARTICLE{Itti1998-sr,
  title    = "A model of saliency-based visual attention for rapid scene
              analysis",
  author   = "Itti, L and Koch, C and Niebur, E",
  abstract = "A visual attention system, inspired by the behavior and the
              neuronal architecture of the early primate visual system, is
              presented. Multiscale image features are combined into a single
              topographical saliency map. A dynamical neural network then
              selects attended locations in order of decreasing saliency. The
              system breaks down the complex problem of scene understanding by
              rapidly selecting, in a computationally efficient manner,
              conspicuous locations to be analyzed in detail",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  20,
  number   =  11,
  pages    = "1254--1259",
  month    =  nov,
  year     =  1998,
  keywords = "computer vision;feature extraction;image recognition;neural
              nets;target tracking;dynamical neural network;feature
              extraction;rapid scene analysis;saliency;scene
              understanding;target detection;topographical saliency map;visual
              attention;visual search;Biological system modeling;Brain
              modeling;Computer architecture;Feature extraction;Hardware;Image
              analysis;Layout;Neural networks;Object detection;Visual
              system;Classic Papers;Models",
  issn     = "0162-8828",
  doi      = "10.1109/34.730558"
}

@ARTICLE{Firat2016-gz,
  title         = "{Multi-Way}, Multilingual Neural Machine Translation with a
                   Shared Attention Mechanism",
  author        = "Firat, Orhan and Cho, Kyunghyun and Bengio, Yoshua",
  abstract      = "We propose multi-way, multilingual neural machine
                   translation. The proposed approach enables a single neural
                   translation model to translate between multiple languages,
                   with a number of parameters that grows only linearly with
                   the number of languages. This is made possible by having a
                   single attention mechanism that is shared across all
                   language pairs. We train the proposed multi-way,
                   multilingual model on ten language pairs from WMT'15
                   simultaneously and observe clear performance improvements
                   over models trained on only one language pair. In
                   particular, we observe that the proposed model significantly
                   improves the translation quality of low-resource language
                   pairs.",
  month         =  jan,
  year          =  2016,
  keywords      = "Models;NLP",
  archivePrefix = "arXiv",
  eprint        = "1601.01073",
  primaryClass  = "cs.CL",
  arxivid       = "1601.01073"
}

@ARTICLE{Wang2016-hp,
  title         = "Survey on the attention based {RNN} model and its
                   applications in computer vision",
  author        = "Wang, Feng and Tax, David M J",
  abstract      = "The recurrent neural networks (RNN) can be used to solve the
                   sequence to sequence problem, where both the input and the
                   output have sequential structures. Usually there are some
                   implicit relations between the structures. However, it is
                   hard for the common RNN model to fully explore the relations
                   between the sequences. In this survey, we introduce some
                   attention based RNN models which can focus on different
                   parts of the input for each output item, in order to explore
                   and take advantage of the implicit relations between the
                   input and the output items. The different attention
                   mechanisms are described in detail. We then introduce some
                   applications in computer vision which apply the attention
                   based RNN models. The superiority of the attention based RNN
                   model is shown by the experimental results. At last some
                   future research directions are given.",
  month         =  jan,
  year          =  2016,
  keywords      = "Survey/Review Paper;Models",
  archivePrefix = "arXiv",
  eprint        = "1601.06823",
  primaryClass  = "cs.CV",
  arxivid       = "1601.06823"
}

@ARTICLE{Bazzani2016-qi,
  title         = "Recurrent Mixture Density Network for Spatiotemporal Visual
                   Attention",
  author        = "Bazzani, Loris and Larochelle, Hugo and Torresani, Lorenzo",
  abstract      = "The high-dimensional and redundant nature of video have
                   pushed researchers to seek the design of attentional models
                   that can dynamically focus computations on the
                   spatiotemporal volumes that are most relevant. Specifically,
                   these models have been used to eliminate or down-weight
                   background pixels that are not important for the task at
                   hand. In order to deal with this problem, we propose an
                   attentional model that learns where to look in a video
                   directly from human fixation data. The proposed model
                   leverages deep 3D convolutional features to represent clip
                   segments in videos. This clip-level representation is
                   aggregated over time by a long short-term memory network
                   that connects into a mixture density network model of the
                   likely positions of fixations in each frame. The resulting
                   model is trained end to end using backpropagation. Our
                   experiments show state-of-the-art performance on saliency
                   prediction for videos. Experiments on Hollywood2 and UCF101
                   also show that the saliency can be used to improve
                   classification accuracy on action recognition tasks.",
  month         =  mar,
  year          =  2016,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "1603.08199",
  primaryClass  = "cs.CV",
  arxivid       = "1603.08199"
}

@ARTICLE{Johnson2016-yv,
  title         = "Google's Multilingual Neural Machine Translation System:
                   Enabling {Zero-Shot} Translation",
  author        = "Johnson, Melvin and Schuster, Mike and Le, Quoc V and
                   Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat,
                   Nikhil and Vi{\'e}gas, Fernanda and Wattenberg, Martin and
                   Corrado, Greg and Hughes, Macduff and Dean, Jeffrey",
  abstract      = "We propose a simple, elegant solution to use a single Neural
                   Machine Translation (NMT) model to translate between
                   multiple languages. Our solution requires no change in the
                   model architecture from our base system but instead
                   introduces an artificial token at the beginning of the input
                   sentence to specify the required target language. The rest
                   of the model, which includes encoder, decoder and attention,
                   remains unchanged and is shared across all languages. Using
                   a shared wordpiece vocabulary, our approach enables
                   Multilingual NMT using a single model without any increase
                   in parameters, which is significantly simpler than previous
                   proposals for Multilingual NMT. Our method often improves
                   the translation quality of all involved language pairs, even
                   while keeping the total number of model parameters constant.
                   On the WMT'14 benchmarks, a single multilingual model
                   achieves comparable performance for
                   English$\rightarrow$French and surpasses state-of-the-art
                   results for English$\rightarrow$German. Similarly, a single
                   multilingual model surpasses state-of-the-art results for
                   French$\rightarrow$English and German$\rightarrow$English on
                   WMT'14 and WMT'15 benchmarks respectively. On production
                   corpora, multilingual models of up to twelve language pairs
                   allow for better translation of many individual pairs. In
                   addition to improving the translation quality of language
                   pairs that the model was trained with, our models can also
                   learn to perform implicit bridging between language pairs
                   never seen explicitly during training, showing that transfer
                   learning and zero-shot translation is possible for neural
                   translation. Finally, we show analyses that hints at a
                   universal interlingua representation in our models and show
                   some interesting examples when mixing languages.",
  month         =  nov,
  year          =  2016,
  keywords      = "Machine Translation",
  archivePrefix = "arXiv",
  eprint        = "1611.04558",
  primaryClass  = "cs.CL",
  arxivid       = "1611.04558"
}

@ARTICLE{Gehring2016-yj,
  title         = "A Convolutional Encoder Model for Neural Machine Translation",
  author        = "Gehring, Jonas and Auli, Michael and Grangier, David and
                   Dauphin, Yann N",
  abstract      = "The prevalent approach to neural machine translation relies
                   on bi-directional LSTMs to encode the source sentence. In
                   this paper we present a faster and conceptually simpler
                   architecture based on a succession of convolutional layers.
                   This allows to encode the entire source sentence
                   simultaneously compared to recurrent networks for which
                   computation is constrained by temporal dependencies. We
                   achieve a new state-of-the-art on WMT'16 English-Romanian
                   translation and outperform several recently published
                   results on the WMT'15 English-German task. We also achieve
                   almost the same accuracy as a very deep LSTM setup on WMT'14
                   English-French translation. Our convolutional encoder speeds
                   up CPU decoding by more than two times at the same or higher
                   accuracy as a strong bi-directional LSTM baseline.",
  month         =  nov,
  year          =  2016,
  keywords      = "Machine Translation;CNN - Classification",
  archivePrefix = "arXiv",
  eprint        = "1611.02344",
  primaryClass  = "cs.CL",
  arxivid       = "1611.02344"
}

@ARTICLE{Lu2016-gz,
  title         = "Hierarchical {Question-Image} {Co-Attention} for Visual
                   Question Answering",
  author        = "Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh,
                   Devi",
  abstract      = "A number of recent works have proposed attention models for
                   Visual Question Answering (VQA) that generate spatial maps
                   highlighting image regions relevant to answering the
                   question. In this paper, we argue that in addition to
                   modeling ``where to look'' or visual attention, it is
                   equally important to model ``what words to listen to'' or
                   question attention. We present a novel co-attention model
                   for VQA that jointly reasons about image and question
                   attention. In addition, our model reasons about the question
                   (and consequently the image via the co-attention mechanism)
                   in a hierarchical fashion via a novel 1-dimensional
                   convolution neural networks (CNN). Our model improves the
                   state-of-the-art on the VQA dataset from 60.3\% to 60.5\%,
                   and from 61.6\% to 63.3\% on the COCO-QA dataset. By using
                   ResNet, the performance is further improved to 62.1\% for
                   VQA and 65.4\% for COCO-QA.",
  month         =  may,
  year          =  2016,
  keywords      = "Models;VisualQA/Captioning",
  archivePrefix = "arXiv",
  eprint        = "1606.00061",
  primaryClass  = "cs.CV",
  arxivid       = "1606.00061"
}

@ARTICLE{Recasens2016-zc,
  title         = "Following Gaze Across Views",
  author        = "Recasens, Adri{\`a} and Vondrick, Carl and Khosla, Aditya
                   and Torralba, Antonio",
  abstract      = "Following the gaze of people inside videos is an important
                   signal for understanding people and their actions. In this
                   paper, we present an approach for following gaze across
                   views by predicting where a particular person is looking
                   throughout a scene. We collect VideoGaze, a new dataset
                   which we use as a benchmark to both train and evaluate
                   models. Given one view with a person in it and a second view
                   of the scene, our model estimates a density for gaze
                   location in the second view. A key aspect of our approach is
                   an end-to-end model that solves the following sub-problems:
                   saliency, gaze pose, and geometric relationships between
                   views. Although our model is supervised only with gaze, we
                   show that the model learns to solve these subproblems
                   automatically without supervision. Experiments suggest that
                   our approach follows gaze better than standard baselines and
                   produces plausible results for everyday situations.",
  month         =  dec,
  year          =  2016,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "1612.03094",
  primaryClass  = "cs.CV",
  arxivid       = "1612.03094"
}

@ARTICLE{Zagoruyko2016-lt,
  title         = "Paying More Attention to Attention: Improving the
                   Performance of Convolutional Neural Networks via Attention
                   Transfer",
  author        = "Zagoruyko, Sergey and Komodakis, Nikos",
  abstract      = "Attention plays a critical role in human visual experience.
                   Furthermore, it has recently been demonstrated that
                   attention can also play an important role in the context of
                   applying artificial neural networks to a variety of tasks
                   from fields such as computer vision and NLP. In this work we
                   show that, by properly defining attention for convolutional
                   neural networks, we can actually use this type of
                   information in order to significantly improve the
                   performance of a student CNN network by forcing it to mimic
                   the attention maps of a powerful teacher network. To that
                   end, we propose several novel methods of transferring
                   attention, showing consistent improvement across a variety
                   of datasets and convolutional neural network architectures.",
  month         =  dec,
  year          =  2016,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "1612.03928",
  primaryClass  = "cs.CV",
  arxivid       = "1612.03928"
}

@ARTICLE{Turner2023-ln,
  title         = "An Introduction to Transformers",
  author        = "Turner, Richard E",
  abstract      = "The transformer is a neural network component that can be
                   used to learn useful representations of sequences or sets of
                   data-points. The transformer has driven recent advances in
                   natural language processing, computer vision, and
                   spatio-temporal modelling. There are many introductions to
                   transformers, but most do not contain precise mathematical
                   descriptions of the architecture and the intuitions behind
                   the design choices are often also missing. Moreover, as
                   research takes a winding path, the explanations for the
                   components of the transformer can be idiosyncratic. In this
                   note we aim for a mathematically precise, intuitive, and
                   clean description of the transformer architecture. We will
                   not discuss training as this is rather standard. We assume
                   that the reader is familiar with fundamental topics in
                   machine learning including multi-layer perceptrons, linear
                   transformations, softmax functions and basic probability.",
  month         =  apr,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2304.10557",
  primaryClass  = "cs.LG",
  arxivid       = "2304.10557"
}

@ARTICLE{Wang2017-ik,
  title         = "Residual Attention Network for Image Classification",
  author        = "Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo
                   and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and
                   Tang, Xiaoou",
  abstract      = "In this work, we propose ``Residual Attention Network'', a
                   convolutional neural network using attention mechanism which
                   can incorporate with state-of-art feed forward network
                   architecture in an end-to-end training fashion. Our Residual
                   Attention Network is built by stacking Attention Modules
                   which generate attention-aware features. The attention-aware
                   features from different modules change adaptively as layers
                   going deeper. Inside each Attention Module, bottom-up
                   top-down feedforward structure is used to unfold the
                   feedforward and feedback attention process into a single
                   feedforward process. Importantly, we propose attention
                   residual learning to train very deep Residual Attention
                   Networks which can be easily scaled up to hundreds of
                   layers. Extensive analyses are conducted on CIFAR-10 and
                   CIFAR-100 datasets to verify the effectiveness of every
                   module mentioned above. Our Residual Attention Network
                   achieves state-of-the-art object recognition performance on
                   three benchmark datasets including CIFAR-10 (3.90\% error),
                   CIFAR-100 (20.45\% error) and ImageNet (4.8\% single model
                   and single crop, top-5 error). Note that, our method
                   achieves 0.6\% top-1 accuracy improvement with 46\% trunk
                   depth and 69\% forward FLOPs comparing to ResNet-200. The
                   experiment also demonstrates that our network is robust
                   against noisy labels.",
  month         =  apr,
  year          =  2017,
  keywords      = "Models;CNN - Classification",
  archivePrefix = "arXiv",
  eprint        = "1704.06904",
  primaryClass  = "cs.CV",
  arxivid       = "1704.06904"
}

@ARTICLE{Gehring2017-oy,
  title         = "Convolutional Sequence to Sequence Learning",
  author        = "Gehring, Jonas and Auli, Michael and Grangier, David and
                   Yarats, Denis and Dauphin, Yann N",
  abstract      = "The prevalent approach to sequence to sequence learning maps
                   an input sequence to a variable length output sequence via
                   recurrent neural networks. We introduce an architecture
                   based entirely on convolutional neural networks. Compared to
                   recurrent models, computations over all elements can be
                   fully parallelized during training and optimization is
                   easier since the number of non-linearities is fixed and
                   independent of the input length. Our use of gated linear
                   units eases gradient propagation and we equip each decoder
                   layer with a separate attention module. We outperform the
                   accuracy of the deep LSTM setup of Wu et al. (2016) on both
                   WMT'14 English-German and WMT'14 English-French translation
                   at an order of magnitude faster speed, both on GPU and CPU.",
  month         =  may,
  year          =  2017,
  keywords      = "Public Code;Machine Translation",
  archivePrefix = "arXiv",
  eprint        = "1705.03122",
  primaryClass  = "cs.CL",
  arxivid       = "1705.03122"
}

@ARTICLE{Kaiser2017-ns,
  title         = "Depthwise Separable Convolutions for Neural Machine
                   Translation",
  author        = "Kaiser, Lukasz and Gomez, Aidan N and Chollet, Francois",
  abstract      = "Depthwise separable convolutions reduce the number of
                   parameters and computation used in convolutional operations
                   while increasing representational efficiency. They have been
                   shown to be successful in image classification models, both
                   in obtaining better models than previously possible for a
                   given parameter count (the Xception architecture) and
                   considerably reducing the number of parameters required to
                   perform at a given level (the MobileNets family of
                   architectures). Recently, convolutional sequence-to-sequence
                   networks have been applied to machine translation tasks with
                   good results. In this work, we study how depthwise separable
                   convolutions can be applied to neural machine translation.
                   We introduce a new architecture inspired by Xception and
                   ByteNet, called SliceNet, which enables a significant
                   reduction of the parameter count and amount of computation
                   needed to obtain results like ByteNet, and, with a similar
                   parameter count, achieves new state-of-the-art results. In
                   addition to showing that depthwise separable convolutions
                   perform well for machine translation, we investigate the
                   architectural changes that they enable: we observe that
                   thanks to depthwise separability, we can increase the length
                   of convolution windows, removing the need for filter
                   dilation. We also introduce a new ``super-separable''
                   convolution operation that further reduces the number of
                   parameters and computational cost for obtaining
                   state-of-the-art results.",
  month         =  jun,
  year          =  2017,
  keywords      = "Machine Translation",
  archivePrefix = "arXiv",
  eprint        = "1706.03059",
  primaryClass  = "cs.CL",
  arxivid       = "1706.03059"
}

@ARTICLE{Lample2017-ny,
  title         = "Unsupervised Machine Translation Using Monolingual Corpora
                   Only",
  author        = "Lample, Guillaume and Denoyer, Ludovic and Ranzato,
                   Marc'aurelio",
  abstract      = "Machine translation has recently achieved impressive
                   performance thanks to recent advances in deep learning and
                   the availability of large-scale parallel corpora. There have
                   been numerous attempts to extend these successes to
                   low-resource language pairs, yet requiring tens of thousands
                   of parallel sentences. In this work, we take this research
                   direction to the extreme and investigate whether it is
                   possible to learn to translate even without any parallel
                   data. We propose a model that takes sentences from
                   monolingual corpora in two different languages and maps them
                   into the same latent space. By learning to reconstruct in
                   both languages from this shared feature space, the model
                   effectively learns to translate without using any labeled
                   data. We demonstrate our model on two widely used datasets
                   and two language pairs, reporting BLEU scores up to 32.8,
                   without using even a single parallel sentence at training
                   time.",
  month         =  oct,
  year          =  2017,
  keywords      = "Machine Translation;GAN",
  archivePrefix = "arXiv",
  eprint        = "1711.00043",
  primaryClass  = "cs.CL",
  arxivid       = "1711.00043"
}

@ARTICLE{Lample2018-it,
  title         = "{Phrase-Based} \& Neural Unsupervised Machine Translation",
  author        = "Lample, Guillaume and Ott, Myle and Conneau, Alexis and
                   Denoyer, Ludovic and Ranzato, Marc'aurelio",
  abstract      = "Machine translation systems achieve near human-level
                   performance on some languages, yet their effectiveness
                   strongly relies on the availability of large amounts of
                   bitexts, which hinders their applicability to the majority
                   of language pairs. This work investigates how to learn to
                   translate when having access to only large monolingual
                   corpora in each language. We propose two model variants, a
                   neural and a phrase-based model. Both versions leverage
                   automatic generation of parallel data by backtranslating
                   with a backward model operating in the other direction, and
                   the denoising effect of a language model trained on the
                   target side. These models are significantly better than
                   methods from the literature, while being simpler and having
                   fewer hyper-parameters. On the widely used WMT14
                   English-French and WMT16 German-English benchmarks, our
                   models respectively obtain 27.1 and 23.6 BLEU points without
                   using a single parallel sentence, outperforming the state of
                   the art by more than 11 BLEU points.",
  month         =  apr,
  year          =  2018,
  keywords      = "Machine Translation;Not Supervised Models",
  archivePrefix = "arXiv",
  eprint        = "1804.07755",
  primaryClass  = "cs.CL",
  arxivid       = "1804.07755"
}

@ARTICLE{Kalchbrenner2016-gn,
  title         = "Neural Machine Translation in Linear Time",
  author        = "Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen
                   and van den Oord, Aaron and Graves, Alex and Kavukcuoglu,
                   Koray",
  abstract      = "We present a neural architecture for sequence processing.
                   The ByteNet is a stack of two dilated convolutional neural
                   networks, one to encode the source sequence and one to
                   decode the target sequence, where the target network unfolds
                   dynamically to generate variable length outputs. The ByteNet
                   has two core properties: it runs in time that is linear in
                   the length of the sequences and it preserves the sequences'
                   temporal resolution. The ByteNet decoder attains
                   state-of-the-art performance on character-level language
                   modelling and outperforms the previous best results obtained
                   with recurrent neural networks. The ByteNet also achieves a
                   performance on raw character-level machine translation that
                   approaches that of the best neural translation models that
                   run in quadratic time. The implicit structure learnt by the
                   ByteNet mirrors the expected alignments between the
                   sequences.",
  month         =  oct,
  year          =  2016,
  keywords      = "Machine Translation",
  archivePrefix = "arXiv",
  eprint        = "1610.10099",
  primaryClass  = "cs.CL",
  arxivid       = "1610.10099"
}

@ARTICLE{Graves2013-di,
  title         = "Generating Sequences With Recurrent Neural Networks",
  author        = "Graves, Alex",
  abstract      = "This paper shows how Long Short-term Memory recurrent neural
                   networks can be used to generate complex sequences with
                   long-range structure, simply by predicting one data point at
                   a time. The approach is demonstrated for text (where the
                   data are discrete) and online handwriting (where the data
                   are real-valued). It is then extended to handwriting
                   synthesis by allowing the network to condition its
                   predictions on a text sequence. The resulting system is able
                   to generate highly realistic cursive handwriting in a wide
                   variety of styles.",
  month         =  aug,
  year          =  2013,
  keywords      = "Spatio-Temporal Data ;Finance;Models;NLP",
  archivePrefix = "arXiv",
  eprint        = "1308.0850",
  primaryClass  = "cs.NE",
  arxivid       = "1308.0850"
}

@ARTICLE{Yao2015-fc,
  title         = "Describing Videos by Exploiting Temporal Structure",
  author        = "Yao, Li and Torabi, Atousa and Cho, Kyunghyun and Ballas,
                   Nicolas and Pal, Christopher and Larochelle, Hugo and
                   Courville, Aaron",
  abstract      = "Recent progress in using recurrent neural networks (RNNs)
                   for image description has motivated the exploration of their
                   application for video description. However, while images are
                   static, working with videos requires modeling their dynamic
                   temporal structure and then properly integrating that
                   information into a natural language description. In this
                   context, we propose an approach that successfully takes into
                   account both the local and global temporal structure of
                   videos to produce descriptions. First, our approach
                   incorporates a spatial temporal 3-D convolutional neural
                   network (3-D CNN) representation of the short temporal
                   dynamics. The 3-D CNN representation is trained on video
                   action recognition tasks, so as to produce a representation
                   that is tuned to human motion and behavior. Second we
                   propose a temporal attention mechanism that allows to go
                   beyond local temporal modeling and learns to automatically
                   select the most relevant temporal segments given the
                   text-generating RNN. Our approach exceeds the current
                   state-of-art for both BLEU and METEOR metrics on the
                   Youtube2Text dataset. We also present results on a new,
                   larger and more challenging dataset of paired video and
                   natural language descriptions.",
  month         =  feb,
  year          =  2015,
  keywords      = "VisualQA/Captioning;Spatio-Temporal Data ;Models;CNN -
                   Classification",
  archivePrefix = "arXiv",
  eprint        = "1502.08029",
  primaryClass  = "stat.ML",
  arxivid       = "1502.08029"
}

@ARTICLE{Zhao2023-bf,
  title         = "A Survey of Large Language Models",
  author        = "Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi
                   and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and
                   Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du,
                   Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and
                   Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu
                   and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen,
                   Ji-Rong",
  abstract      = "Language is essentially a complex, intricate system of human
                   expressions governed by grammatical rules. It poses a
                   significant challenge to develop capable AI algorithms for
                   comprehending and grasping a language. As a major approach,
                   language modeling has been widely studied for language
                   understanding and generation in the past two decades,
                   evolving from statistical language models to neural language
                   models. Recently, pre-trained language models (PLMs) have
                   been proposed by pre-training Transformer models over
                   large-scale corpora, showing strong capabilities in solving
                   various NLP tasks. Since researchers have found that model
                   scaling can lead to performance improvement, they further
                   study the scaling effect by increasing the model size to an
                   even larger size. Interestingly, when the parameter scale
                   exceeds a certain level, these enlarged language models not
                   only achieve a significant performance improvement but also
                   show some special abilities that are not present in
                   small-scale language models. To discriminate the difference
                   in parameter scale, the research community has coined the
                   term large language models (LLM) for the PLMs of significant
                   size. Recently, the research on LLMs has been largely
                   advanced by both academia and industry, and a remarkable
                   progress is the launch of ChatGPT, which has attracted
                   widespread attention from society. The technical evolution
                   of LLMs has been making an important impact on the entire AI
                   community, which would revolutionize the way how we develop
                   and use AI algorithms. In this survey, we review the recent
                   advances of LLMs by introducing the background, key
                   findings, and mainstream techniques. In particular, we focus
                   on four major aspects of LLMs, namely pre-training,
                   adaptation tuning, utilization, and capacity evaluation.
                   Besides, we also summarize the available resources for
                   developing LLMs and discuss the remaining issues for future
                   directions.",
  month         =  mar,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2303.18223v13",
  primaryClass  = "cs.CL",
  arxivid       = "2303.18223v13"
}

@ARTICLE{Girdhar2018-lb,
  title         = "Video Action Transformer Network",
  author        = "Girdhar, Rohit and Carreira, Jo{\~a}o and Doersch, Carl and
                   Zisserman, Andrew",
  abstract      = "We introduce the Action Transformer model for recognizing
                   and localizing human actions in video clips. We repurpose a
                   Transformer-style architecture to aggregate features from
                   the spatiotemporal context around the person whose actions
                   we are trying to classify. We show that by using
                   high-resolution, person-specific, class-agnostic queries,
                   the model spontaneously learns to track individual people
                   and to pick up on semantic context from the actions of
                   others. Additionally its attention mechanism learns to
                   emphasize hands and faces, which are often crucial to
                   discriminate an action - all without explicit supervision
                   other than boxes and class labels. We train and test our
                   Action Transformer network on the Atomic Visual Actions
                   (AVA) dataset, outperforming the state-of-the-art by a
                   significant margin - more than 7.5\% absolute (40\%
                   relative) improvement, using only raw RGB frames as input.",
  month         =  dec,
  year          =  2018,
  keywords      = "Video;Multimodal (Vision, speech, etc)",
  archivePrefix = "arXiv",
  eprint        = "1812.02707",
  primaryClass  = "cs.CV",
  arxivid       = "1812.02707"
}

@ARTICLE{Jang2023-rp,
  title         = "Consistency Analysis of {ChatGPT}",
  author        = "Jang, Myeongjun and Lukasiewicz, Thomas",
  abstract      = "ChatGPT, a question-and-answer dialogue system based on a
                   large language model, has gained huge popularity since its
                   introduction. Its positive aspects have been reported
                   through many media platforms, and some analyses even showed
                   that ChatGPT achieved a decent grade in professional exams,
                   including the law, medical, and finance domains, adding
                   extra support to the claim that AI now can assist and, even,
                   replace humans in industrial fields. Others, however, doubt
                   its reliability and trustworthiness. In this paper, we
                   investigate ChatGPT's trustworthiness regarding logically
                   consistent behaviours. Our findings suggest that, although
                   ChatGPT seems to achieve an improved language understanding
                   ability, it still fails to generate logically correct
                   predictions frequently. Hence, while it is true that ChatGPT
                   is an impressive and promising new technique, we conclude
                   that its usage in real-world applications without thorough
                   human inspection requires further consideration, especially
                   for risk-sensitive areas.",
  month         =  mar,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2303.06273",
  primaryClass  = "cs.CL",
  arxivid       = "2303.06273"
}

@ARTICLE{Chalmers2023-ik,
  title         = "Could a Large Language Model be Conscious?",
  author        = "Chalmers, David J",
  abstract      = "There has recently been widespread discussion of whether
                   large language models might be sentient or conscious. Should
                   we take this idea seriously? I will break down the strongest
                   reasons for and against. Given mainstream assumptions in the
                   science of consciousness, there are significant obstacles to
                   consciousness in current models: for example, their lack of
                   recurrent processing, a global workspace, and unified
                   agency. At the same time, it is quite possible that these
                   obstacles will be overcome in the next decade or so. I
                   conclude that while it is somewhat unlikely that current
                   large language models are conscious, we should take
                   seriously the possibility that successors to large language
                   models may be conscious in the not-too-distant future.",
  month         =  mar,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2303.07103",
  primaryClass  = "cs.AI",
  arxivid       = "2303.07103"
}

@ARTICLE{You2019-fe,
  title         = "Reducing {BERT} {Pre-Training} Time from 3 Days to 76
                   Minutes",
  author        = "You, Yang and Li, Jing and Hseu, Jonathan and Song, Xiaodan
                   and Demmel, James and Hsieh, Cho-Jui",
  abstract      = "Large-batch training is key to speeding up deep neural
                   network training in large distributed systems. However,
                   large-batch training is difficult because it produces a
                   generalization gap. Straightforward optimization often leads
                   to accuracy loss on the test set. BERT
                   \textbackslashcite\{devlin2018bert\} is a state-of-the-art
                   deep learning model that builds on top of deep bidirectional
                   transformers for language understanding. Previous
                   large-batch training techniques do not perform well for BERT
                   when we scale the batch size (e.g. beyond 8192). BERT
                   pre-training also takes a long time to finish (around three
                   days on 16 TPUv3 chips). To solve this problem, we propose
                   the LAMB optimizer, which helps us to scale the batch size
                   to 65536 without losing accuracy. LAMB is a general
                   optimizer that works for both small and large batch sizes
                   and does not need hyper-parameter tuning besides the
                   learning rate. The baseline BERT-Large model needs 1 million
                   iterations to finish pre-training, while LAMB with batch
                   size 65536/32768 only needs 8599 iterations. We push the
                   batch size to the memory limit of a TPUv3 pod and can finish
                   BERT training in 76 minutes.",
  month         =  apr,
  year          =  2019,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "1904.00962",
  primaryClass  = "cs.LG",
  arxivid       = "1904.00962"
}

@ARTICLE{Dehghani2018-hc,
  title         = "Universal Transformers",
  author        = "Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and
                   Uszkoreit, Jakob and Kaiser, {\L}ukasz",
  abstract      = "Recurrent neural networks (RNNs) sequentially process data
                   by updating their state with each new data point, and have
                   long been the de facto choice for sequence modeling tasks.
                   However, their inherently sequential computation makes them
                   slow to train. Feed-forward and convolutional architectures
                   have recently been shown to achieve superior results on some
                   sequence modeling tasks such as machine translation, with
                   the added advantage that they concurrently process all
                   inputs in the sequence, leading to easy parallelization and
                   faster training times. Despite these successes, however,
                   popular feed-forward sequence models like the Transformer
                   fail to generalize in many simple tasks that recurrent
                   models handle with ease, e.g. copying strings or even simple
                   logical inference when the string or formula lengths exceed
                   those observed at training time. We propose the Universal
                   Transformer (UT), a parallel-in-time self-attentive
                   recurrent sequence model which can be cast as a
                   generalization of the Transformer model and which addresses
                   these issues. UTs combine the parallelizability and global
                   receptive field of feed-forward sequence models like the
                   Transformer with the recurrent inductive bias of RNNs. We
                   also add a dynamic per-position halting mechanism and find
                   that it improves accuracy on several tasks. In contrast to
                   the standard Transformer, under certain assumptions, UTs can
                   be shown to be Turing-complete. Our experiments show that
                   UTs outperform standard Transformers on a wide range of
                   algorithmic and language understanding tasks, including the
                   challenging LAMBADA language modeling task where UTs achieve
                   a new state of the art, and machine translation where UTs
                   achieve a 0.9 BLEU improvement over Transformers on the
                   WMT14 En-De dataset.",
  month         =  jul,
  year          =  2018,
  keywords      = "Language;LLMs",
  archivePrefix = "arXiv",
  eprint        = "1807.03819",
  primaryClass  = "cs.CL",
  arxivid       = "1807.03819"
}

@ARTICLE{Wang2023-vm,
  title         = "Is {ChatGPT} a Good {NLG} Evaluator? A Preliminary Study",
  author        = "Wang, Jiaan and Liang, Yunlong and Meng, Fandong and Sun,
                   Zengkui and Shi, Haoxiang and Li, Zhixu and Xu, Jinan and
                   Qu, Jianfeng and Zhou, Jie",
  abstract      = "Recently, the emergence of ChatGPT has attracted wide
                   attention from the computational linguistics community. Many
                   prior studies have shown that ChatGPT achieves remarkable
                   performance on various NLP tasks in terms of automatic
                   evaluation metrics. However, the ability of ChatGPT to serve
                   as an evaluation metric is still underexplored. Considering
                   assessing the quality of natural language generation (NLG)
                   models is an arduous task and NLG metrics notoriously show
                   their poor correlation with human judgments, we wonder
                   whether ChatGPT is a good NLG evaluation metric. In this
                   report, we provide a preliminary meta-evaluation on ChatGPT
                   to show its reliability as an NLG metric. In detail, we
                   regard ChatGPT as a human evaluator and give task-specific
                   (e.g., summarization) and aspect-specific (e.g., relevance)
                   instruction to prompt ChatGPT to evaluate the generated
                   results of NLG models. We conduct experiments on five NLG
                   meta-evaluation datasets (including summarization, story
                   generation and data-to-text tasks). Experimental results
                   show that compared with previous automatic metrics, ChatGPT
                   achieves state-of-the-art or competitive correlation with
                   human judgments in most cases. In addition, we find that the
                   effectiveness of the ChatGPT evaluator might be influenced
                   by the creation method of the meta-evaluation datasets. For
                   the meta-evaluation datasets which are created greatly
                   depending on the reference and thus are biased, the ChatGPT
                   evaluator might lose its effectiveness. We hope our
                   preliminary study could prompt the emergence of a
                   general-purposed reliable NLG metric.",
  month         =  mar,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2303.04048",
  primaryClass  = "cs.CL",
  arxivid       = "2303.04048"
}

@ARTICLE{noauthor_undated-uo,
  title    = "Multiplicative Interactions and Where to Find Them",
  author   = "Jayakumar, Siddhant M and Czarnecki, Wojciech M and Menick, Jacob
              and Schwarz, Jonathan and Rae, Jack and Osindero, Simon and Teh,
              Yee Whye and Harley, Tim and Pascanu, Razvan",
  abstract = "We explore the role of multiplicative interaction as a unifying
              framework to describe a range of classical and modern neural
              network architectural motifs, such as gating, attention layers,
              hypernetworks, and dynamic convolutions amongst others.
              Multiplicative interaction layers as primitive operations have a
              long-established presence in the literature, though this often
              not emphasized and thus under-appreciated. We begin by showing
              that such layers strictly enrich the representable function
              classes of neural networks. We conjecture that multiplicative
              interactions offer a particularly powerful inductive bias when
              fusing multiple streams of information or when conditional
              computation is required. We therefore argue that they should be
              considered in many situation where multiple compute or
              information paths need to be combined, in place of the simple and
              oft-used concatenation operation. Finally, we back up our claims
              and demonstrate the potential of multiplicative interactions by
              applying them in large-scale complex RL and sequence modelling
              tasks, where their use allows us to deliver state-of-the-art
              results, and thereby provides new evidence in support of
              multiplicative interactions playing a more prominent role when
              designing new neural network architectures.",
  month    =  sep,
  year     =  2019,
  keywords = "LLMs"
}

@ARTICLE{Ramachandran2019-ml,
  title         = "{Stand-Alone} {Self-Attention} in Vision Models",
  author        = "Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish
                   and Bello, Irwan and Levskaya, Anselm and Shlens, Jonathon",
  abstract      = "Convolutions are a fundamental building block of modern
                   computer vision systems. Recent approaches have argued for
                   going beyond convolutions in order to capture long-range
                   dependencies. These efforts focus on augmenting
                   convolutional models with content-based interactions, such
                   as self-attention and non-local means, to achieve gains on a
                   number of vision tasks. The natural question that arises is
                   whether attention can be a stand-alone primitive for vision
                   models instead of serving as just an augmentation on top of
                   convolutions. In developing and testing a pure
                   self-attention vision model, we verify that self-attention
                   can indeed be an effective stand-alone layer. A simple
                   procedure of replacing all instances of spatial convolutions
                   with a form of self-attention applied to ResNet model
                   produces a fully self-attentional model that outperforms the
                   baseline on ImageNet classification with 12\% fewer FLOPS
                   and 29\% fewer parameters. On COCO object detection, a pure
                   self-attention model matches the mAP of a baseline RetinaNet
                   while having 39\% fewer FLOPS and 34\% fewer parameters.
                   Detailed ablation studies demonstrate that self-attention is
                   especially impactful when used in later layers. These
                   results establish that stand-alone self-attention is an
                   important addition to the vision practitioner's toolbox.",
  month         =  jun,
  year          =  2019,
  keywords      = "Models;Vision;LLMs",
  archivePrefix = "arXiv",
  eprint        = "1906.05909",
  primaryClass  = "cs.CV",
  arxivid       = "1906.05909"
}

@ARTICLE{Cordonnier2019-az,
  title         = "On the Relationship between {Self-Attention} and
                   Convolutional Layers",
  author        = "Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi,
                   Martin",
  abstract      = "Recent trends of incorporating attention mechanisms in
                   vision have led researchers to reconsider the supremacy of
                   convolutional layers as a primary building block. Beyond
                   helping CNNs to handle long-range dependencies, Ramachandran
                   et al. (2019) showed that attention can completely replace
                   convolution and achieve state-of-the-art performance on
                   vision tasks. This raises the question: do learned attention
                   layers operate similarly to convolutional layers? This work
                   provides evidence that attention layers can perform
                   convolution and, indeed, they often learn to do so in
                   practice. Specifically, we prove that a multi-head
                   self-attention layer with sufficient number of heads is at
                   least as expressive as any convolutional layer. Our
                   numerical experiments then show that self-attention layers
                   attend to pixel-grid patterns similarly to CNN layers,
                   corroborating our analysis. Our code is publicly available.",
  month         =  nov,
  year          =  2019,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "1911.03584",
  primaryClass  = "cs.LG",
  arxivid       = "1911.03584"
}

@MISC{Huyen2023-qy,
  title        = "Building {LLM} applications for production",
  author       = "Huyen, Chip",
  abstract     = "[Hacker News discussion, LinkedIn discussion, Twitter thread]",
  month        =  apr,
  year         =  2023,
  howpublished = "\url{https://huyenchip.com/2023/04/11/llm-engineering.html}",
  note         = "Accessed: 2023-5-2",
  keywords     = "LLMs",
  language     = "en"
}

@ARTICLE{Jain2023-ew,
  title         = "Bring your own data! Self-supervised evaluation for Large
                   Language Models",
  author        = "Jain, Neel and Saifullah, Khalid and Wen, Yuxin and
                   Kirchenbauer, John and Shu, Manli and Saha, Aniruddha and
                   Goldblum, Micah and Geiping, Jonas and Goldstein, Tom",
  abstract      = "With the rise of Large Language Models (LLMs) and their
                   ubiquitous deployment in diverse domains, measuring language
                   model behavior on realistic data is imperative. For example,
                   a company deploying a client-facing chatbot must ensure that
                   the model will not respond to client requests with
                   profanity. Current evaluations approach this problem using
                   small, domain-specific datasets with human-curated labels.
                   These evaluation sets are often sampled from a narrow and
                   simplified distribution, and data sources can unknowingly be
                   leaked into the training set which can lead to misleading
                   evaluations. To bypass these drawbacks, we propose a
                   framework for self-supervised evaluation of LLMs by
                   analyzing their sensitivity or invariance to transformations
                   on the input text. Self-supervised evaluation can directly
                   monitor LLM behavior on datasets collected in the wild or
                   streamed during live model deployment. We demonstrate
                   self-supervised evaluation strategies for measuring
                   closed-book knowledge, toxicity, and long-range context
                   dependence, in addition to sensitivity to grammatical
                   structure and tokenization errors. When comparisons to
                   similar human-labeled benchmarks are available, we find
                   strong correlations between self-supervised and
                   human-supervised evaluations. The self-supervised paradigm
                   complements current evaluation strategies that rely on
                   labeled data.",
  month         =  jun,
  year          =  2023,
  keywords      = "Evaluation",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2306.13651",
  primaryClass  = "cs.CL",
  arxivid       = "2306.13651"
}

@INPROCEEDINGS{Papineni2002-zk,
  title     = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
  booktitle = "Proceedings of the 40th Annual Meeting of the Association for
               Computational Linguistics",
  author    = "Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu,
               Wei-Jing",
  publisher = "Association for Computational Linguistics",
  pages     = "311--318",
  month     =  jul,
  year      =  2002,
  address   = "Philadelphia, Pennsylvania, USA",
  keywords  = "Evaluation",
  doi       = "10.3115/1073083.1073135"
}

@ARTICLE{Kojima2022-cd,
  title         = "Large Language Models are {Zero-Shot} Reasoners",
  author        = "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and
                   Matsuo, Yutaka and Iwasawa, Yusuke",
  abstract      = "Pretrained large language models (LLMs) are widely used in
                   many sub-fields of natural language processing (NLP) and
                   generally known as excellent few-shot learners with
                   task-specific exemplars. Notably, chain of thought (CoT)
                   prompting, a recent technique for eliciting complex
                   multi-step reasoning through step-by-step answer examples,
                   achieved the state-of-the-art performances in arithmetics
                   and symbolic reasoning, difficult system-2 tasks that do not
                   follow the standard scaling laws for LLMs. While these
                   successes are often attributed to LLMs' ability for few-shot
                   learning, we show that LLMs are decent zero-shot reasoners
                   by simply adding ``Let's think step by step'' before each
                   answer. Experimental results demonstrate that our
                   Zero-shot-CoT, using the same single prompt template,
                   significantly outperforms zero-shot LLM performances on
                   diverse benchmark reasoning tasks including arithmetics
                   (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning
                   (Last Letter, Coin Flip), and other logical reasoning tasks
                   (Date Understanding, Tracking Shuffled Objects), without any
                   hand-crafted few-shot examples, e.g. increasing the accuracy
                   on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to
                   40.7\% with large InstructGPT model (text-davinci-002), as
                   well as similar magnitudes of improvements with another
                   off-the-shelf large model, 540B parameter PaLM. The
                   versatility of this single prompt across very diverse
                   reasoning tasks hints at untapped and understudied
                   fundamental zero-shot capabilities of LLMs, suggesting
                   high-level, multi-task broad cognitive capabilities may be
                   extracted by simple prompting. We hope our work not only
                   serves as the minimal strongest zero-shot baseline for the
                   challenging reasoning benchmarks, but also highlights the
                   importance of carefully exploring and analyzing the enormous
                   zero-shot knowledge hidden inside LLMs before crafting
                   finetuning datasets or few-shot exemplars.",
  month         =  may,
  year          =  2022,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2205.11916",
  primaryClass  = "cs.CL",
  arxivid       = "2205.11916"
}

@MISC{noauthor_undated-ca,
  howpublished = "\url{https://arxiv.org/abs/2309.07852}",
  note         = "Accessed: 2023-10-3",
  keywords     = "LLMs"
}

@MISC{noauthor_undated-rg,
  title    = "1000037815",
  keywords = "Evaluation"
}

@ARTICLE{Ding2023-ri,
  title     = "Parameter-efficient fine-tuning of large-scale pre-trained
               language models",
  author    = "Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and
               Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin
               and Chan, Chi-Min and Chen, Weize and Yi, Jing and Zhao, Weilin
               and Wang, Xiaozhi and Liu, Zhiyuan and Zheng, Hai-Tao and Chen,
               Jianfei and Liu, Yang and Tang, Jie and Li, Juanzi and Sun,
               Maosong",
  abstract  = "With the prevalence of pre-trained language models (PLMs) and
               the pre-training--fine-tuning paradigm, it has been continuously
               shown that larger models tend to yield better performance.
               However, as PLMs scale up, fine-tuning and storing all the
               parameters is prohibitively costly and eventually becomes
               practically infeasible. This necessitates a new branch of
               research focusing on the parameter-efficient adaptation of PLMs,
               which optimizes a small portion of the model parameters while
               keeping the rest fixed, drastically cutting down computation and
               storage costs. In general, it demonstrates that large-scale
               models could be effectively stimulated by the optimization of a
               few parameters. Despite the various designs, here we discuss and
               analyse the approaches under a more consistent and accessible
               term `delta-tuning', where `delta' a mathematical notation often
               used to denote changes, is borrowed to refer to the portion of
               parameters that are `changed' during training. We formally
               describe the problem and propose a unified categorization
               criterion for existing delta-tuning methods to explore their
               correlations and differences. We also discuss the theoretical
               principles underlying the effectiveness of delta-tuning and
               interpret them from the perspectives of optimization and optimal
               control. Furthermore, we provide a holistic empirical study on
               over 100 natural language processing tasks and investigate
               various aspects of delta-tuning. With comprehensive study and
               analysis, our research demonstrates the theoretical and
               practical properties of delta-tuning in the adaptation of PLMs.
               Training a deep neural network can be costly but training time
               is reduced when a pre-trained network can be adapted to
               different use cases. Ideally, only a small number of parameters
               needs to be changed in this process of fine-tuning, which can
               then be more easily distributed. In this Analysis, different
               methods of fine-tuning with only a small number of parameters
               are compared on a large set of natural language processing
               tasks.",
  journal   = "Nature Machine Intelligence",
  publisher = "Nature Publishing Group",
  volume    =  5,
  number    =  3,
  pages     = "220--235",
  month     =  mar,
  year      =  2023,
  keywords  = "Efficient;Fine-tuning",
  language  = "en",
  issn      = "2522-5839, 2522-5839",
  doi       = "10.1038/s42256-023-00626-4"
}

@ARTICLE{Dai_undated-zf,
  title    = "Emu: Enhancing Image Generation Models Using Photogenic Needles
              in a Haystack",
  author   = "Dai, Xiaoliang and Hou, Ji and Ma, Chih-Yao and Tsai, Sam and
              Wang, Jialiang and Wang, Rui and Zhang, Peizhao and Vandenhende,
              Simon and Wang, Xiaofang and Dubey, Abhimanyu and Yu, Matthew and
              Kadian, Abhishek and Radenovic, Filip and Mahajan, Dhruv and Li,
              Kunpeng and Zhao, Yue and Petrovic, Vladan and Singh, Mitesh
              Kumar and Motwani, Simran and Wen, Yi and Song, Yiwen and
              Sumbaly, Roshan and Ramanathan, Vignesh and He, Zijian and Vajda,
              Peter and Parikh, Devi",
  keywords = "Multimodal (Vision, speech, etc)"
}

@ARTICLE{Kirillov2023-xe,
  title         = "Segment Anything",
  author        = "Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and
                   Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao,
                   Tete and Whitehead, Spencer and Berg, Alexander C and Lo,
                   Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross",
  abstract      = "We introduce the Segment Anything (SA) project: a new task,
                   model, and dataset for image segmentation. Using our
                   efficient model in a data collection loop, we built the
                   largest segmentation dataset to date (by far), with over 1
                   billion masks on 11M licensed and privacy respecting images.
                   The model is designed and trained to be promptable, so it
                   can transfer zero-shot to new image distributions and tasks.
                   We evaluate its capabilities on numerous tasks and find that
                   its zero-shot performance is impressive -- often competitive
                   with or even superior to prior fully supervised results. We
                   are releasing the Segment Anything Model (SAM) and
                   corresponding dataset (SA-1B) of 1B masks and 11M images at
                   https://segment-anything.com to foster research into
                   foundation models for computer vision.",
  month         =  apr,
  year          =  2023,
  keywords      = "Multimodal (Vision, speech, etc)",
  archivePrefix = "arXiv",
  eprint        = "2304.02643",
  primaryClass  = "cs.CV",
  arxivid       = "2304.02643"
}

@ARTICLE{Ji2022-hz,
  title         = "Survey of Hallucination in Natural Language Generation",
  author        = "Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng
                   and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin
                   and Dai, Wenliang and Madotto, Andrea and Fung, Pascale",
  abstract      = "Natural Language Generation (NLG) has improved exponentially
                   in recent years thanks to the development of
                   sequence-to-sequence deep learning technologies such as
                   Transformer-based language models. This advancement has led
                   to more fluent and coherent NLG, leading to improved
                   development in downstream tasks such as abstractive
                   summarization, dialogue generation and data-to-text
                   generation. However, it is also apparent that deep learning
                   based generation is prone to hallucinate unintended text,
                   which degrades the system performance and fails to meet user
                   expectations in many real-world scenarios. To address this
                   issue, many studies have been presented in measuring and
                   mitigating hallucinated texts, but these have never been
                   reviewed in a comprehensive manner before. In this survey,
                   we thus provide a broad overview of the research progress
                   and challenges in the hallucination problem in NLG. The
                   survey is organized into two parts: (1) a general overview
                   of metrics, mitigation methods, and future directions; and
                   (2) an overview of task-specific research progress on
                   hallucinations in the following downstream tasks, namely
                   abstractive summarization, dialogue generation, generative
                   question answering, data-to-text generation, machine
                   translation, and visual-language generation. This survey
                   serves to facilitate collaborative efforts among researchers
                   in tackling the challenge of hallucinated texts in NLG.",
  month         =  feb,
  year          =  2022,
  keywords      = "Hallucination",
  archivePrefix = "arXiv",
  eprint        = "2202.03629",
  primaryClass  = "cs.CL",
  arxivid       = "2202.03629"
}

@ARTICLE{Bengio2003-zb,
  title    = "A Neural Probabilistic Language Model",
  author   = "Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and
              Jauvin, Christian",
  journal  = "J. Mach. Learn. Res.",
  volume   =  3,
  pages    = "1137--1155",
  year     =  2003,
  keywords = "Classic Papers;Language;NLP;LLMs",
  issn     = "1532-4435"
}

@ARTICLE{Sharma2023-tt,
  title         = "Towards Understanding Sycophancy in Language Models",
  author        = "Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and
                   Duvenaud, David and Askell, Amanda and Bowman, Samuel R and
                   Cheng, Newton and Durmus, Esin and Hatfield-Dodds, Zac and
                   Johnston, Scott R and Kravec, Shauna and Maxwell, Timothy
                   and McCandlish, Sam and Ndousse, Kamal and Rausch, Oliver
                   and Schiefer, Nicholas and Yan, Da and Zhang, Miranda and
                   Perez, Ethan",
  abstract      = "Human feedback is commonly utilized to finetune AI
                   assistants. But human feedback may also encourage model
                   responses that match user beliefs over truthful ones, a
                   behaviour known as sycophancy. We investigate the prevalence
                   of sycophancy in models whose finetuning procedure made use
                   of human feedback, and the potential role of human
                   preference judgments in such behavior. We first demonstrate
                   that five state-of-the-art AI assistants consistently
                   exhibit sycophancy across four varied free-form
                   text-generation tasks. To understand if human preferences
                   drive this broadly observed behavior, we analyze existing
                   human preference data. We find that when a response matches
                   a user's views, it is more likely to be preferred. Moreover,
                   both humans and preference models (PMs) prefer
                   convincingly-written sycophantic responses over correct ones
                   a non-negligible fraction of the time. Optimizing model
                   outputs against PMs also sometimes sacrifices truthfulness
                   in favor of sycophancy. Overall, our results indicate that
                   sycophancy is a general behavior of state-of-the-art AI
                   assistants, likely driven in part by human preference
                   judgments favoring sycophantic responses.",
  month         =  oct,
  year          =  2023,
  keywords      = "Hallucination",
  archivePrefix = "arXiv",
  eprint        = "2310.13548",
  primaryClass  = "cs.CL",
  arxivid       = "2310.13548"
}

@ARTICLE{Weston2023-wg,
  title         = "System 2 Attention (is something you might need too)",
  author        = "Weston, Jason and Sukhbaatar, Sainbayar",
  abstract      = "Soft attention in Transformer-based Large Language Models
                   (LLMs) is susceptible to incorporating irrelevant
                   information from the context into its latent
                   representations, which adversely affects next token
                   generations. To help rectify these issues, we introduce
                   System 2 Attention (S2A), which leverages the ability of
                   LLMs to reason in natural language and follow instructions
                   in order to decide what to attend to. S2A regenerates the
                   input context to only include the relevant portions, before
                   attending to the regenerated context to elicit the final
                   response. In experiments, S2A outperforms standard
                   attention-based LLMs on three tasks containing opinion or
                   irrelevant information, QA, math word problems and longform
                   generation, where S2A increases factuality and objectivity,
                   and decreases sycophancy.",
  month         =  nov,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2311.11829",
  primaryClass  = "cs.CL",
  arxivid       = "2311.11829"
}

@ARTICLE{Chen2023-xv,
  title         = "{ChatGPT's} One-year Anniversary: Are {Open-Source} Large
                   Language Models Catching up?",
  author        = "Chen, Hailin and Jiao, Fangkai and Li, Xingxuan and Qin,
                   Chengwei and Ravaut, Mathieu and Zhao, Ruochen and Xiong,
                   Caiming and Joty, Shafiq",
  abstract      = "Upon its release in late 2022, ChatGPT has brought a seismic
                   shift in the entire landscape of AI, both in research and
                   commerce. Through instruction-tuning a large language model
                   (LLM) with supervised fine-tuning and reinforcement learning
                   from human feedback, it showed that a model could answer
                   human questions and follow instructions on a broad panel of
                   tasks. Following this success, interests in LLMs have
                   intensified, with new LLMs flourishing at frequent interval
                   across academia and industry, including many start-ups
                   focused on LLMs. While closed-source LLMs (e.g., OpenAI's
                   GPT, Anthropic's Claude) generally outperform their
                   open-source counterparts, the progress on the latter has
                   been rapid with claims of achieving parity or even better on
                   certain tasks. This has crucial implications not only on
                   research but also on business. In this work, on the first
                   anniversary of ChatGPT, we provide an exhaustive overview of
                   this success, surveying all tasks where an open-source LLM
                   has claimed to be on par or better than ChatGPT.",
  month         =  nov,
  year          =  2023,
  keywords      = "Survey/Review Paper;LLMs",
  archivePrefix = "arXiv",
  eprint        = "2311.16989",
  primaryClass  = "cs.CL",
  arxivid       = "2311.16989"
}

@ARTICLE{Lewis2020-wz,
  title         = "{Retrieval-Augmented} Generation for {Knowledge-Intensive}
                   {NLP} Tasks",
  author        = "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and
                   Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and
                   K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-Tau and
                   Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe",
  abstract      = "Large pre-trained language models have been shown to store
                   factual knowledge in their parameters, and achieve
                   state-of-the-art results when fine-tuned on downstream NLP
                   tasks. However, their ability to access and precisely
                   manipulate knowledge is still limited, and hence on
                   knowledge-intensive tasks, their performance lags behind
                   task-specific architectures. Additionally, providing
                   provenance for their decisions and updating their world
                   knowledge remain open research problems. Pre-trained models
                   with a differentiable access mechanism to explicit
                   non-parametric memory can overcome this issue, but have so
                   far been only investigated for extractive downstream tasks.
                   We explore a general-purpose fine-tuning recipe for
                   retrieval-augmented generation (RAG) -- models which combine
                   pre-trained parametric and non-parametric memory for
                   language generation. We introduce RAG models where the
                   parametric memory is a pre-trained seq2seq model and the
                   non-parametric memory is a dense vector index of Wikipedia,
                   accessed with a pre-trained neural retriever. We compare two
                   RAG formulations, one which conditions on the same retrieved
                   passages across the whole generated sequence, the other can
                   use different passages per token. We fine-tune and evaluate
                   our models on a wide range of knowledge-intensive NLP tasks
                   and set the state-of-the-art on three open domain QA tasks,
                   outperforming parametric seq2seq models and task-specific
                   retrieve-and-extract architectures. For language generation
                   tasks, we find that RAG models generate more specific,
                   diverse and factual language than a state-of-the-art
                   parametric-only seq2seq baseline.",
  month         =  may,
  year          =  2020,
  keywords      = "RAG",
  archivePrefix = "arXiv",
  eprint        = "2005.11401",
  primaryClass  = "cs.CL",
  arxivid       = "2005.11401"
}

@ARTICLE{Nasr2023-gy,
  title         = "Scalable Extraction of Training Data from (Production)
                   Language Models",
  author        = "Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and
                   Jagielski, Matthew and Feder Cooper, A and Ippolito, Daphne
                   and Choquette-Choo, Christopher A and Wallace, Eric and
                   Tram{\`e}r, Florian and Lee, Katherine",
  abstract      = "This paper studies extractable memorization: training data
                   that an adversary can efficiently extract by querying a
                   machine learning model without prior knowledge of the
                   training dataset. We show an adversary can extract gigabytes
                   of training data from open-source language models like
                   Pythia or GPT-Neo, semi-open models like LLaMA or Falcon,
                   and closed models like ChatGPT. Existing techniques from the
                   literature suffice to attack unaligned models; in order to
                   attack the aligned ChatGPT, we develop a new divergence
                   attack that causes the model to diverge from its
                   chatbot-style generations and emit training data at a rate
                   150x higher than when behaving properly. Our methods show
                   practical attacks can recover far more data than previously
                   thought, and reveal that current alignment techniques do not
                   eliminate memorization.",
  month         =  nov,
  year          =  2023,
  keywords      = "Data Extraction",
  archivePrefix = "arXiv",
  eprint        = "2311.17035",
  primaryClass  = "cs.LG",
  arxivid       = "2311.17035"
}

@ARTICLE{Ignat2023-et,
  title         = "A {PhD} student's perspective on research in {NLP} in the
                   era of very large language models",
  author        = "Ignat, Oana and Jin, Zhijing and Abzaliev, Artem and
                   Biester, Laura and Castro, Santiago and Deng, Naihao and
                   Gao, Xinyi and Gunal, Aylin and He, Jacky and Kazemi, Ashkan
                   and Khalifa, Muhammad and Koh, Namho and Lee, Andrew and
                   Liu, Siyang and Min, Do June and Mori, Shinka and Nwatu,
                   Joan and Perez-Rosas, Veronica and Shen, Siqi and Wang,
                   Zekun and Wu, Winston and Mihalcea, Rada",
  abstract      = "Recent progress in large language models has enabled the
                   deployment of many generative NLP applications. At the same
                   time, it has also led to a misleading public discourse that
                   ``it's all been solved.'' Not surprisingly, this has in turn
                   made many NLP researchers -- especially those at the
                   beginning of their career -- wonder about what NLP research
                   area they should focus on. This document is a compilation of
                   NLP research directions that are rich for exploration,
                   reflecting the views of a diverse group of PhD students in
                   an academic research lab. While we identify many research
                   areas, many others exist; we do not cover those areas that
                   are currently addressed by LLMs but where LLMs lag behind in
                   performance, or those focused on LLM development. We welcome
                   suggestions for other research directions to include:
                   https://bit.ly/nlp-era-llm",
  month         =  may,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2305.12544",
  primaryClass  = "cs.CL",
  arxivid       = "2305.12544"
}

@ARTICLE{Jiang2023-ra,
  title         = "Mistral {7B}",
  author        = "Jiang, Albert Q and Sablayrolles, Alexandre and Mensch,
                   Arthur and Bamford, Chris and Chaplot, Devendra Singh and
                   Casas, Diego de las and Bressand, Florian and Lengyel,
                   Gianna and Lample, Guillaume and Saulnier, Lucile and
                   Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock,
                   Pierre and Scao, Teven Le and Lavril, Thibaut and Wang,
                   Thomas and Lacroix, Timoth{\'e}e and Sayed, William El",
  abstract      = "We introduce Mistral 7B v0.1, a 7-billion-parameter language
                   model engineered for superior performance and efficiency.
                   Mistral 7B outperforms Llama 2 13B across all evaluated
                   benchmarks, and Llama 1 34B in reasoning, mathematics, and
                   code generation. Our model leverages grouped-query attention
                   (GQA) for faster inference, coupled with sliding window
                   attention (SWA) to effectively handle sequences of arbitrary
                   length with a reduced inference cost. We also provide a
                   model fine-tuned to follow instructions, Mistral 7B --
                   Instruct, that surpasses the Llama 2 13B -- Chat model both
                   on human and automated benchmarks. Our models are released
                   under the Apache 2.0 license.",
  month         =  oct,
  year          =  2023,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2310.06825",
  primaryClass  = "cs.CL",
  arxivid       = "2310.06825"
}

@ARTICLE{Sun2023-hr,
  title         = "A survey of reasoning with foundation models",
  author        = "Sun, Jiankai and Zheng, Chuanyang and Xie, Enze and Liu,
                   Zhengying and Chu, Ruihang and Qiu, Jianing and Xu, Jiaqi
                   and Ding, Mingyu and Li, Hongyang and Geng, Mengzhe and Wu,
                   Yue and Wang, Wenhai and Chen, Junsong and Yin, Zhangyue and
                   Ren, Xiaozhe and Fu, Jie and He, Junxian and Yuan, Wu and
                   Liu, Qi and Liu, Xihui and Li, Yu and Dong, Hao and Cheng,
                   Yu and Zhang, Ming and Heng, Pheng Ann and Dai, Jifeng and
                   Luo, Ping and Wang, Jingdong and Wen, Ji-Rong and Qiu,
                   Xipeng and Guo, Yike and Xiong, Hui and Liu, Qun and Li,
                   Zhenguo",
  abstract      = "Reasoning, a crucial ability for complex problem-solving,
                   plays a pivotal role in various real-world settings such as
                   negotiation, medical diagnosis, and criminal investigation.
                   It serves as a fundamental methodology in the field of
                   Artificial General Intelligence (AGI). With the ongoing
                   development of foundation models, there is a growing
                   interest in exploring their abilities in reasoning tasks. In
                   this paper, we introduce seminal foundation models proposed
                   or adaptable for reasoning, highlighting the latest
                   advancements in various reasoning tasks, methods, and
                   benchmarks. We then delve into the potential future
                   directions behind the emergence of reasoning abilities
                   within foundation models. We also discuss the relevance of
                   multimodal learning, autonomous agents, and super alignment
                   in the context of reasoning. By discussing these future
                   research directions, we hope to inspire researchers in their
                   exploration of this field, stimulate further advancements in
                   reasoning with foundation models, and contribute to the
                   development of AGI.",
  month         =  dec,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2312.11562",
  primaryClass  = "cs.AI",
  arxivid       = "2312.11562"
}

@ARTICLE{noauthor_undated-ie,
  title         = "Is {Space-Time} Attention All You Need for Video
                   Understanding?",
  author        = "Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo",
  abstract      = "We present a convolution-free approach to video
                   classification built exclusively on self-attention over
                   space and time. Our method, named ``TimeSformer,'' adapts
                   the standard Transformer architecture to video by enabling
                   spatiotemporal feature learning directly from a sequence of
                   frame-level patches. Our experimental study compares
                   different self-attention schemes and suggests that ``divided
                   attention,'' where temporal attention and spatial attention
                   are separately applied within each block, leads to the best
                   video classification accuracy among the design choices
                   considered. Despite the radically different design compared
                   to the prominent paradigm of 3D convolutional architectures
                   for video, TimeSformer achieves state-of-the-art results on
                   several major action recognition benchmarks, including the
                   best reported accuracy on Kinetics-400 and Kinetics-600.
                   Furthermore, our model is faster to train and has higher
                   test-time efficiency compared to competing architectures.
                   Code and pretrained models will be made publicly available.",
  journal       = "arXiv [cs.CV]",
  month         =  feb,
  year          =  2021,
  keywords      = "Video;LLMs",
  archivePrefix = "arXiv",
  eprint        = "2102.05095",
  primaryClass  = "cs.CV",
  arxivid       = "2102.05095"
}

@MISC{Ai_undated-rz,
  title    = "Large Language Models (in 2023)",
  author   = "Ai, Hyung Won Chung",
  keywords = "Models;Fine-tuning;Survey"
}

@ARTICLE{Tay2020-hc,
  title         = "Efficient Transformers: A Survey",
  author        = "Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler,
                   Donald",
  abstract      = "Transformer model architectures have garnered immense
                   interest lately due to their effectiveness across a range of
                   domains like language, vision and reinforcement learning. In
                   the field of natural language processing for example,
                   Transformers have become an indispensable staple in the
                   modern deep learning stack. Recently, a dizzying number of
                   ``X-former'' models have been proposed - Reformer,
                   Linformer, Performer, Longformer, to name a few - which
                   improve upon the original Transformer architecture, many of
                   which make improvements around computational and memory
                   efficiency. With the aim of helping the avid researcher
                   navigate this flurry, this paper characterizes a large and
                   thoughtful selection of recent efficiency-flavored
                   ``X-former'' models, providing an organized and
                   comprehensive overview of existing work and models across
                   multiple domains.",
  month         =  sep,
  year          =  2020,
  keywords      = "Survey;Efficient;LLMs",
  archivePrefix = "arXiv",
  eprint        = "2009.06732",
  primaryClass  = "cs.LG",
  arxivid       = "2009.06732"
}

@ARTICLE{Srinivas2021-ca,
  title         = "Bottleneck Transformers for Visual Recognition",
  author        = "Srinivas, Aravind and Lin, Tsung-Yi and Parmar, Niki and
                   Shlens, Jonathon and Abbeel, Pieter and Vaswani, Ashish",
  abstract      = "We present BoTNet, a conceptually simple yet powerful
                   backbone architecture that incorporates self-attention for
                   multiple computer vision tasks including image
                   classification, object detection and instance segmentation.
                   By just replacing the spatial convolutions with global
                   self-attention in the final three bottleneck blocks of a
                   ResNet and no other changes, our approach improves upon the
                   baselines significantly on instance segmentation and object
                   detection while also reducing the parameters, with minimal
                   overhead in latency. Through the design of BoTNet, we also
                   point out how ResNet bottleneck blocks with self-attention
                   can be viewed as Transformer blocks. Without any bells and
                   whistles, BoTNet achieves 44.4\% Mask AP and 49.7\% Box AP
                   on the COCO Instance Segmentation benchmark using the Mask
                   R-CNN framework; surpassing the previous best published
                   single model and single scale results of ResNeSt evaluated
                   on the COCO validation set. Finally, we present a simple
                   adaptation of the BoTNet design for image classification,
                   resulting in models that achieve a strong performance of
                   84.7\% top-1 accuracy on the ImageNet benchmark while being
                   up to 2.33x faster in compute time than the popular
                   EfficientNet models on TPU-v3 hardware. We hope our simple
                   and effective approach will serve as a strong baseline for
                   future research in self-attention models for vision.",
  month         =  jan,
  year          =  2021,
  keywords      = "Multimodal (Vision, speech, etc);LLMs",
  archivePrefix = "arXiv",
  eprint        = "2101.11605",
  primaryClass  = "cs.CV",
  arxivid       = "2101.11605"
}

@ARTICLE{noauthor_undated-gk,
  title         = "Music Transformer",
  author        = "Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit,
                   Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis
                   and Dai, Andrew M and Hoffman, Matthew D and Dinculescu,
                   Monica and Eck, Douglas",
  abstract      = "Music relies heavily on repetition to build structure and
                   meaning. Self-reference occurs on multiple timescales, from
                   motifs to phrases to reusing of entire sections of music,
                   such as in pieces with ABA structure. The Transformer
                   (Vaswani et al., 2017), a sequence model based on
                   self-attention, has achieved compelling results in many
                   generation tasks that require maintaining long-range
                   coherence. This suggests that self-attention might also be
                   well-suited to modeling music. In musical composition and
                   performance, however, relative timing is critically
                   important. Existing approaches for representing relative
                   positional information in the Transformer modulate attention
                   based on pairwise distance (Shaw et al., 2018). This is
                   impractical for long sequences such as musical compositions
                   since their memory complexity for intermediate relative
                   information is quadratic in the sequence length. We propose
                   an algorithm that reduces their intermediate memory
                   requirement to linear in the sequence length. This enables
                   us to demonstrate that a Transformer with our modified
                   relative attention mechanism can generate minute-long
                   compositions (thousands of steps, four times the length
                   modeled in Oore et al., 2018) with compelling structure,
                   generate continuations that coherently elaborate on a given
                   motif, and in a seq2seq setup generate accompaniments
                   conditioned on melodies. We evaluate the Transformer with
                   our relative attention mechanism on two datasets, JSB
                   Chorales and Piano-e-Competition, and obtain
                   state-of-the-art results on the latter.",
  journal       = "arXiv [cs.LG]",
  month         =  sep,
  year          =  2018,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "1809.04281",
  primaryClass  = "cs.LG",
  arxivid       = "1809.04281"
}

@ARTICLE{Liu2020-ub,
  title         = "Understanding the Difficulty of Training Transformers",
  author        = "Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen,
                   Weizhu and Han, Jiawei",
  abstract      = "Transformers have proved effective in many NLP tasks.
                   However, their training requires non-trivial efforts
                   regarding designing cutting-edge optimizers and learning
                   rate schedulers carefully (e.g., conventional SGD fails to
                   train Transformers effectively). Our objective here is to
                   understand $\textit\{what complicates Transformer
                   training\}$ from both empirical and theoretical
                   perspectives. Our analysis reveals that unbalanced gradients
                   are not the root cause of the instability of training.
                   Instead, we identify an amplification effect that influences
                   training substantially -- for each layer in a multi-layer
                   Transformer model, heavy dependency on its residual branch
                   makes training unstable, since it amplifies small parameter
                   perturbations (e.g., parameter updates) and results in
                   significant disturbances in the model output. Yet we observe
                   that a light dependency limits the model potential and leads
                   to inferior trained models. Inspired by our analysis, we
                   propose Admin ($\textbf\{Ad\}$aptive $\textbf\{m\}$odel
                   $\textbf\{in\}$itialization) to stabilize stabilize the
                   early stage's training and unleash its full potential in the
                   late stage. Extensive experiments show that Admin is more
                   stable, converges faster, and leads to better performance.
                   Implementations are released at:
                   https://github.com/LiyuanLucasLiu/Transforemr-Clinic.",
  month         =  apr,
  year          =  2020,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2004.08249",
  primaryClass  = "cs.LG",
  arxivid       = "2004.08249"
}

@ARTICLE{noauthor_undated-yj,
  title         = "Decoupling the Role of Data, Attention, and Losses in
                   Multimodal Transformers",
  author        = "Hendricks, Lisa Anne and Mellor, John and Schneider, Rosalia
                   and Alayrac, Jean-Baptiste and Nematzadeh, Aida",
  abstract      = "Recently multimodal transformer models have gained
                   popularity because their performance on language and vision
                   tasks suggest they learn rich visual-linguistic
                   representations. Focusing on zero-shot image retrieval
                   tasks, we study three important factors which can impact the
                   quality of learned representations: pretraining data, the
                   attention mechanism, and loss functions. By pretraining
                   models on six datasets, we observe that dataset noise and
                   language similarity to our downstream task are important
                   indicators of model performance. Through architectural
                   analysis, we learn that models with a multimodal attention
                   mechanism can outperform deeper models with modality
                   specific attention mechanisms. Finally, we show that
                   successful contrastive losses used in the self-supervised
                   learning literature do not yield similar performance gains
                   when used in multimodal transformers",
  journal       = "arXiv [cs.CL]",
  month         =  jan,
  year          =  2021,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2102.00529",
  primaryClass  = "cs.CL",
  arxivid       = "2102.00529"
}

@ARTICLE{Zhao2020-ut,
  title         = "Exploring Self-attention for Image Recognition",
  author        = "Zhao, Hengshuang and Jia, Jiaya and Koltun, Vladlen",
  abstract      = "Recent work has shown that self-attention can serve as a
                   basic building block for image recognition models. We
                   explore variations of self-attention and assess their
                   effectiveness for image recognition. We consider two forms
                   of self-attention. One is pairwise self-attention, which
                   generalizes standard dot-product attention and is
                   fundamentally a set operator. The other is patchwise
                   self-attention, which is strictly more powerful than
                   convolution. Our pairwise self-attention networks match or
                   outperform their convolutional counterparts, and the
                   patchwise models substantially outperform the convolutional
                   baselines. We also conduct experiments that probe the
                   robustness of learned representations and conclude that
                   self-attention networks may have significant benefits in
                   terms of robustness and generalization.",
  month         =  apr,
  year          =  2020,
  keywords      = "Multimodal (Vision, speech, etc)",
  archivePrefix = "arXiv",
  eprint        = "2004.13621",
  primaryClass  = "cs.CV",
  arxivid       = "2004.13621"
}

@ARTICLE{Manvi2023-dj,
  title         = "{GeoLLM}: Extracting Geospatial Knowledge from Large
                   Language Models",
  author        = "Manvi, Rohin and Khanna, Samar and Mai, Gengchen and Burke,
                   Marshall and Lobell, David and Ermon, Stefano",
  abstract      = "The application of machine learning (ML) in a range of
                   geospatial tasks is increasingly common but often relies on
                   globally available covariates such as satellite imagery that
                   can either be expensive or lack predictive power. Here we
                   explore the question of whether the vast amounts of
                   knowledge found in Internet language corpora, now compressed
                   within large language models (LLMs), can be leveraged for
                   geospatial prediction tasks. We first demonstrate that LLMs
                   embed remarkable spatial information about locations, but
                   naively querying LLMs using geographic coordinates alone is
                   ineffective in predicting key indicators like population
                   density. We then present GeoLLM, a novel method that can
                   effectively extract geospatial knowledge from LLMs with
                   auxiliary map data from OpenStreetMap. We demonstrate the
                   utility of our approach across multiple tasks of central
                   interest to the international community, including the
                   measurement of population density and economic livelihoods.
                   Across these tasks, our method demonstrates a 70\%
                   improvement in performance (measured using Pearson's $r^2$)
                   relative to baselines that use nearest neighbors or use
                   information directly from the prompt, and performance equal
                   to or exceeding satellite-based benchmarks in the
                   literature. With GeoLLM, we observe that GPT-3.5 outperforms
                   Llama 2 and RoBERTa by 19\% and 51\% respectively,
                   suggesting that the performance of our method scales well
                   with the size of the model and its pretraining dataset. Our
                   experiments reveal that LLMs are remarkably
                   sample-efficient, rich in geospatial information, and robust
                   across the globe. Crucially, GeoLLM shows promise in
                   mitigating the limitations of existing geospatial covariates
                   and complementing them well.",
  month         =  oct,
  year          =  2023,
  keywords      = "Applications",
  archivePrefix = "arXiv",
  eprint        = "2310.06213",
  primaryClass  = "cs.CL",
  arxivid       = "2310.06213",
  doi           = "10.5258/SOTON/WP00647"
}

@ARTICLE{Rawte2023-qp,
  title         = "The troubling emergence of hallucination in Large Language
                   Models -- an extensive definition, quantification, and
                   prescriptive remediations",
  author        = "Rawte, Vipula and Chakraborty, Swagata and Pathak, Agnibh
                   and Sarkar, Anubhav and Tonmoy, S M Towhidul Islam and
                   Chadha, Aman and Sheth, Amit P and Das, Amitava",
  abstract      = "The recent advancements in Large Language Models (LLMs) have
                   garnered widespread acclaim for their remarkable emerging
                   capabilities. However, the issue of hallucination has
                   parallelly emerged as a by-product, posing significant
                   concerns. While some recent endeavors have been made to
                   identify and mitigate different types of hallucination,
                   there has been a limited emphasis on the nuanced
                   categorization of hallucination and associated mitigation
                   methods. To address this gap, we offer a fine-grained
                   discourse on profiling hallucination based on its degree,
                   orientation, and category, along with offering strategies
                   for alleviation. As such, we define two overarching
                   orientations of hallucination: (i) factual mirage (FM) and
                   (ii) silver lining (SL). To provide a more comprehensive
                   understanding, both orientations are further sub-categorized
                   into intrinsic and extrinsic, with three degrees of severity
                   - (i) mild, (ii) moderate, and (iii) alarming. We also
                   meticulously categorize hallucination into six types: (i)
                   acronym ambiguity, (ii) numeric nuisance, (iii) generated
                   golem, (iv) virtual voice, (v) geographic erratum, and (vi)
                   time wrap. Furthermore, we curate HallucInation eLiciTation
                   (HILT), a publicly available dataset comprising of 75,000
                   samples generated using 15 contemporary LLMs along with
                   human annotations for the aforementioned categories.
                   Finally, to establish a method for quantifying and to offer
                   a comparative spectrum that allows us to evaluate and rank
                   LLMs based on their vulnerability to producing
                   hallucinations, we propose Hallucination Vulnerability Index
                   (HVI). We firmly believe that HVI holds significant value as
                   a tool for the wider NLP community, with the potential to
                   serve as a rubric in AI-related policy-making. In
                   conclusion, we propose two solution strategies for
                   mitigating hallucinations.",
  month         =  oct,
  year          =  2023,
  keywords      = "Hallucination",
  archivePrefix = "arXiv",
  eprint        = "2310.04988",
  primaryClass  = "cs.AI",
  arxivid       = "2310.04988"
}

@ARTICLE{Devvrit2023-se,
  title         = "{MatFormer}: Nested Transformer for elastic inference",
  author        = "{Devvrit} and Kudugunta, Sneha and Kusupati, Aditya and
                   Dettmers, Tim and Chen, Kaifeng and Dhillon, Inderjit and
                   Tsvetkov, Yulia and Hajishirzi, Hannaneh and Kakade, Sham
                   and Farhadi, Ali and Jain, Prateek",
  abstract      = "Transformer models are deployed in a wide range of settings,
                   from multi-accelerator clusters to standalone mobile phones.
                   The diverse inference constraints in these scenarios
                   necessitate practitioners to train foundation models such as
                   PaLM 2, Llama, \& ViTs as a series of models of varying
                   sizes. Due to significant training costs, only a select few
                   model sizes are trained and supported, limiting more
                   fine-grained control over relevant tradeoffs, including
                   latency, cost, and accuracy. This work introduces MatFormer,
                   a nested Transformer architecture designed to offer
                   elasticity in a variety of deployment constraints. Each Feed
                   Forward Network (FFN) block of a MatFormer model is jointly
                   optimized with a few nested smaller FFN blocks. This
                   training procedure allows for the Mix'n'Match of model
                   granularities across layers -- i.e., a trained universal
                   MatFormer model enables extraction of hundreds of accurate
                   smaller models, which were never explicitly optimized. We
                   empirically demonstrate MatFormer's effectiveness across
                   different model classes (decoders \& encoders), modalities
                   (language \& vision), and scales (up to 2.6B parameters). We
                   find that a 2.6B decoder-only MatFormer language model
                   (MatLM) allows us to extract smaller models spanning from
                   1.5B to 2.6B, each exhibiting comparable validation loss and
                   one-shot downstream evaluations to their independently
                   trained counterparts. Furthermore, we observe that smaller
                   encoders extracted from a universal MatFormer-based ViT
                   (MatViT) encoder preserve the metric-space structure for
                   adaptive large-scale retrieval. Finally, we showcase that
                   speculative decoding with the accurate and consistent
                   submodels extracted from MatFormer can further reduce
                   inference latency.",
  month         =  oct,
  year          =  2023,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2310.07707",
  primaryClass  = "cs.LG",
  arxivid       = "2310.07707"
}

@ARTICLE{Narang2021-mm,
  title         = "Do Transformer Modifications Transfer Across Implementations
                   and Applications?",
  author        = "Narang, Sharan and Chung, Hyung Won and Tay, Yi and Fedus,
                   William and Fevry, Thibault and Matena, Michael and Malkan,
                   Karishma and Fiedel, Noah and Shazeer, Noam and Lan,
                   Zhenzhong and Zhou, Yanqi and Li, Wei and Ding, Nan and
                   Marcus, Jake and Roberts, Adam and Raffel, Colin",
  abstract      = "The research community has proposed copious modifications to
                   the Transformer architecture since it was introduced over
                   three years ago, relatively few of which have seen
                   widespread adoption. In this paper, we comprehensively
                   evaluate many of these modifications in a shared
                   experimental setting that covers most of the common uses of
                   the Transformer in natural language processing.
                   Surprisingly, we find that most modifications do not
                   meaningfully improve performance. Furthermore, most of the
                   Transformer variants we found beneficial were either
                   developed in the same codebase that we used or are
                   relatively minor changes. We conjecture that performance
                   improvements may strongly depend on implementation details
                   and correspondingly make some recommendations for improving
                   the generality of experimental results.",
  month         =  feb,
  year          =  2021,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2102.11972",
  primaryClass  = "cs.LG",
  arxivid       = "2102.11972"
}

@ARTICLE{Asai2023-aj,
  title         = "{Self-RAG}: Learning to retrieve, generate, and critique
                   through self-reflection",
  author        = "Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup
                   and Hajishirzi, Hannaneh",
  abstract      = "Despite their remarkable capabilities, large language models
                   (LLMs) often produce responses containing factual
                   inaccuracies due to their sole reliance on the parametric
                   knowledge they encapsulate. Retrieval-Augmented Generation
                   (RAG), an ad hoc approach that augments LMs with retrieval
                   of relevant knowledge, decreases such issues. However,
                   indiscriminately retrieving and incorporating a fixed number
                   of retrieved passages, regardless of whether retrieval is
                   necessary, or passages are relevant, diminishes LM
                   versatility or can lead to unhelpful response generation. We
                   introduce a new framework called Self-Reflective
                   Retrieval-Augmented Generation (Self-RAG) that enhances an
                   LM's quality and factuality through retrieval and
                   self-reflection. Our framework trains a single arbitrary LM
                   that adaptively retrieves passages on-demand, and generates
                   and reflects on retrieved passages and its own generations
                   using special tokens, called reflection tokens. Generating
                   reflection tokens makes the LM controllable during the
                   inference phase, enabling it to tailor its behavior to
                   diverse task requirements. Experiments show that Self-RAG
                   (7B and 13B parameters) significantly outperforms
                   state-of-the-art LLMs and retrieval-augmented models on a
                   diverse set of tasks. Specifically, Self-RAG outperforms
                   ChatGPT and retrieval-augmented Llama2-chat on Open-domain
                   QA, reasoning and fact verification tasks, and it shows
                   significant gains in improving factuality and citation
                   accuracy for long-form generations relative to these models.",
  month         =  oct,
  year          =  2023,
  keywords      = "RAG",
  archivePrefix = "arXiv",
  eprint        = "2310.11511",
  primaryClass  = "cs.CL",
  arxivid       = "2310.11511"
}

@ARTICLE{Packer2023-nw,
  title         = "{MemGPT}: Towards {LLMs} as Operating Systems",
  author        = "Packer, Charles and Fang, Vivian and Patil, Shishir G and
                   Lin, Kevin and Wooders, Sarah and Gonzalez, Joseph E",
  abstract      = "Large language models (LLMs) have revolutionized AI, but are
                   constrained by limited context windows, hindering their
                   utility in tasks like extended conversations and document
                   analysis. To enable using context beyond limited context
                   windows, we propose virtual context management, a technique
                   drawing inspiration from hierarchical memory systems in
                   traditional operating systems that provide the appearance of
                   large memory resources through data movement between fast
                   and slow memory. Using this technique, we introduce MemGPT
                   (Memory-GPT), a system that intelligently manages different
                   memory tiers in order to effectively provide extended
                   context within the LLM's limited context window, and
                   utilizes interrupts to manage control flow between itself
                   and the user. We evaluate our OS-inspired design in two
                   domains where the limited context windows of modern LLMs
                   severely handicaps their performance: document analysis,
                   where MemGPT is able to analyze large documents that far
                   exceed the underlying LLM's context window, and
                   multi-session chat, where MemGPT can create conversational
                   agents that remember, reflect, and evolve dynamically
                   through long-term interactions with their users. We release
                   MemGPT code and data for our experiments at
                   https://memgpt.ai.",
  month         =  oct,
  year          =  2023,
  keywords      = "Models;RAG",
  archivePrefix = "arXiv",
  eprint        = "2310.08560",
  primaryClass  = "cs.AI",
  arxivid       = "2310.08560"
}

@ARTICLE{Wei2023-fn,
  title         = "Jailbroken: How Does {LLM} Safety Training Fail?",
  author        = "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
  abstract      = "Large language models trained for safety and harmlessness
                   remain susceptible to adversarial misuse, as evidenced by
                   the prevalence of ``jailbreak'' attacks on early releases of
                   ChatGPT that elicit undesired behavior. Going beyond
                   recognition of the issue, we investigate why such attacks
                   succeed and how they can be created. We hypothesize two
                   failure modes of safety training: competing objectives and
                   mismatched generalization. Competing objectives arise when a
                   model's capabilities and safety goals conflict, while
                   mismatched generalization occurs when safety training fails
                   to generalize to a domain for which capabilities exist. We
                   use these failure modes to guide jailbreak design and then
                   evaluate state-of-the-art models, including OpenAI's GPT-4
                   and Anthropic's Claude v1.3, against both existing and newly
                   designed attacks. We find that vulnerabilities persist
                   despite the extensive red-teaming and safety-training
                   efforts behind these models. Notably, new attacks utilizing
                   our failure modes succeed on every prompt in a collection of
                   unsafe requests from the models' red-teaming evaluation sets
                   and outperform existing ad hoc jailbreaks. Our analysis
                   emphasizes the need for safety-capability parity -- that
                   safety mechanisms should be as sophisticated as the
                   underlying model -- and argues against the idea that scaling
                   alone can resolve these safety failure modes.",
  month         =  jul,
  year          =  2023,
  keywords      = "Jailbreak",
  archivePrefix = "arXiv",
  eprint        = "2307.02483",
  primaryClass  = "cs.LG",
  arxivid       = "2307.02483"
}

@ARTICLE{Qi2023-dn,
  title         = "Visual Adversarial Examples Jailbreak Aligned Large Language
                   Models",
  author        = "Qi, Xiangyu and Huang, Kaixuan and Panda, Ashwinee and
                   Henderson, Peter and Wang, Mengdi and Mittal, Prateek",
  abstract      = "Recently, there has been a surge of interest in integrating
                   vision into Large Language Models (LLMs), exemplified by
                   Visual Language Models (VLMs) such as Flamingo and GPT-4.
                   This paper sheds light on the security and safety
                   implications of this trend. First, we underscore that the
                   continuous and high-dimensional nature of the visual input
                   makes it a weak link against adversarial attacks,
                   representing an expanded attack surface of vision-integrated
                   LLMs. Second, we highlight that the versatility of LLMs also
                   presents visual attackers with a wider array of achievable
                   adversarial objectives, extending the implications of
                   security failures beyond mere misclassification. As an
                   illustration, we present a case study in which we exploit
                   visual adversarial examples to circumvent the safety
                   guardrail of aligned LLMs with integrated vision.
                   Intriguingly, we discover that a single visual adversarial
                   example can universally jailbreak an aligned LLM, compelling
                   it to heed a wide range of harmful instructions that it
                   otherwise would not) and generate harmful content that
                   transcends the narrow scope of a `few-shot' derogatory
                   corpus initially employed to optimize the adversarial
                   example. Our study underscores the escalating adversarial
                   risks associated with the pursuit of multimodality. Our
                   findings also connect the long-studied adversarial
                   vulnerabilities of neural networks to the nascent field of
                   AI alignment. The presented attack suggests a fundamental
                   adversarial challenge for AI alignment, especially in light
                   of the emerging trend toward multimodality in frontier
                   foundation models.",
  month         =  jun,
  year          =  2023,
  keywords      = "Jailbreak",
  archivePrefix = "arXiv",
  eprint        = "2306.13213",
  primaryClass  = "cs.CR",
  arxivid       = "2306.13213"
}

@ARTICLE{Carlini2020-pw,
  title         = "Extracting Training Data from Large Language Models",
  author        = "Carlini, Nicholas and Tramer, Florian and Wallace, Eric and
                   Jagielski, Matthew and Herbert-Voss, Ariel and Lee,
                   Katherine and Roberts, Adam and Brown, Tom and Song, Dawn
                   and Erlingsson, Ulfar and Oprea, Alina and Raffel, Colin",
  abstract      = "It has become common to publish large (billion parameter)
                   language models that have been trained on private datasets.
                   This paper demonstrates that in such settings, an adversary
                   can perform a training data extraction attack to recover
                   individual training examples by querying the language model.
                   We demonstrate our attack on GPT-2, a language model trained
                   on scrapes of the public Internet, and are able to extract
                   hundreds of verbatim text sequences from the model's
                   training data. These extracted examples include (public)
                   personally identifiable information (names, phone numbers,
                   and email addresses), IRC conversations, code, and 128-bit
                   UUIDs. Our attack is possible even though each of the above
                   sequences are included in just one document in the training
                   data. We comprehensively evaluate our extraction attack to
                   understand the factors that contribute to its success.
                   Worryingly, we find that larger models are more vulnerable
                   than smaller models. We conclude by drawing lessons and
                   discussing possible safeguards for training large language
                   models.",
  month         =  dec,
  year          =  2020,
  keywords      = "Data Extraction",
  archivePrefix = "arXiv",
  eprint        = "2012.07805",
  primaryClass  = "cs.CR",
  arxivid       = "2012.07805"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Bender2021-up,
  title     = "On the Dangers of Stochastic Parrots: Can Language Models Be Too
               Big? 🦜",
  booktitle = "Proceedings of the 2021 {ACM} Conference on Fairness,
               Accountability, and Transparency",
  author    = "Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina
               and Shmitchell, Shmargaret",
  abstract  = "The past 3 years of work in NLP have been characterized by the
               development and deployment of ever larger language models,
               especially for English. BERT, its variants, GPT-2/3, and others,
               most recently Switch-C, have pushed the boundaries of the
               possible both through architectural innovations and through
               sheer size. Using these pretrained models and the methodology of
               fine-tuning them for specific tasks, researchers have extended
               the state of the art on a wide array of tasks as measured by
               leaderboards on specific benchmarks for English. In this paper,
               we take a step back and ask: How big is too big? What are the
               possible risks associated with this technology and what paths
               are available for mitigating those risks? We provide
               recommendations including weighing the environmental and
               financial costs first, investing resources into curating and
               carefully documenting datasets rather than ingesting everything
               on the web, carrying out pre-development exercises evaluating
               how the planned approach fits into research and development
               goals and supports stakeholder values, and encouraging research
               directions beyond ever larger language models.",
  publisher = "Association for Computing Machinery",
  pages     = "610--623",
  series    = "FAccT '21",
  month     =  mar,
  year      =  2021,
  address   = "New York, NY, USA",
  keywords  = "Fairness, Bias, Toxicity",
  location  = "Virtual Event, Canada",
  isbn      = "9781450383097",
  doi       = "10.1145/3442188.3445922"
}

@ARTICLE{Gehman2020-na,
  title         = "{RealToxicityPrompts}: Evaluating Neural Toxic Degeneration
                   in Language Models",
  author        = "Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and
                   Choi, Yejin and Smith, Noah A",
  abstract      = "Pretrained neural language models (LMs) are prone to
                   generating racist, sexist, or otherwise toxic language which
                   hinders their safe deployment. We investigate the extent to
                   which pretrained LMs can be prompted to generate toxic
                   language, and the effectiveness of controllable text
                   generation algorithms at preventing such toxic degeneration.
                   We create and release RealToxicityPrompts, a dataset of 100K
                   naturally occurring, sentence-level prompts derived from a
                   large corpus of English web text, paired with toxicity
                   scores from a widely-used toxicity classifier. Using
                   RealToxicityPrompts, we find that pretrained LMs can
                   degenerate into toxic text even from seemingly innocuous
                   prompts. We empirically assess several controllable
                   generation methods, and find that while data- or
                   compute-intensive methods (e.g., adaptive pretraining on
                   non-toxic data) are more effective at steering away from
                   toxicity than simpler solutions (e.g., banning ``bad''
                   words), no current method is failsafe against neural toxic
                   degeneration. To pinpoint the potential cause of such
                   persistent toxic degeneration, we analyze two web text
                   corpora used to pretrain several LMs (including GPT-2;
                   Radford et. al, 2019), and find a significant amount of
                   offensive, factually unreliable, and otherwise toxic
                   content. Our work provides a test bed for evaluating toxic
                   generations by LMs and stresses the need for better data
                   selection processes for pretraining.",
  month         =  sep,
  year          =  2020,
  keywords      = "Fairness, Bias, Toxicity",
  archivePrefix = "arXiv",
  eprint        = "2009.11462",
  primaryClass  = "cs.CL",
  arxivid       = "2009.11462"
}

@ARTICLE{Gao2020-qx,
  title         = "The Pile: An {800GB} Dataset of Diverse Text for Language
                   Modeling",
  author        = "Gao, Leo and Biderman, Stella and Black, Sid and Golding,
                   Laurence and Hoppe, Travis and Foster, Charles and Phang,
                   Jason and He, Horace and Thite, Anish and Nabeshima, Noa and
                   Presser, Shawn and Leahy, Connor",
  abstract      = "Recent work has demonstrated that increased training dataset
                   diversity improves general cross-domain knowledge and
                   downstream generalization capability for large-scale
                   language models. With this in mind, we present
                   \textbackslashtextit\{the Pile\}: an 825 GiB English text
                   corpus targeted at training large-scale language models. The
                   Pile is constructed from 22 diverse high-quality subsets --
                   both existing and newly constructed -- many of which derive
                   from academic or professional sources. Our evaluation of the
                   untuned performance of GPT-2 and GPT-3 on the Pile shows
                   that these models struggle on many of its components, such
                   as academic writing. Conversely, models trained on the Pile
                   improve significantly over both Raw CC and CC-100 on all
                   components of the Pile, while improving performance on
                   downstream evaluations. Through an in-depth exploratory
                   analysis, we document potentially concerning aspects of the
                   data for prospective users. We make publicly available the
                   code used in its construction.",
  month         =  dec,
  year          =  2020,
  keywords      = "Datasets",
  archivePrefix = "arXiv",
  eprint        = "2101.00027",
  primaryClass  = "cs.CL",
  arxivid       = "2101.00027"
}

@ARTICLE{Dai2019-no,
  title         = "{Transformer-XL}: Attentive Language Models Beyond a
                   {Fixed-Length} Context",
  author        = "Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell,
                   Jaime and Le, Quoc V and Salakhutdinov, Ruslan",
  abstract      = "Transformers have a potential of learning longer-term
                   dependency, but are limited by a fixed-length context in the
                   setting of language modeling. We propose a novel neural
                   architecture Transformer-XL that enables learning dependency
                   beyond a fixed length without disrupting temporal coherence.
                   It consists of a segment-level recurrence mechanism and a
                   novel positional encoding scheme. Our method not only
                   enables capturing longer-term dependency, but also resolves
                   the context fragmentation problem. As a result,
                   Transformer-XL learns dependency that is 80\% longer than
                   RNNs and 450\% longer than vanilla Transformers, achieves
                   better performance on both short and long sequences, and is
                   up to 1,800+ times faster than vanilla Transformers during
                   evaluation. Notably, we improve the state-of-the-art results
                   of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on
                   WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn
                   Treebank (without finetuning). When trained only on
                   WikiText-103, Transformer-XL manages to generate reasonably
                   coherent, novel text articles with thousands of tokens. Our
                   code, pretrained models, and hyperparameters are available
                   in both Tensorflow and PyTorch.",
  month         =  jan,
  year          =  2019,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "1901.02860",
  primaryClass  = "cs.LG",
  arxivid       = "1901.02860"
}

@ARTICLE{Lester2021-dp,
  title         = "The Power of Scale for {Parameter-Efficient} Prompt Tuning",
  author        = "Lester, Brian and Al-Rfou, Rami and Constant, Noah",
  abstract      = "In this work, we explore ``prompt tuning'', a simple yet
                   effective mechanism for learning ``soft prompts'' to
                   condition frozen language models to perform specific
                   downstream tasks. Unlike the discrete text prompts used by
                   GPT-3, soft prompts are learned through backpropagation and
                   can be tuned to incorporate signal from any number of
                   labeled examples. Our end-to-end learned approach
                   outperforms GPT-3's ``few-shot'' learning by a large margin.
                   More remarkably, through ablations on model size using T5,
                   we show that prompt tuning becomes more competitive with
                   scale: as models exceed billions of parameters, our method
                   ``closes the gap'' and matches the strong performance of
                   model tuning (where all model weights are tuned). This
                   finding is especially relevant in that large models are
                   costly to share and serve, and the ability to reuse one
                   frozen model for multiple downstream tasks can ease this
                   burden. Our method can be seen as a simplification of the
                   recently proposed ``prefix tuning'' of Li and Liang (2021),
                   and we provide a comparison to this and other similar
                   approaches. Finally, we show that conditioning a frozen
                   model with soft prompts confers benefits in robustness to
                   domain transfer, as compared to full model tuning.",
  month         =  apr,
  year          =  2021,
  keywords      = "In-Context;Efficient;Scaling",
  archivePrefix = "arXiv",
  eprint        = "2104.08691",
  primaryClass  = "cs.CL",
  arxivid       = "2104.08691"
}

@ARTICLE{Patterson2021-ta,
  title         = "Carbon Emissions and Large Neural Network Training",
  author        = "Patterson, David and Gonzalez, Joseph and Le, Quoc and
                   Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel
                   and So, David and Texier, Maud and Dean, Jeff",
  abstract      = "The computation demand for machine learning (ML) has grown
                   rapidly recently, which comes with a number of costs.
                   Estimating the energy cost helps measure its environmental
                   impact and finding greener strategies, yet it is challenging
                   without detailed information. We calculate the energy use
                   and carbon footprint of several recent large models-T5,
                   Meena, GShard, Switch Transformer, and GPT-3-and refine
                   earlier estimates for the neural architecture search that
                   found Evolved Transformer. We highlight the following
                   opportunities to improve energy efficiency and CO2
                   equivalent emissions (CO2e): Large but sparsely activated
                   DNNs can consume <1/10th the energy of large, dense DNNs
                   without sacrificing accuracy despite using as many or even
                   more parameters. Geographic location matters for ML workload
                   scheduling since the fraction of carbon-free energy and
                   resulting CO2e vary ~5X-10X, even within the same country
                   and the same organization. We are now optimizing where and
                   when large models are trained. Specific datacenter
                   infrastructure matters, as Cloud datacenters can be ~1.4-2X
                   more energy efficient than typical datacenters, and the
                   ML-oriented accelerators inside them can be ~2-5X more
                   effective than off-the-shelf systems. Remarkably, the choice
                   of DNN, datacenter, and processor can reduce the carbon
                   footprint up to ~100-1000X. These large factors also make
                   retroactive estimates of energy cost difficult. To avoid
                   miscalculations, we believe ML papers requiring large
                   computational resources should make energy consumption and
                   CO2e explicit when practical. We are working to be more
                   transparent about energy use and CO2e in our future
                   research. To help reduce the carbon footprint of ML, we
                   believe energy usage and CO2e should be a key metric in
                   evaluating models, and we are collaborating with MLPerf
                   developers to include energy usage during training and
                   inference in this industry standard benchmark.",
  month         =  apr,
  year          =  2021,
  keywords      = "Safety/Risks",
  archivePrefix = "arXiv",
  eprint        = "2104.10350",
  primaryClass  = "cs.LG",
  arxivid       = "2104.10350"
}

@ARTICLE{Gu2023-ej,
  title         = "Mamba: {Linear-Time} Sequence Modeling with Selective State
                   Spaces",
  author        = "Gu, Albert and Dao, Tri",
  abstract      = "Foundation models, now powering most of the exciting
                   applications in deep learning, are almost universally based
                   on the Transformer architecture and its core attention
                   module. Many subquadratic-time architectures such as linear
                   attention, gated convolution and recurrent models, and
                   structured state space models (SSMs) have been developed to
                   address Transformers' computational inefficiency on long
                   sequences, but they have not performed as well as attention
                   on important modalities such as language. We identify that a
                   key weakness of such models is their inability to perform
                   content-based reasoning, and make several improvements.
                   First, simply letting the SSM parameters be functions of the
                   input addresses their weakness with discrete modalities,
                   allowing the model to selectively propagate or forget
                   information along the sequence length dimension depending on
                   the current token. Second, even though this change prevents
                   the use of efficient convolutions, we design a
                   hardware-aware parallel algorithm in recurrent mode. We
                   integrate these selective SSMs into a simplified end-to-end
                   neural network architecture without attention or even MLP
                   blocks (Mamba). Mamba enjoys fast inference (5$\times$
                   higher throughput than Transformers) and linear scaling in
                   sequence length, and its performance improves on real data
                   up to million-length sequences. As a general sequence model
                   backbone, Mamba achieves state-of-the-art performance across
                   several modalities such as language, audio, and genomics. On
                   language modeling, our Mamba-3B model outperforms
                   Transformers of the same size and matches Transformers twice
                   its size, both in pretraining and downstream evaluation.",
  month         =  dec,
  year          =  2023,
  keywords      = "Transformer-alt",
  archivePrefix = "arXiv",
  eprint        = "2312.00752",
  primaryClass  = "cs.LG",
  arxivid       = "2312.00752"
}

@ARTICLE{Eldan2023-wa,
  title         = "{TinyStories}: How Small Can Language Models Be and Still
                   Speak Coherent English?",
  author        = "Eldan, Ronen and Li, Yuanzhi",
  abstract      = "Language models (LMs) are powerful tools for natural
                   language processing, but they often struggle to produce
                   coherent and fluent text when they are small. Models with
                   around 125M parameters such as GPT-Neo (small) or GPT-2
                   (small) can rarely generate coherent and consistent English
                   text beyond a few words even after extensive training. This
                   raises the question of whether the emergence of the ability
                   to produce coherent English text only occurs at larger
                   scales (with hundreds of millions of parameters or more) and
                   complex architectures (with many layers of global
                   attention). In this work, we introduce TinyStories, a
                   synthetic dataset of short stories that only contain words
                   that a typical 3 to 4-year-olds usually understand,
                   generated by GPT-3.5 and GPT-4. We show that TinyStories can
                   be used to train and evaluate LMs that are much smaller than
                   the state-of-the-art models (below 10 million total
                   parameters), or have much simpler architectures (with only
                   one transformer block), yet still produce fluent and
                   consistent stories with several paragraphs that are diverse
                   and have almost perfect grammar, and demonstrate reasoning
                   capabilities. We also introduce a new paradigm for the
                   evaluation of language models: We suggest a framework which
                   uses GPT-4 to grade the content generated by these models as
                   if those were stories written by students and graded by a
                   (human) teacher. This new paradigm overcomes the flaws of
                   standard benchmarks which often requires the model's output
                   to be very structures, and moreover provides a
                   multidimensional score for the model, providing scores for
                   different capabilities such as grammar, creativity and
                   consistency. We hope that TinyStories can facilitate the
                   development, analysis and research of LMs, especially for
                   low-resource or specialized domains, and shed light on the
                   emergence of language capabilities in LMs.",
  month         =  may,
  year          =  2023,
  keywords      = "Small Models",
  archivePrefix = "arXiv",
  eprint        = "2305.07759",
  primaryClass  = "cs.CL",
  arxivid       = "2305.07759"
}

@ARTICLE{Gale2022-ki,
  title         = "{MegaBlocks}: Efficient Sparse Training with
                   {Mixture-of-Experts}",
  author        = "Gale, Trevor and Narayanan, Deepak and Young, Cliff and
                   Zaharia, Matei",
  abstract      = "We present MegaBlocks, a system for efficient
                   Mixture-of-Experts (MoE) training on GPUs. Our system is
                   motivated by the limitations of current frameworks, which
                   restrict the dynamic routing in MoE layers to satisfy the
                   constraints of existing software and hardware. These
                   formulations force a tradeoff between model quality and
                   hardware efficiency, as users must choose between dropping
                   tokens from the computation or wasting computation and
                   memory on padding. To address these limitations, we
                   reformulate MoE computation in terms of block-sparse
                   operations and develop new block-sparse GPU kernels that
                   efficiently handle the dynamism present in MoEs. Our
                   approach never drops tokens and maps efficiently to modern
                   hardware, enabling end-to-end training speedups of up to
                   40\% over MoEs trained with the state-of-the-art Tutel
                   library and 2.4x over DNNs trained with the highly-optimized
                   Megatron-LM framework.",
  month         =  nov,
  year          =  2022,
  keywords      = "Efficient",
  archivePrefix = "arXiv",
  eprint        = "2211.15841",
  primaryClass  = "cs.LG",
  arxivid       = "2211.15841"
}

@ARTICLE{Zhang2023-kh,
  title         = "{BiomedGPT}: A Unified and Generalist Biomedical Generative
                   Pre-trained Transformer for Vision, Language, and Multimodal
                   Tasks",
  author        = "Zhang, Kai and Yu, Jun and Yan, Zhiling and Liu, Yixin and
                   Adhikarla, Eashan and Fu, Sunyang and Chen, Xun and Chen,
                   Chen and Zhou, Yuyin and Li, Xiang and He, Lifang and
                   Davison, Brian D and Li, Quanzheng and Chen, Yong and Liu,
                   Hongfang and Sun, Lichao",
  abstract      = "In this paper, we introduce a unified and generalist
                   Biomedical Generative Pre-trained Transformer (BiomedGPT)
                   model, which leverages self-supervision on large and diverse
                   datasets to accept multi-modal inputs and perform a range of
                   downstream tasks. Our experiments demonstrate that BiomedGPT
                   delivers expansive and inclusive representations of
                   biomedical data, outperforming the majority of preceding
                   state-of-the-art models across five distinct tasks with 20
                   public datasets spanning over 15 unique biomedical
                   modalities. Through the ablation study, we also showcase the
                   efficacy of our multi-modal and multi-task pretraining
                   approach in transferring knowledge to previously unseen
                   data. Overall, our work presents a significant step forward
                   in developing unified and generalist models for biomedicine,
                   with far-reaching implications for improving healthcare
                   outcomes.",
  month         =  may,
  year          =  2023,
  keywords      = "Health",
  archivePrefix = "arXiv",
  eprint        = "2305.17100",
  primaryClass  = "cs.CL",
  arxivid       = "2305.17100"
}

@ARTICLE{Friel2023-tk,
  title         = "Chainpoll: A high efficacy method for {LLM} hallucination
                   detection",
  author        = "Friel, Robert and Sanyal, Atindriyo",
  abstract      = "Large language models (LLMs) have experienced notable
                   advancements in generating coherent and contextually
                   relevant responses. However, hallucinations - incorrect or
                   unfounded claims - are still prevalent, prompting the
                   creation of automated metrics to detect these in LLM
                   outputs. Our contributions include: introducing ChainPoll,
                   an innovative hallucination detection method that excels
                   compared to its counterparts, and unveiling RealHall, a
                   refined collection of benchmark datasets to assess
                   hallucination detection metrics from recent studies. While
                   creating RealHall, we assessed tasks and datasets from
                   previous hallucination detection studies and observed that
                   many are not suitable for the potent LLMs currently in use.
                   Overcoming this, we opted for four datasets challenging for
                   modern LLMs and pertinent to real-world scenarios. Using
                   RealHall, we conducted a comprehensive comparison of
                   ChainPoll with numerous hallucination metrics from recent
                   studies. Our findings indicate that ChainPoll outperforms in
                   all RealHall benchmarks, achieving an overall AUROC of
                   0.781. This surpasses the next best theoretical method by
                   11\% and exceeds industry standards by over 23\%.
                   Additionally, ChainPoll is cost-effective and offers greater
                   transparency than other metrics. We introduce two novel
                   metrics to assess LLM hallucinations: Adherence and
                   Correctness. Adherence is relevant to Retrieval Augmented
                   Generation workflows, evaluating an LLM's analytical
                   capabilities within given documents and contexts. In
                   contrast, Correctness identifies logical and reasoning
                   errors.",
  month         =  oct,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2310.18344",
  primaryClass  = "cs.CL",
  arxivid       = "2310.18344"
}

@ARTICLE{Davenport2019-dy,
  title    = "The potential for artificial intelligence in healthcare",
  author   = "Davenport, Thomas and Kalakota, Ravi",
  abstract = "The complexity and rise of data in healthcare means that
              artificial intelligence (AI) will increasingly be applied within
              the field. Several types of AI are already being employed by
              payers and providers of care, and life sciences companies. The
              key categories of applications involve diagnosis and treatment
              recommendations, patient engagement and adherence, and
              administrative activities. Although there are many instances in
              which AI can perform healthcare tasks as well or better than
              humans, implementation factors will prevent large-scale
              automation of healthcare professional jobs for a considerable
              period. Ethical issues in the application of AI to healthcare are
              also discussed.",
  journal  = "Future Healthc J",
  volume   =  6,
  number   =  2,
  pages    = "94--98",
  month    =  jun,
  year     =  2019,
  keywords = "Artificial intelligence; clinical decision support; electronic
              health record systems;Health",
  language = "en",
  issn     = "2514-6645",
  pmid     = "31363513",
  doi      = "10.7861/futurehosp.6-2-94",
  pmc      = "PMC6616181"
}

@ARTICLE{Tunstall2023-wn,
  title         = "Zephyr: Direct Distillation of {LM} Alignment",
  author        = "Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and
                   Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and
                   Huang, Shengyi and von Werra, Leandro and Fourrier,
                   Cl{\'e}mentine and Habib, Nathan and Sarrazin, Nathan and
                   Sanseviero, Omar and Rush, Alexander M and Wolf, Thomas",
  abstract      = "We aim to produce a smaller language model that is aligned
                   to user intent. Previous research has shown that applying
                   distilled supervised fine-tuning (dSFT) on larger models
                   significantly improves task accuracy; however, these models
                   are unaligned, i.e. they do not respond well to natural
                   prompts. To distill this property, we experiment with the
                   use of preference data from AI Feedback (AIF). Starting from
                   a dataset of outputs ranked by a teacher model, we apply
                   distilled direct preference optimization (dDPO) to learn a
                   chat model with significantly improved intent alignment. The
                   approach requires only a few hours of training without any
                   additional sampling during fine-tuning. The final result,
                   Zephyr-7B, sets the state-of-the-art on chat benchmarks for
                   7B parameter models, and requires no human annotation. In
                   particular, results on MT-Bench show that Zephyr-7B
                   surpasses Llama2-Chat-70B, the best open-access RLHF-based
                   model. Code, models, data, and tutorials for the system are
                   available at
                   https://github.com/huggingface/alignment-handbook.",
  month         =  oct,
  year          =  2023,
  keywords      = "Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "2310.16944",
  primaryClass  = "cs.LG",
  arxivid       = "2310.16944"
}

@ARTICLE{Roberts2022-zr,
  title         = "Scaling Up Models and Data with $\texttt{t5x}$ and
                   $\texttt{seqio}$",
  author        = "Roberts, Adam and Chung, Hyung Won and Levskaya, Anselm and
                   Mishra, Gaurav and Bradbury, James and Andor, Daniel and
                   Narang, Sharan and Lester, Brian and Gaffney, Colin and
                   Mohiuddin, Afroz and Hawthorne, Curtis and Lewkowycz, Aitor
                   and Salcianu, Alex and van Zee, Marc and Austin, Jacob and
                   Goodman, Sebastian and Soares, Livio Baldini and Hu, Haitang
                   and Tsvyashchenko, Sasha and Chowdhery, Aakanksha and
                   Bastings, Jasmijn and Bulian, Jannis and Garcia, Xavier and
                   Ni, Jianmo and Chen, Andrew and Kenealy, Kathleen and Clark,
                   Jonathan H and Lee, Stephan and Garrette, Dan and Lee-Thorp,
                   James and Raffel, Colin and Shazeer, Noam and Ritter, Marvin
                   and Bosma, Maarten and Passos, Alexandre and Maitin-Shepard,
                   Jeremy and Fiedel, Noah and Omernick, Mark and Saeta,
                   Brennan and Sepassi, Ryan and Spiridonov, Alexander and
                   Newlan, Joshua and Gesmundo, Andrea",
  abstract      = "Recent neural network-based language models have benefited
                   greatly from scaling up the size of training datasets and
                   the number of parameters in the models themselves. Scaling
                   can be complicated due to various factors including the need
                   to distribute computation on supercomputer clusters (e.g.,
                   TPUs), prevent bottlenecks when infeeding data, and ensure
                   reproducible results. In this work, we present two software
                   libraries that ease these issues: $\texttt\{t5x\}$
                   simplifies the process of building and training large
                   language models at scale while maintaining ease of use, and
                   $\texttt\{seqio\}$ provides a task-based API for simple
                   creation of fast and reproducible training data and
                   evaluation pipelines. These open-source libraries have been
                   used to train models with hundreds of billions of parameters
                   on datasets with multiple terabytes of training data. Along
                   with the libraries, we release configurations and
                   instructions for T5-like encoder-decoder models as well as
                   GPT-like decoder-only architectures. $\texttt\{t5x\}$ and
                   $\texttt\{seqio\}$ are open source and available at
                   https://github.com/google-research/t5x and
                   https://github.com/google/seqio, respectively.",
  month         =  mar,
  year          =  2022,
  keywords      = "Models;Scaling",
  archivePrefix = "arXiv",
  eprint        = "2203.17189",
  primaryClass  = "cs.LG",
  arxivid       = "2203.17189"
}

@ARTICLE{Manning2022-rh,
  title     = "Human language understanding \& reasoning",
  author    = "Manning, Christopher D",
  abstract  = "Abstract The last decade has yielded dramatic and quite
               surprising breakthroughs in natural language processing through
               the use of simple artificial neural network computations,
               replicated on a very large scale and trained over exceedingly
               large amounts of data. The resulting pretrained language models,
               such as BERT and GPT-3, have provided a powerful universal
               language understanding and generation base, which can easily be
               adapted to many understanding, writing, and reasoning tasks.
               These models show the first inklings of a more general form of
               artificial intelligence, which may lead to powerful foundation
               models in domains of sensory experience beyond just language.",
  journal   = "Daedalus",
  publisher = "MIT Press",
  volume    =  151,
  number    =  2,
  pages     = "127--138",
  month     =  may,
  year      =  2022,
  keywords  = "Understanding",
  copyright = "https://creativecommons.org/licenses/by-nc/4.0/",
  language  = "en",
  issn      = "0011-5266, 1548-6192",
  doi       = "10.1162/daed\_a\_01905"
}

@ARTICLE{Devlin2018-xn,
  title         = "{BERT}: Pre-training of Deep Bidirectional Transformers for
                   Language Understanding",
  author        = "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and
                   Toutanova, Kristina",
  abstract      = "We introduce a new language representation model called
                   BERT, which stands for Bidirectional Encoder Representations
                   from Transformers. Unlike recent language representation
                   models, BERT is designed to pre-train deep bidirectional
                   representations from unlabeled text by jointly conditioning
                   on both left and right context in all layers. As a result,
                   the pre-trained BERT model can be fine-tuned with just one
                   additional output layer to create state-of-the-art models
                   for a wide range of tasks, such as question answering and
                   language inference, without substantial task-specific
                   architecture modifications. BERT is conceptually simple and
                   empirically powerful. It obtains new state-of-the-art
                   results on eleven natural language processing tasks,
                   including pushing the GLUE score to 80.5\% (7.7\% point
                   absolute improvement), MultiNLI accuracy to 86.7\% (4.6\%
                   absolute improvement), SQuAD v1.1 question answering Test F1
                   to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test
                   F1 to 83.1 (5.1 point absolute improvement).",
  month         =  oct,
  year          =  2018,
  keywords      = "Models;Encoder only",
  archivePrefix = "arXiv",
  eprint        = "1810.04805",
  primaryClass  = "cs.CL",
  arxivid       = "1810.04805"
}

@ARTICLE{Peters2018-jx,
  title         = "Deep contextualized word representations",
  author        = "Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and
                   Gardner, Matt and Clark, Christopher and Lee, Kenton and
                   Zettlemoyer, Luke",
  abstract      = "We introduce a new type of deep contextualized word
                   representation that models both (1) complex characteristics
                   of word use (e.g., syntax and semantics), and (2) how these
                   uses vary across linguistic contexts (i.e., to model
                   polysemy). Our word vectors are learned functions of the
                   internal states of a deep bidirectional language model
                   (biLM), which is pre-trained on a large text corpus. We show
                   that these representations can be easily added to existing
                   models and significantly improve the state of the art across
                   six challenging NLP problems, including question answering,
                   textual entailment and sentiment analysis. We also present
                   an analysis showing that exposing the deep internals of the
                   pre-trained network is crucial, allowing downstream models
                   to mix different types of semi-supervision signals.",
  month         =  feb,
  year          =  2018,
  keywords      = "Models;Encoder only",
  archivePrefix = "arXiv",
  eprint        = "1802.05365",
  primaryClass  = "cs.CL",
  arxivid       = "1802.05365"
}

@ARTICLE{Liu2019-iv,
  title         = "{RoBERTa}: A robustly optimized {BERT} pretraining approach",
  author        = "Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei
                   and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis,
                   Mike and Zettlemoyer, Luke and Stoyanov, Veselin",
  abstract      = "Language model pretraining has led to significant
                   performance gains but careful comparison between different
                   approaches is challenging. Training is computationally
                   expensive, often done on private datasets of different
                   sizes, and, as we will show, hyperparameter choices have
                   significant impact on the final results. We present a
                   replication study of BERT pretraining (Devlin et al., 2019)
                   that carefully measures the impact of many key
                   hyperparameters and training data size. We find that BERT
                   was significantly undertrained, and can match or exceed the
                   performance of every model published after it. Our best
                   model achieves state-of-the-art results on GLUE, RACE and
                   SQuAD. These results highlight the importance of previously
                   overlooked design choices, and raise questions about the
                   source of recently reported improvements. We release our
                   models and code.",
  month         =  jul,
  year          =  2019,
  keywords      = "Models;Encoder only",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "1907.11692",
  primaryClass  = "cs.CL",
  arxivid       = "1907.11692"
}

@MISC{Radford_undated-bn,
  title        = "Improving language understanding by generative pre-training",
  author       = "Radford, Alec and Narasimhan, Karthik and Salimans, Tim and
                  Sutskever, Ilya",
  howpublished = "\url{https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}",
  note         = "Accessed: 2023-10-27",
  keywords     = "Models;Encoder only"
}

@ARTICLE{Clark2020-le,
  title         = "{ELECTRA}: Pre-training Text Encoders as Discriminators
                   Rather Than Generators",
  author        = "Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and
                   Manning, Christopher D",
  abstract      = "Masked language modeling (MLM) pre-training methods such as
                   BERT corrupt the input by replacing some tokens with [MASK]
                   and then train a model to reconstruct the original tokens.
                   While they produce good results when transferred to
                   downstream NLP tasks, they generally require large amounts
                   of compute to be effective. As an alternative, we propose a
                   more sample-efficient pre-training task called replaced
                   token detection. Instead of masking the input, our approach
                   corrupts it by replacing some tokens with plausible
                   alternatives sampled from a small generator network. Then,
                   instead of training a model that predicts the original
                   identities of the corrupted tokens, we train a
                   discriminative model that predicts whether each token in the
                   corrupted input was replaced by a generator sample or not.
                   Thorough experiments demonstrate this new pre-training task
                   is more efficient than MLM because the task is defined over
                   all input tokens rather than just the small subset that was
                   masked out. As a result, the contextual representations
                   learned by our approach substantially outperform the ones
                   learned by BERT given the same model size, data, and
                   compute. The gains are particularly strong for small models;
                   for example, we train a model on one GPU for 4 days that
                   outperforms GPT (trained using 30x more compute) on the GLUE
                   natural language understanding benchmark. Our approach also
                   works well at scale, where it performs comparably to RoBERTa
                   and XLNet while using less than 1/4 of their compute and
                   outperforms them when using the same amount of compute.",
  month         =  mar,
  year          =  2020,
  keywords      = "Models;Encoder only",
  archivePrefix = "arXiv",
  eprint        = "2003.10555",
  primaryClass  = "cs.CL",
  arxivid       = "2003.10555"
}

@ARTICLE{Lewis2019-aa,
  title         = "{BART}: Denoising {Sequence-to-Sequence} Pre-training for
                   Natural Language Generation, Translation, and Comprehension",
  author        = "Lewis, Mike and Liu, Yinhan and Goyal, Naman and
                   Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy,
                   Omer and Stoyanov, Ves and Zettlemoyer, Luke",
  abstract      = "We present BART, a denoising autoencoder for pretraining
                   sequence-to-sequence models. BART is trained by (1)
                   corrupting text with an arbitrary noising function, and (2)
                   learning a model to reconstruct the original text. It uses a
                   standard Tranformer-based neural machine translation
                   architecture which, despite its simplicity, can be seen as
                   generalizing BERT (due to the bidirectional encoder), GPT
                   (with the left-to-right decoder), and many other more recent
                   pretraining schemes. We evaluate a number of noising
                   approaches, finding the best performance by both randomly
                   shuffling the order of the original sentences and using a
                   novel in-filling scheme, where spans of text are replaced
                   with a single mask token. BART is particularly effective
                   when fine tuned for text generation but also works well for
                   comprehension tasks. It matches the performance of RoBERTa
                   with comparable training resources on GLUE and SQuAD,
                   achieves new state-of-the-art results on a range of
                   abstractive dialogue, question answering, and summarization
                   tasks, with gains of up to 6 ROUGE. BART also provides a 1.1
                   BLEU increase over a back-translation system for machine
                   translation, with only target language pretraining. We also
                   report ablation experiments that replicate other pretraining
                   schemes within the BART framework, to better measure which
                   factors most influence end-task performance.",
  month         =  oct,
  year          =  2019,
  keywords      = "Encoder-Decoder",
  archivePrefix = "arXiv",
  eprint        = "1910.13461",
  primaryClass  = "cs.CL",
  arxivid       = "1910.13461"
}

@ARTICLE{Xue2020-cc,
  title         = "mT5: A massively multilingual pre-trained text-to-text
                   transformer",
  author        = "Xue, Linting and Constant, Noah and Roberts, Adam and Kale,
                   Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua,
                   Aditya and Raffel, Colin",
  abstract      = "The recent ``Text-to-Text Transfer Transformer'' (T5)
                   leveraged a unified text-to-text format and scale to attain
                   state-of-the-art results on a wide variety of
                   English-language NLP tasks. In this paper, we introduce mT5,
                   a multilingual variant of T5 that was pre-trained on a new
                   Common Crawl-based dataset covering 101 languages. We detail
                   the design and modified training of mT5 and demonstrate its
                   state-of-the-art performance on many multilingual
                   benchmarks. We also describe a simple technique to prevent
                   ``accidental translation'' in the zero-shot setting, where a
                   generative model chooses to (partially) translate its
                   prediction into the wrong language. All of the code and
                   model checkpoints used in this work are publicly available.",
  month         =  oct,
  year          =  2020,
  keywords      = "Encoder-Decoder",
  archivePrefix = "arXiv",
  eprint        = "2010.11934",
  primaryClass  = "cs.CL",
  arxivid       = "2010.11934"
}

@ARTICLE{Soltan2022-ga,
  title         = "{AlexaTM} 20B: {Few-Shot} Learning Using a {Large-Scale}
                   Multilingual {Seq2Seq} Model",
  author        = "Soltan, Saleh and Ananthakrishnan, Shankar and FitzGerald,
                   Jack and Gupta, Rahul and Hamza, Wael and Khan, Haidar and
                   Peris, Charith and Rawls, Stephen and Rosenbaum, Andy and
                   Rumshisky, Anna and Prakash, Chandana Satya and Sridhar,
                   Mukund and Triefenbach, Fabian and Verma, Apurv and Tur,
                   Gokhan and Natarajan, Prem",
  abstract      = "In this work, we demonstrate that multilingual large-scale
                   sequence-to-sequence (seq2seq) models, pre-trained on a
                   mixture of denoising and Causal Language Modeling (CLM)
                   tasks, are more efficient few-shot learners than
                   decoder-only models on various tasks. In particular, we
                   train a 20 billion parameter multilingual seq2seq model
                   called Alexa Teacher Model (AlexaTM 20B) and show that it
                   achieves state-of-the-art (SOTA) performance on 1-shot
                   summarization tasks, outperforming a much larger 540B PaLM
                   decoder model. AlexaTM 20B also achieves SOTA in 1-shot
                   machine translation, especially for low-resource languages,
                   across almost all language pairs supported by the model
                   (Arabic, English, French, German, Hindi, Italian, Japanese,
                   Marathi, Portuguese, Spanish, Tamil, and Telugu) on
                   Flores-101 dataset. We also show in zero-shot setting,
                   AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2
                   datasets and provides SOTA performance on multilingual tasks
                   such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our
                   results present a compelling case for seq2seq models as a
                   powerful alternative to decoder-only models for Large-scale
                   Language Model (LLM) training.",
  month         =  aug,
  year          =  2022,
  keywords      = "Encoder-Decoder",
  archivePrefix = "arXiv",
  eprint        = "2208.01448",
  primaryClass  = "cs.CL",
  arxivid       = "2208.01448"
}

@MISC{Radford_undated-og,
  title        = "Language Models are Unsupervised Multitask Learners",
  author       = "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan,
                  David and Amodei, Dario and Sutskever, Ilya",
  howpublished = "\url{https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}",
  note         = "Accessed: 2023-10-27",
  keywords     = "Decorder only"
}

@ARTICLE{Zhang2022-aa,
  title         = "{OPT}: Open Pre-trained transformer language models",
  author        = "Zhang, Susan and Roller, Stephen and Goyal, Naman and
                   Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan,
                   Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria
                   and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and
                   Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and
                   Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke",
  abstract      = "Large language models, which are often trained for hundreds
                   of thousands of compute days, have shown remarkable
                   capabilities for zero- and few-shot learning. Given their
                   computational cost, these models are difficult to replicate
                   without significant capital. For the few that are available
                   through APIs, no access is granted to the full model
                   weights, making them difficult to study. We present Open
                   Pre-trained Transformers (OPT), a suite of decoder-only
                   pre-trained transformers ranging from 125M to 175B
                   parameters, which we aim to fully and responsibly share with
                   interested researchers. We show that OPT-175B is comparable
                   to GPT-3, while requiring only 1/7th the carbon footprint to
                   develop. We are also releasing our logbook detailing the
                   infrastructure challenges we faced, along with code for
                   experimenting with all of the released models.",
  month         =  may,
  year          =  2022,
  keywords      = "Decorder only",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2205.01068",
  primaryClass  = "cs.CL",
  arxivid       = "2205.01068"
}

@ARTICLE{Aghajanyan_undated-ay,
  title    = "{CM3}: A {CAUSAL} {MASKED} {MULTIMODAL} {MODEL} {OF} {THE}
              {INTERNET}",
  author   = "Aghajanyan, Armen and Huang, Bernie and Ross, Candace and
              Karpukhin, Vlad and Xu, Hu and Goyal, Naman and Okhonko, Dmytro
              and Joshi, Mandar and Ghosh, Gargi and Lewis, Mike and
              Zettlemoyer, Luke",
  keywords = "Multimodal (Vision, speech, etc)",
  arxivid  = "2201.07520"
}

@ARTICLE{Tsimpoukelli2021-uf,
  title         = "Multimodal {Few-Shot} Learning with Frozen Language Models",
  author        = "Tsimpoukelli, Maria and Menick, Jacob and Cabi, Serkan and
                   Ali Eslami, S M and Vinyals, Oriol and Hill, Felix",
  abstract      = "When trained at sufficient scale, auto-regressive language
                   models exhibit the notable ability to learn a new language
                   task after being prompted with just a few examples. Here, we
                   present a simple, yet effective, approach for transferring
                   this few-shot learning ability to a multimodal setting
                   (vision and language). Using aligned image and caption data,
                   we train a vision encoder to represent each image as a
                   sequence of continuous embeddings, such that a pre-trained,
                   frozen language model prompted with this prefix generates
                   the appropriate caption. The resulting system is a
                   multimodal few-shot learner, with the surprising ability to
                   learn a variety of new tasks when conditioned on examples,
                   represented as a sequence of multiple interleaved image and
                   text embeddings. We demonstrate that it can rapidly learn
                   words for new objects and novel visual categories, do visual
                   question-answering with only a handful of examples, and make
                   use of outside knowledge, by measuring a single model on a
                   variety of established and new benchmarks.",
  month         =  jun,
  year          =  2021,
  keywords      = "Multimodal (Vision, speech, etc)",
  archivePrefix = "arXiv",
  eprint        = "2106.13884",
  primaryClass  = "cs.CV",
  arxivid       = "2106.13884"
}

@ARTICLE{Alayrac2022-pr,
  title         = "Flamingo: a Visual Language Model for {Few-Shot} Learning",
  author        = "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline
                   and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc,
                   Karel and Mensch, Arthur and Millican, Katie and Reynolds,
                   Malcolm and Ring, Roman and Rutherford, Eliza and Cabi,
                   Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina
                   and Monteiro, Marianne and Menick, Jacob and Borgeaud,
                   Sebastian and Brock, Andrew and Nematzadeh, Aida and
                   Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira,
                   Ricardo and Vinyals, Oriol and Zisserman, Andrew and
                   Simonyan, Karen",
  abstract      = "Building models that can be rapidly adapted to novel tasks
                   using only a handful of annotated examples is an open
                   challenge for multimodal machine learning research. We
                   introduce Flamingo, a family of Visual Language Models (VLM)
                   with this ability. We propose key architectural innovations
                   to: (i) bridge powerful pretrained vision-only and
                   language-only models, (ii) handle sequences of arbitrarily
                   interleaved visual and textual data, and (iii) seamlessly
                   ingest images or videos as inputs. Thanks to their
                   flexibility, Flamingo models can be trained on large-scale
                   multimodal web corpora containing arbitrarily interleaved
                   text and images, which is key to endow them with in-context
                   few-shot learning capabilities. We perform a thorough
                   evaluation of our models, exploring and measuring their
                   ability to rapidly adapt to a variety of image and video
                   tasks. These include open-ended tasks such as visual
                   question-answering, where the model is prompted with a
                   question which it has to answer; captioning tasks, which
                   evaluate the ability to describe a scene or an event; and
                   close-ended tasks such as multiple-choice visual
                   question-answering. For tasks lying anywhere on this
                   spectrum, a single Flamingo model can achieve a new state of
                   the art with few-shot learning, simply by prompting the
                   model with task-specific examples. On numerous benchmarks,
                   Flamingo outperforms models fine-tuned on thousands of times
                   more task-specific data.",
  month         =  apr,
  year          =  2022,
  keywords      = "Multimodal (Vision, speech, etc)",
  archivePrefix = "arXiv",
  eprint        = "2204.14198",
  primaryClass  = "cs.CV",
  arxivid       = "2204.14198"
}

@ARTICLE{noauthor_undated-fp,
  title         = "Flamingo: a Visual Language Model for {Few-Shot} Learning",
  author        = "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline
                   and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc,
                   Karel and Mensch, Arthur and Millican, Katie and Reynolds,
                   Malcolm and Ring, Roman and Rutherford, Eliza and Cabi,
                   Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina
                   and Monteiro, Marianne and Menick, Jacob and Borgeaud,
                   Sebastian and Brock, Andrew and Nematzadeh, Aida and
                   Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira,
                   Ricardo and Vinyals, Oriol and Zisserman, Andrew and
                   Simonyan, Karen",
  abstract      = "Building models that can be rapidly adapted to novel tasks
                   using only a handful of annotated examples is an open
                   challenge for multimodal machine learning research. We
                   introduce Flamingo, a family of Visual Language Models (VLM)
                   with this ability. We propose key architectural innovations
                   to: (i) bridge powerful pretrained vision-only and
                   language-only models, (ii) handle sequences of arbitrarily
                   interleaved visual and textual data, and (iii) seamlessly
                   ingest images or videos as inputs. Thanks to their
                   flexibility, Flamingo models can be trained on large-scale
                   multimodal web corpora containing arbitrarily interleaved
                   text and images, which is key to endow them with in-context
                   few-shot learning capabilities. We perform a thorough
                   evaluation of our models, exploring and measuring their
                   ability to rapidly adapt to a variety of image and video
                   tasks. These include open-ended tasks such as visual
                   question-answering, where the model is prompted with a
                   question which it has to answer; captioning tasks, which
                   evaluate the ability to describe a scene or an event; and
                   close-ended tasks such as multiple-choice visual
                   question-answering. For tasks lying anywhere on this
                   spectrum, a single Flamingo model can achieve a new state of
                   the art with few-shot learning, simply by prompting the
                   model with task-specific examples. On numerous benchmarks,
                   Flamingo outperforms models fine-tuned on thousands of times
                   more task-specific data.",
  journal       = "arXiv [cs.CV]",
  month         =  apr,
  year          =  2022,
  keywords      = "Multimodal (Vision, speech, etc)",
  archivePrefix = "arXiv",
  eprint        = "2204.14198",
  primaryClass  = "cs.CV",
  arxivid       = "2204.14198"
}

@ARTICLE{Radford2021-pk,
  title         = "Learning transferable visual models from natural language
                   supervision",
  author        = "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and
                   Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and
                   Sastry, Girish and Askell, Amanda and Mishkin, Pamela and
                   Clark, Jack and Krueger, Gretchen and Sutskever, Ilya",
  abstract      = "State-of-the-art computer vision systems are trained to
                   predict a fixed set of predetermined object categories. This
                   restricted form of supervision limits their generality and
                   usability since additional labeled data is needed to specify
                   any other visual concept. Learning directly from raw text
                   about images is a promising alternative which leverages a
                   much broader source of supervision. We demonstrate that the
                   simple pre-training task of predicting which caption goes
                   with which image is an efficient and scalable way to learn
                   SOTA image representations from scratch on a dataset of 400
                   million (image, text) pairs collected from the internet.
                   After pre-training, natural language is used to reference
                   learned visual concepts (or describe new ones) enabling
                   zero-shot transfer of the model to downstream tasks. We
                   study the performance of this approach by benchmarking on
                   over 30 different existing computer vision datasets,
                   spanning tasks such as OCR, action recognition in videos,
                   geo-localization, and many types of fine-grained object
                   classification. The model transfers non-trivially to most
                   tasks and is often competitive with a fully supervised
                   baseline without the need for any dataset specific training.
                   For instance, we match the accuracy of the original
                   ResNet-50 on ImageNet zero-shot without needing to use any
                   of the 1.28 million training examples it was trained on. We
                   release our code and pre-trained model weights at
                   https://github.com/OpenAI/CLIP.",
  month         =  feb,
  year          =  2021,
  keywords      = "Multimodal (Vision, speech, etc)",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2103.00020",
  primaryClass  = "cs.CV",
  arxivid       = "2103.00020"
}

@ARTICLE{Zhong2022-lu,
  title         = "Training language models with memory augmentation",
  author        = "Zhong, Zexuan and Lei, Tao and Chen, Danqi",
  abstract      = "Recent work has improved language models (LMs) remarkably by
                   equipping them with a non-parametric memory component.
                   However, most existing approaches only introduce mem-ories
                   at testing time or represent them using a separately trained
                   encoder, resulting in suboptimal training of the language
                   model. In this work, we present TRIME, a novel yet simple
                   training approach designed for training LMs with memory
                   augmentation. Our approach uses a training objective that
                   directly takes in-batch examples as accessible memory. We
                   also present new methods for memory construction and data
                   batching, which are used for adapting to different sets of
                   memories--local, long-term, and external memory--at testing
                   time. We evaluate TRIME on multiple language modeling and
                   machine translation benchmarks and show that it is able to
                   achieve significant improvements across all the settings.
                   Concretely, TRIME reduces the perplexity from 18.70 to 15.37
                   on WIKITEXT-103, by effectively leveraging a large memory
                   set from the training corpus. Compared to standard LM
                   training, TRIME adds negligible computational overhead and
                   is compatible with different neural architectures, making it
                   a versatile solution for training memory-augmented LMs.",
  month         =  may,
  year          =  2022,
  keywords      = "RAG",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2205.12674",
  primaryClass  = "cs.CL",
  arxivid       = "2205.12674"
}

@ARTICLE{Tay2021-bw,
  title         = "Scale Efficiently: Insights from Pre-training and
                   Fine-tuning Transformers",
  author        = "Tay, Yi and Dehghani, Mostafa and Rao, Jinfeng and Fedus,
                   William and Abnar, Samira and Chung, Hyung Won and Narang,
                   Sharan and Yogatama, Dani and Vaswani, Ashish and Metzler,
                   Donald",
  abstract      = "There remain many open questions pertaining to the scaling
                   behaviour of Transformer architectures. These scaling
                   decisions and findings can be critical, as training runs
                   often come with an associated computational cost which have
                   both financial and/or environmental impact. The goal of this
                   paper is to present scaling insights from pretraining and
                   finetuning Transformers. While Kaplan et al. presents a
                   comprehensive study of the scaling behaviour of Transformer
                   language models, the scope is only on the upstream
                   (pretraining) loss. Therefore, it is still unclear if these
                   set of findings transfer to downstream task within the
                   context of the pretrain-finetune paradigm. The key findings
                   of this paper are as follows: (1) we show that aside from
                   only the model size, model shape matters for downstream
                   fine-tuning, (2) scaling protocols operate differently at
                   different compute regions, (3) widely adopted T5-base and
                   T5-large sizes are Pareto-inefficient. To this end, we
                   present improved scaling protocols whereby our redesigned
                   models achieve similar downstream fine-tuning quality while
                   having 50\% fewer parameters and training 40\% faster
                   compared to the widely adopted T5-base model. We publicly
                   release over 100 pretrained checkpoints of different T5
                   configurations to facilitate future research and analysis.",
  month         =  sep,
  year          =  2021,
  keywords      = "Scaling",
  archivePrefix = "arXiv",
  eprint        = "2109.10686",
  primaryClass  = "cs.CL",
  arxivid       = "2109.10686"
}

@ARTICLE{Henighan2020-va,
  title         = "Scaling Laws for Autoregressive Generative Modeling",
  author        = "Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark
                   and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo
                   and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and
                   Hallacy, Chris and Mann, Benjamin and Radford, Alec and
                   Ramesh, Aditya and Ryder, Nick and Ziegler, Daniel M and
                   Schulman, John and Amodei, Dario and McCandlish, Sam",
  abstract      = "We identify empirical scaling laws for the cross-entropy
                   loss in four domains: generative image modeling, video
                   modeling, multimodal image$\leftrightarrow$text models, and
                   mathematical problem solving. In all cases autoregressive
                   Transformers smoothly improve in performance as model size
                   and compute budgets increase, following a power-law plus
                   constant scaling law. The optimal model size also depends on
                   the compute budget through a power-law, with exponents that
                   are nearly universal across all data domains. The
                   cross-entropy loss has an information theoretic
                   interpretation as $S($True$) +
                   D_\{\mathrm\{KL\}\}($True$||$Model$)$, and the empirical
                   scaling laws suggest a prediction for both the true data
                   distribution's entropy and the KL divergence between the
                   true and model distributions. With this interpretation,
                   billion-parameter Transformers are nearly perfect models of
                   the YFCC100M image distribution downsampled to an $8\times
                   8$ resolution, and we can forecast the model size needed to
                   achieve any given reducible loss (ie $D_\{\mathrm\{KL\}\}$)
                   in nats/image for other resolutions. We find a number of
                   additional scaling laws in specific domains: (a) we identify
                   a scaling relation for the mutual information between
                   captions and images in multimodal models, and show how to
                   answer the question ``Is a picture worth a thousand
                   words?''; (b) in the case of mathematical problem solving,
                   we identify scaling laws for model performance when
                   extrapolating beyond the training distribution; (c) we
                   finetune generative image models for ImageNet classification
                   and find smooth scaling of the classification loss and error
                   rate, even as the generative loss levels off. Taken
                   together, these results strengthen the case that scaling
                   laws have important implications for neural network
                   performance, including on downstream tasks.",
  month         =  oct,
  year          =  2020,
  keywords      = "Scaling",
  archivePrefix = "arXiv",
  eprint        = "2010.14701",
  primaryClass  = "cs.LG",
  arxivid       = "2010.14701"
}

@ARTICLE{Hoffmann2022-am,
  title         = "Training {Compute-Optimal} Large Language Models",
  author        = "Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur
                   and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza
                   and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl,
                   Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric
                   and Millican, Katie and van den Driessche, George and Damoc,
                   Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan,
                   Karen and Elsen, Erich and Rae, Jack W and Vinyals, Oriol
                   and Sifre, Laurent",
  abstract      = "We investigate the optimal model size and number of tokens
                   for training a transformer language model under a given
                   compute budget. We find that current large language models
                   are significantly undertrained, a consequence of the recent
                   focus on scaling language models whilst keeping the amount
                   of training data constant. By training over 400 language
                   models ranging from 70 million to over 16 billion parameters
                   on 5 to 500 billion tokens, we find that for compute-optimal
                   training, the model size and the number of training tokens
                   should be scaled equally: for every doubling of model size
                   the number of training tokens should also be doubled. We
                   test this hypothesis by training a predicted compute-optimal
                   model, Chinchilla, that uses the same compute budget as
                   Gopher but with 70B parameters and 4$\times$ more more data.
                   Chinchilla uniformly and significantly outperforms Gopher
                   (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing
                   NLG (530B) on a large range of downstream evaluation tasks.
                   This also means that Chinchilla uses substantially less
                   compute for fine-tuning and inference, greatly facilitating
                   downstream usage. As a highlight, Chinchilla reaches a
                   state-of-the-art average accuracy of 67.5\% on the MMLU
                   benchmark, greater than a 7\% improvement over Gopher.",
  month         =  mar,
  year          =  2022,
  keywords      = "Scaling",
  archivePrefix = "arXiv",
  eprint        = "2203.15556",
  primaryClass  = "cs.CL",
  arxivid       = "2203.15556"
}

@ARTICLE{Kaplan2020-se,
  title         = "Scaling Laws for Neural Language Models",
  author        = "Kaplan, Jared and McCandlish, Sam and Henighan, Tom and
                   Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray,
                   Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario",
  abstract      = "We study empirical scaling laws for language model
                   performance on the cross-entropy loss. The loss scales as a
                   power-law with model size, dataset size, and the amount of
                   compute used for training, with some trends spanning more
                   than seven orders of magnitude. Other architectural details
                   such as network width or depth have minimal effects within a
                   wide range. Simple equations govern the dependence of
                   overfitting on model/dataset size and the dependence of
                   training speed on model size. These relationships allow us
                   to determine the optimal allocation of a fixed compute
                   budget. Larger models are significantly more
                   sample-efficient, such that optimally compute-efficient
                   training involves training very large models on a relatively
                   modest amount of data and stopping significantly before
                   convergence.",
  month         =  jan,
  year          =  2020,
  keywords      = "Scaling",
  archivePrefix = "arXiv",
  eprint        = "2001.08361",
  primaryClass  = "cs.LG",
  arxivid       = "2001.08361"
}

@ARTICLE{Chowdhery2022-fd,
  title         = "{PaLM}: Scaling Language Modeling with Pathways",
  author        = "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob
                   and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and
                   Barham, Paul and Chung, Hyung Won and Sutton, Charles and
                   Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and
                   Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek
                   and Barnes, Parker and Tay, Yi and Shazeer, Noam and
                   Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and
                   Hutchinson, Ben and Pope, Reiner and Bradbury, James and
                   Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin,
                   Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat,
                   Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia,
                   Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam
                   and Zhou, Denny and Ippolito, Daphne and Luan, David and
                   Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander
                   and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and
                   Omernick, Mark and Dai, Andrew M and Pillai, Thanumalayan
                   Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and
                   Moreira, Erica and Child, Rewon and Polozov, Oleksandr and
                   Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta,
                   Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele
                   and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas
                   and Dean, Jeff and Petrov, Slav and Fiedel, Noah",
  abstract      = "Large language models have been shown to achieve remarkable
                   performance across a variety of natural language tasks using
                   few-shot learning, which drastically reduces the number of
                   task-specific training examples needed to adapt the model to
                   a particular application. To further our understanding of
                   the impact of scale on few-shot learning, we trained a
                   540-billion parameter, densely activated, Transformer
                   language model, which we call Pathways Language Model PaLM.
                   We trained PaLM on 6144 TPU v4 chips using Pathways, a new
                   ML system which enables highly efficient training across
                   multiple TPU Pods. We demonstrate continued benefits of
                   scaling by achieving state-of-the-art few-shot learning
                   results on hundreds of language understanding and generation
                   benchmarks. On a number of these tasks, PaLM 540B achieves
                   breakthrough performance, outperforming the finetuned
                   state-of-the-art on a suite of multi-step reasoning tasks,
                   and outperforming average human performance on the recently
                   released BIG-bench benchmark. A significant number of
                   BIG-bench tasks showed discontinuous improvements from model
                   scale, meaning that performance steeply increased as we
                   scaled to our largest model. PaLM also has strong
                   capabilities in multilingual tasks and source code
                   generation, which we demonstrate on a wide array of
                   benchmarks. We additionally provide a comprehensive analysis
                   on bias and toxicity, and study the extent of training data
                   memorization with respect to model scale. Finally, we
                   discuss the ethical considerations related to large language
                   models and discuss potential mitigation strategies.",
  month         =  apr,
  year          =  2022,
  keywords      = "Decorder only",
  archivePrefix = "arXiv",
  eprint        = "2204.02311",
  primaryClass  = "cs.CL",
  arxivid       = "2204.02311"
}

@ARTICLE{Chung2022-qg,
  title         = "Scaling instruction-finetuned language models",
  author        = "Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph,
                   Barret and Tay, Yi and Fedus, William and Li, Yunxuan and
                   Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha
                   and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun
                   and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha
                   and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin
                   and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and
                   Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai,
                   Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H and
                   Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou,
                   Denny and Le, Quoc V and Wei, Jason",
  abstract      = "Finetuning language models on a collection of datasets
                   phrased as instructions has been shown to improve model
                   performance and generalization to unseen tasks. In this
                   paper we explore instruction finetuning with a particular
                   focus on (1) scaling the number of tasks, (2) scaling the
                   model size, and (3) finetuning on chain-of-thought data. We
                   find that instruction finetuning with the above aspects
                   dramatically improves performance on a variety of model
                   classes (PaLM, T5, U-PaLM), prompting setups (zero-shot,
                   few-shot, CoT), and evaluation benchmarks (MMLU, BBH,
                   TyDiQA, MGSM, open-ended generation). For instance,
                   Flan-PaLM 540B instruction-finetuned on 1.8K tasks
                   outperforms PALM 540B by a large margin (+9.4\% on average).
                   Flan-PaLM 540B achieves state-of-the-art performance on
                   several benchmarks, such as 75.2\% on five-shot MMLU. We
                   also publicly release Flan-T5 checkpoints, which achieve
                   strong few-shot performance even compared to much larger
                   models, such as PaLM 62B. Overall, instruction finetuning is
                   a general method for improving the performance and usability
                   of pretrained language models.",
  month         =  oct,
  year          =  2022,
  keywords      = "Fine-tuning;Scaling;Models",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2210.11416",
  primaryClass  = "cs.LG",
  arxivid       = "2210.11416"
}

@ARTICLE{Kadavath2022-hj,
  title         = "Language Models (Mostly) Know What They Know",
  author        = "Kadavath, Saurav and Conerly, Tom and Askell, Amanda and
                   Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer,
                   Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and
                   Tran-Johnson, Eli and Johnston, Scott and El-Showk, Sheer
                   and Jones, Andy and Elhage, Nelson and Hume, Tristan and
                   Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort,
                   Stanislav and Ganguli, Deep and Hernandez, Danny and
                   Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and
                   Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and
                   Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack
                   and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and
                   Olah, Chris and Kaplan, Jared",
  abstract      = "We study whether language models can evaluate the validity
                   of their own claims and predict which questions they will be
                   able to answer correctly. We first show that larger models
                   are well-calibrated on diverse multiple choice and
                   true/false questions when they are provided in the right
                   format. Thus we can approach self-evaluation on open-ended
                   sampling tasks by asking models to first propose answers,
                   and then to evaluate the probability ``P(True)'' that their
                   answers are correct. We find encouraging performance,
                   calibration, and scaling for P(True) on a diverse array of
                   tasks. Performance at self-evaluation further improves when
                   we allow models to consider many of their own samples before
                   predicting the validity of one specific possibility. Next,
                   we investigate whether models can be trained to predict
                   ``P(IK)'', the probability that ``I know'' the answer to a
                   question, without reference to any particular proposed
                   answer. Models perform well at predicting P(IK) and
                   partially generalize across tasks, though they struggle with
                   calibration of P(IK) on new tasks. The predicted P(IK)
                   probabilities also increase appropriately in the presence of
                   relevant source materials in the context, and in the
                   presence of hints towards the solution of mathematical word
                   problems. We hope these observations lay the groundwork for
                   training more honest models, and for investigating how
                   honesty generalizes to cases where models are trained on
                   objectives other than the imitation of human writing.",
  month         =  jul,
  year          =  2022,
  keywords      = "Understanding;Intriguing Properties;LLMs",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2207.05221",
  primaryClass  = "cs.CL",
  arxivid       = "2207.05221"
}

@ARTICLE{Zheng2023-ww,
  title         = "{GPT-Fathom}: Benchmarking large language models to decipher
                   the evolutionary path towards {GPT-4} and beyond",
  author        = "Zheng, Shen and Zhang, Yuyu and Zhu, Yijie and Xi, Chenguang
                   and Gao, Pengyang and Zhou, Xun and Chang, Kevin Chen-Chuan",
  abstract      = "With the rapid advancement of large language models (LLMs),
                   there is a pressing need for a comprehensive evaluation
                   suite to assess their capabilities and limitations. Existing
                   LLM leaderboards often reference scores reported in other
                   papers without consistent settings and prompts, which may
                   inadvertently encourage cherry-picking favored settings and
                   prompts for better results. In this work, we introduce
                   GPT-Fathom, an open-source and reproducible LLM evaluation
                   suite built on top of OpenAI Evals. We systematically
                   evaluate 10+ leading LLMs as well as OpenAI's legacy models
                   on 20+ curated benchmarks across 7 capability categories,
                   all under aligned settings. Our retrospective study on
                   OpenAI's earlier models offers valuable insights into the
                   evolutionary path from GPT-3 to GPT-4. Currently, the
                   community is eager to know how GPT-3 progressively improves
                   to GPT-4, including technical details like whether adding
                   code data improves LLM's reasoning capability, which aspects
                   of LLM capability can be improved by SFT and RLHF, how much
                   is the alignment tax, etc. Our analysis sheds light on many
                   of these questions, aiming to improve the transparency of
                   advanced LLMs.",
  month         =  sep,
  year          =  2023,
  keywords      = "LLMs;Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2309.16583",
  primaryClass  = "cs.CL",
  arxivid       = "2309.16583"
}

@ARTICLE{Borgeaud2021-ow,
  title         = "Improving language models by retrieving from trillions of
                   tokens",
  author        = "Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan
                   and Cai, Trevor and Rutherford, Eliza and Millican, Katie
                   and van den Driessche, George and Lespiau, Jean-Baptiste and
                   Damoc, Bogdan and Clark, Aidan and de Las Casas, Diego and
                   Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan,
                   Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris
                   and Cassirer, Albin and Brock, Andy and Paganini, Michela
                   and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon
                   and Simonyan, Karen and Rae, Jack W and Elsen, Erich and
                   Sifre, Laurent",
  abstract      = "We enhance auto-regressive language models by conditioning
                   on document chunks retrieved from a large corpus, based on
                   local similarity with preceding tokens. With a $2$ trillion
                   token database, our Retrieval-Enhanced Transformer (RETRO)
                   obtains comparable performance to GPT-3 and Jurassic-1 on
                   the Pile, despite using 25$\times$ fewer parameters. After
                   fine-tuning, RETRO performance translates to downstream
                   knowledge-intensive tasks such as question answering. RETRO
                   combines a frozen Bert retriever, a differentiable encoder
                   and a chunked cross-attention mechanism to predict tokens
                   based on an order of magnitude more data than what is
                   typically consumed during training. We typically train RETRO
                   from scratch, yet can also rapidly RETROfit pre-trained
                   transformers with retrieval and still achieve good
                   performance. Our work opens up new avenues for improving
                   language models through explicit memory at unprecedented
                   scale.",
  month         =  dec,
  year          =  2021,
  keywords      = "RAG",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2112.04426",
  primaryClass  = "cs.CL",
  arxivid       = "2112.04426"
}

@ARTICLE{Chan2022-mv,
  title         = "Data Distributional Properties Drive Emergent {In-Context}
                   Learning in Transformers",
  author        = "Chan, Stephanie C Y and Santoro, Adam and Lampinen, Andrew K
                   and Wang, Jane X and Singh, Aaditya and Richemond, Pierre H
                   and McClelland, Jay and Hill, Felix",
  abstract      = "Large transformer-based models are able to perform
                   in-context few-shot learning, without being explicitly
                   trained for it. This observation raises the question: what
                   aspects of the training regime lead to this emergent
                   behavior? Here, we show that this behavior is driven by the
                   distributions of the training data itself. In-context
                   learning emerges when the training data exhibits particular
                   distributional properties such as burstiness (items appear
                   in clusters rather than being uniformly distributed over
                   time) and having large numbers of rarely occurring classes.
                   In-context learning also emerges more strongly when item
                   meanings or interpretations are dynamic rather than fixed.
                   These properties are exemplified by natural language, but
                   are also inherent to naturalistic data in a wide range of
                   other domains. They also depart significantly from the
                   uniform, i.i.d. training distributions typically used for
                   standard supervised learning. In our initial experiments, we
                   found that in-context learning traded off against more
                   conventional weight-based learning, and models were unable
                   to achieve both simultaneously. However, our later
                   experiments uncovered that the two modes of learning could
                   co-exist in a single model when it was trained on data
                   following a skewed Zipfian distribution -- another common
                   property of naturalistic data, including language. In
                   further experiments, we found that naturalistic data
                   distributions were only able to elicit in-context learning
                   in transformers, and not in recurrent models. In sum, our
                   findings indicate how the transformer architecture works
                   together with particular properties of the training data to
                   drive the intriguing emergent in-context learning behaviour
                   of large language models, and how future work might
                   encourage both in-context and in-weights learning in domains
                   beyond language.",
  month         =  apr,
  year          =  2022,
  keywords      = "In-Context",
  archivePrefix = "arXiv",
  eprint        = "2205.05055",
  primaryClass  = "cs.LG",
  arxivid       = "2205.05055"
}

@ARTICLE{Wei2022-fs,
  title         = "{Chain-of-Thought} Prompting Elicits Reasoning in Large
                   Language Models",
  author        = "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma,
                   Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le,
                   Quoc and Zhou, Denny",
  abstract      = "We explore how generating a chain of thought -- a series of
                   intermediate reasoning steps -- significantly improves the
                   ability of large language models to perform complex
                   reasoning. In particular, we show how such reasoning
                   abilities emerge naturally in sufficiently large language
                   models via a simple method called chain of thought
                   prompting, where a few chain of thought demonstrations are
                   provided as exemplars in prompting. Experiments on three
                   large language models show that chain of thought prompting
                   improves performance on a range of arithmetic, commonsense,
                   and symbolic reasoning tasks. The empirical gains can be
                   striking. For instance, prompting a 540B-parameter language
                   model with just eight chain of thought exemplars achieves
                   state of the art accuracy on the GSM8K benchmark of math
                   word problems, surpassing even finetuned GPT-3 with a
                   verifier.",
  month         =  jan,
  year          =  2022,
  keywords      = "In-Context",
  archivePrefix = "arXiv",
  eprint        = "2201.11903",
  primaryClass  = "cs.CL",
  arxivid       = "2201.11903"
}

@ARTICLE{Schulman2017-yn,
  title         = "Proximal Policy Optimization Algorithms",
  author        = "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and
                   Radford, Alec and Klimov, Oleg",
  abstract      = "We propose a new family of policy gradient methods for
                   reinforcement learning, which alternate between sampling
                   data through interaction with the environment, and
                   optimizing a ``surrogate'' objective function using
                   stochastic gradient ascent. Whereas standard policy gradient
                   methods perform one gradient update per data sample, we
                   propose a novel objective function that enables multiple
                   epochs of minibatch updates. The new methods, which we call
                   proximal policy optimization (PPO), have some of the
                   benefits of trust region policy optimization (TRPO), but
                   they are much simpler to implement, more general, and have
                   better sample complexity (empirically). Our experiments test
                   PPO on a collection of benchmark tasks, including simulated
                   robotic locomotion and Atari game playing, and we show that
                   PPO outperforms other online policy gradient methods, and
                   overall strikes a favorable balance between sample
                   complexity, simplicity, and wall-time.",
  month         =  jul,
  year          =  2017,
  keywords      = "Alignment (RLHF, etc)",
  archivePrefix = "arXiv",
  eprint        = "1707.06347",
  primaryClass  = "cs.LG",
  arxivid       = "1707.06347"
}

@MISC{Touvron_undated-ol,
  title       = "llama: Inference code for {LLaMA} models",
  author      = "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and
                 Lachaux, Xavier Martinet Marie-Anne and Lacroix, Timothee and
                 Rozi{\`e}re, Baptiste and Hambro, Naman Goyal Eric and Azhar,
                 Faisal and Rodriguez, Aurelien and Grave, Armand Joulin
                 Edouard and Lample, Guillaume",
  abstract    = "Inference code for LLaMA models. Contribute to
                 facebookresearch/llama development by creating an account on
                 GitHub.",
  institution = "Github",
  keywords    = "Models",
  language    = "en"
}

@ARTICLE{Zou2023-hp,
  title         = "Universal and Transferable Adversarial Attacks on Aligned
                   Language Models",
  author        = "Zou, Andy and Wang, Zifan and Zico Kolter, J and Fredrikson,
                   Matt",
  abstract      = "Because ``out-of-the-box'' large language models are capable
                   of generating a great deal of objectionable content, recent
                   work has focused on aligning these models in an attempt to
                   prevent undesirable generation. While there has been some
                   success at circumventing these measures -- so-called
                   ``jailbreaks'' against LLMs -- these attacks have required
                   significant human ingenuity and are brittle in practice. In
                   this paper, we propose a simple and effective attack method
                   that causes aligned language models to generate
                   objectionable behaviors. Specifically, our approach finds a
                   suffix that, when attached to a wide range of queries for an
                   LLM to produce objectionable content, aims to maximize the
                   probability that the model produces an affirmative response
                   (rather than refusing to answer). However, instead of
                   relying on manual engineering, our approach automatically
                   produces these adversarial suffixes by a combination of
                   greedy and gradient-based search techniques, and also
                   improves over past automatic prompt generation methods.
                   Surprisingly, we find that the adversarial prompts generated
                   by our approach are quite transferable, including to
                   black-box, publicly released LLMs. Specifically, we train an
                   adversarial attack suffix on multiple prompts (i.e., queries
                   asking for many different types of objectionable content),
                   as well as multiple models (in our case, Vicuna-7B and 13B).
                   When doing so, the resulting attack suffix is able to induce
                   objectionable content in the public interfaces to ChatGPT,
                   Bard, and Claude, as well as open source LLMs such as
                   LLaMA-2-Chat, Pythia, Falcon, and others. In total, this
                   work significantly advances the state-of-the-art in
                   adversarial attacks against aligned language models, raising
                   important questions about how such systems can be prevented
                   from producing objectionable information. Code is available
                   at github.com/llm-attacks/llm-attacks.",
  month         =  jul,
  year          =  2023,
  keywords      = "Safety/Risks",
  archivePrefix = "arXiv",
  eprint        = "2307.15043",
  primaryClass  = "cs.CL",
  arxivid       = "2307.15043"
}

@ARTICLE{Shazeer2017-mj,
  title         = "Outrageously Large Neural Networks: The {Sparsely-Gated}
                   {Mixture-of-Experts} Layer",
  author        = "Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof
                   and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean,
                   Jeff",
  abstract      = "The capacity of a neural network to absorb information is
                   limited by its number of parameters. Conditional
                   computation, where parts of the network are active on a
                   per-example basis, has been proposed in theory as a way of
                   dramatically increasing model capacity without a
                   proportional increase in computation. In practice, however,
                   there are significant algorithmic and performance
                   challenges. In this work, we address these challenges and
                   finally realize the promise of conditional computation,
                   achieving greater than 1000x improvements in model capacity
                   with only minor losses in computational efficiency on modern
                   GPU clusters. We introduce a Sparsely-Gated
                   Mixture-of-Experts layer (MoE), consisting of up to
                   thousands of feed-forward sub-networks. A trainable gating
                   network determines a sparse combination of these experts to
                   use for each example. We apply the MoE to the tasks of
                   language modeling and machine translation, where model
                   capacity is critical for absorbing the vast quantities of
                   knowledge available in the training corpora. We present
                   model architectures in which a MoE with up to 137 billion
                   parameters is applied convolutionally between stacked LSTM
                   layers. On large language modeling and machine translation
                   benchmarks, these models achieve significantly better
                   results than state-of-the-art at lower computational cost.",
  month         =  jan,
  year          =  2017,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "1701.06538",
  primaryClass  = "cs.LG",
  arxivid       = "1701.06538"
}

@ARTICLE{Schick2023-rj,
  title         = "Toolformer: Language Models Can Teach Themselves to Use
                   Tools",
  author        = "Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto
                   and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer,
                   Luke and Cancedda, Nicola and Scialom, Thomas",
  abstract      = "Language models (LMs) exhibit remarkable abilities to solve
                   new tasks from just a few examples or textual instructions,
                   especially at scale. They also, paradoxically, struggle with
                   basic functionality, such as arithmetic or factual lookup,
                   where much simpler and smaller models excel. In this paper,
                   we show that LMs can teach themselves to use external tools
                   via simple APIs and achieve the best of both worlds. We
                   introduce Toolformer, a model trained to decide which APIs
                   to call, when to call them, what arguments to pass, and how
                   to best incorporate the results into future token
                   prediction. This is done in a self-supervised way, requiring
                   nothing more than a handful of demonstrations for each API.
                   We incorporate a range of tools, including a calculator, a
                   Q\&A system, two different search engines, a translation
                   system, and a calendar. Toolformer achieves substantially
                   improved zero-shot performance across a variety of
                   downstream tasks, often competitive with much larger models,
                   without sacrificing its core language modeling abilities.",
  month         =  feb,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2302.04761",
  primaryClass  = "cs.CL",
  arxivid       = "2302.04761",
  doi           = "10.48550/ARXIV.2302.04761"
}

@ARTICLE{Kenton_undated-ez,
  title    = "Alignment of Language Agents",
  author   = "Kenton, Zachary and Everitt, Tom and Weidinger, Laura and
              Gabriel, Iason and Mikulik, Vladimir and Irving, Geoffrey",
  keywords = "Alignment (RLHF, etc)",
  arxivid  = "2103.14659"
}

@ARTICLE{Askell2021-ou,
  title         = "A General Language Assistant as a Laboratory for Alignment",
  author        = "Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain,
                   Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and
                   Joseph, Nicholas and Mann, Ben and DasSarma, Nova and
                   Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny
                   and Kernion, Jackson and Ndousse, Kamal and Olsson,
                   Catherine and Amodei, Dario and Brown, Tom and Clark, Jack
                   and McCandlish, Sam and Olah, Chris and Kaplan, Jared",
  abstract      = "Given the broad capabilities of large language models, it
                   should be possible to work towards a general-purpose,
                   text-based assistant that is aligned with human values,
                   meaning that it is helpful, honest, and harmless. As an
                   initial foray in this direction we study simple baseline
                   techniques and evaluations, such as prompting. We find that
                   the benefits from modest interventions increase with model
                   size, generalize to a variety of alignment evaluations, and
                   do not compromise the performance of large models. Next we
                   investigate scaling trends for several training objectives
                   relevant to alignment, comparing imitation learning, binary
                   discrimination, and ranked preference modeling. We find that
                   ranked preference modeling performs much better than
                   imitation learning, and often scales more favorably with
                   model size. In contrast, binary discrimination typically
                   performs and scales very similarly to imitation learning.
                   Finally we study a `preference model pre-training' stage of
                   training, with the goal of improving sample efficiency when
                   finetuning on human preferences.",
  month         =  dec,
  year          =  2021,
  keywords      = "Alignment (RLHF, etc)",
  archivePrefix = "arXiv",
  eprint        = "2112.00861",
  primaryClass  = "cs.CL",
  arxivid       = "2112.00861"
}

@ARTICLE{Nijkamp2022-lj,
  title         = "{CodeGen}: An open large language model for code with
                   multi-turn program synthesis",
  author        = "Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu
                   and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and
                   Xiong, Caiming",
  abstract      = "Program synthesis strives to generate a computer program as
                   a solution to a given problem specification, expressed with
                   input-output examples or natural language descriptions. The
                   prevalence of large language models advances the
                   state-of-the-art for program synthesis, though limited
                   training resources and data impede open access to such
                   models. To democratize this, we train and release a family
                   of large language models up to 16.1B parameters, called
                   CODEGEN, on natural language and programming language data,
                   and open source the training library JAXFORMER. We show the
                   utility of the trained model by demonstrating that it is
                   competitive with the previous state-of-the-art on zero-shot
                   Python code generation on HumanEval. We further investigate
                   the multi-step paradigm for program synthesis, where a
                   single program is factorized into multiple prompts
                   specifying subproblems. To this end, we construct an open
                   benchmark, Multi-Turn Programming Benchmark (MTPB),
                   consisting of 115 diverse problem sets that are factorized
                   into multi-turn prompts. Our analysis on MTPB shows that the
                   same intent provided to CODEGEN in multi-turn fashion
                   significantly improves program synthesis over that provided
                   as a single turn. We make the training library JAXFORMER and
                   model checkpoints available as open source contribution:
                   https://github.com/salesforce/CodeGen.",
  month         =  mar,
  year          =  2022,
  keywords      = "Code",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2203.13474",
  primaryClass  = "cs.LG",
  arxivid       = "2203.13474"
}

@ARTICLE{Chen2021-er,
  title         = "Evaluating Large Language Models Trained on Code",
  author        = "Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan,
                   Qiming and de Oliveira Pinto, Henrique Ponde and Kaplan,
                   Jared and Edwards, Harri and Burda, Yuri and Joseph,
                   Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and
                   Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and
                   Sastry, Girish and Mishkin, Pamela and Chan, Brooke and
                   Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power,
                   Alethea and Kaiser, Lukasz and Bavarian, Mohammad and
                   Winter, Clemens and Tillet, Philippe and Such, Felipe
                   Petroski and Cummings, Dave and Plappert, Matthias and
                   Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss,
                   Ariel and Guss, William Hebgen and Nichol, Alex and Paino,
                   Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor
                   and Balaji, Suchir and Jain, Shantanu and Saunders, William
                   and Hesse, Christopher and Carr, Andrew N and Leike, Jan and
                   Achiam, Josh and Misra, Vedant and Morikawa, Evan and
                   Radford, Alec and Knight, Matthew and Brundage, Miles and
                   Murati, Mira and Mayer, Katie and Welinder, Peter and
                   McGrew, Bob and Amodei, Dario and McCandlish, Sam and
                   Sutskever, Ilya and Zaremba, Wojciech",
  abstract      = "We introduce Codex, a GPT language model fine-tuned on
                   publicly available code from GitHub, and study its Python
                   code-writing capabilities. A distinct production version of
                   Codex powers GitHub Copilot. On HumanEval, a new evaluation
                   set we release to measure functional correctness for
                   synthesizing programs from docstrings, our model solves
                   28.8\% of the problems, while GPT-3 solves 0\% and GPT-J
                   solves 11.4\%. Furthermore, we find that repeated sampling
                   from the model is a surprisingly effective strategy for
                   producing working solutions to difficult prompts. Using this
                   method, we solve 70.2\% of our problems with 100 samples per
                   problem. Careful investigation of our model reveals its
                   limitations, including difficulty with docstrings describing
                   long chains of operations and with binding operations to
                   variables. Finally, we discuss the potential broader impacts
                   of deploying powerful code generation technologies, covering
                   safety, security, and economics.",
  month         =  jul,
  year          =  2021,
  keywords      = "Code;Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2107.03374",
  primaryClass  = "cs.LG",
  arxivid       = "2107.03374"
}

@MISC{Hellendoorn_undated-bs,
  title       = "{Code-LMs}: Guide to using pre-trained large language models
                 of source code",
  author      = "Hellendoorn, Vincent",
  abstract    = "Guide to using pre-trained large language models of source
                 code - VHellendoorn/Code-LMs: Guide to using pre-trained large
                 language models of source code",
  institution = "Github",
  keywords    = "Code;Evaluation",
  language    = "en"
}

@ARTICLE{Fried2022-vw,
  title         = "{InCoder}: A Generative Model for Code Infilling and
                   Synthesis",
  author        = "Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang,
                   Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and
                   Yih, Wen-Tau and Zettlemoyer, Luke and Lewis, Mike",
  abstract      = "Code is seldom written in a single left-to-right pass and is
                   instead repeatedly edited and refined. We introduce InCoder,
                   a unified generative model that can perform program
                   synthesis (via left-to-right generation) as well as editing
                   (via infilling). InCoder is trained to generate code files
                   from a large corpus of permissively licensed code, where
                   regions of code have been randomly masked and moved to the
                   end of each file, allowing code infilling with bidirectional
                   context. Our model is the first generative model that is
                   able to directly perform zero-shot code infilling, which we
                   evaluate on challenging tasks such as type inference,
                   comment generation, and variable re-naming. We find that the
                   ability to condition on bidirectional context substantially
                   improves performance on these tasks, while still performing
                   comparably on standard program synthesis benchmarks in
                   comparison to left-to-right only models pretrained at
                   similar scale. The InCoder models and code are publicly
                   released. https://sites.google.com/view/incoder-code-models",
  month         =  apr,
  year          =  2022,
  keywords      = "Code",
  archivePrefix = "arXiv",
  eprint        = "2204.05999",
  primaryClass  = "cs.SE",
  arxivid       = "2204.05999"
}

@ARTICLE{Li2022-qz,
  title    = "Competition-level code generation with {AlphaCode}",
  author   = "Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate
              and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom
              and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and
              Hubert, Thomas and Choy, Peter and de Masson d'Autume, Cyprien
              and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and
              Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and
              Molloy, James and Mankowitz, Daniel J and Sutherland Robson, Esme
              and Kohli, Pushmeet and de Freitas, Nando and Kavukcuoglu, Koray
              and Vinyals, Oriol",
  abstract = "Programming is a powerful and ubiquitous problem-solving tool.
              Systems that can assist programmers or even generate programs
              themselves could make programming more productive and accessible.
              Recent transformer-based neural network models show impressive
              code generation abilities yet still perform poorly on more
              complex tasks requiring problem-solving skills, such as
              competitive programming problems. Here, we introduce AlphaCode, a
              system for code generation that achieved an average ranking in
              the top 54.3\% in simulated evaluations on recent programming
              competitions on the Codeforces platform. AlphaCode solves
              problems by generating millions of diverse programs using
              specially trained transformer-based networks and then filtering
              and clustering those programs to a maximum of just 10
              submissions. This result marks the first time an artificial
              intelligence system has performed competitively in programming
              competitions.",
  journal  = "Science",
  volume   =  378,
  number   =  6624,
  pages    = "1092--1097",
  month    =  dec,
  year     =  2022,
  keywords = "Code",
  language = "en",
  issn     = "0036-8075, 1095-9203",
  pmid     = "36480631",
  doi      = "10.1126/science.abq1158"
}

@ARTICLE{Madaan2022-in,
  title         = "Language Models of Code are {Few-Shot} Commonsense Learners",
  author        = "Madaan, Aman and Zhou, Shuyan and Alon, Uri and Yang, Yiming
                   and Neubig, Graham",
  abstract      = "We address the general task of structured commonsense
                   reasoning: given a natural language input, the goal is to
                   generate a graph such as an event -- or a reasoning-graph.
                   To employ large language models (LMs) for this task,
                   existing approaches ``serialize'' the output graph as a flat
                   list of nodes and edges. Although feasible, these serialized
                   graphs strongly deviate from the natural language corpora
                   that LMs were pre-trained on, hindering LMs from generating
                   them correctly. In this paper, we show that when we instead
                   frame structured commonsense reasoning tasks as code
                   generation tasks, pre-trained LMs of code are better
                   structured commonsense reasoners than LMs of natural
                   language, even when the downstream task does not involve
                   source code at all. We demonstrate our approach across three
                   diverse structured commonsense reasoning tasks. In all these
                   natural language tasks, we show that using our approach, a
                   code generation LM (CODEX) outperforms natural-LMs that are
                   fine-tuned on the target task (e.g., T5) and other strong
                   LMs such as GPT-3 in the few-shot setting.",
  month         =  oct,
  year          =  2022,
  keywords      = "Code",
  archivePrefix = "arXiv",
  eprint        = "2210.07128",
  primaryClass  = "cs.CL",
  arxivid       = "2210.07128"
}

@ARTICLE{Ouyang2022-us,
  title         = "Training language models to follow instructions with human
                   feedback",
  author        = "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo
                   and Wainwright, Carroll L and Mishkin, Pamela and Zhang,
                   Chong and Agarwal, Sandhini and Slama, Katarina and Ray,
                   Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser
                   and Miller, Luke and Simens, Maddie and Askell, Amanda and
                   Welinder, Peter and Christiano, Paul and Leike, Jan and
                   Lowe, Ryan",
  abstract      = "Making language models bigger does not inherently make them
                   better at following a user's intent. For example, large
                   language models can generate outputs that are untruthful,
                   toxic, or simply not helpful to the user. In other words,
                   these models are not aligned with their users. In this
                   paper, we show an avenue for aligning language models with
                   user intent on a wide range of tasks by fine-tuning with
                   human feedback. Starting with a set of labeler-written
                   prompts and prompts submitted through the OpenAI API, we
                   collect a dataset of labeler demonstrations of the desired
                   model behavior, which we use to fine-tune GPT-3 using
                   supervised learning. We then collect a dataset of rankings
                   of model outputs, which we use to further fine-tune this
                   supervised model using reinforcement learning from human
                   feedback. We call the resulting models InstructGPT. In human
                   evaluations on our prompt distribution, outputs from the
                   1.3B parameter InstructGPT model are preferred to outputs
                   from the 175B GPT-3, despite having 100x fewer parameters.
                   Moreover, InstructGPT models show improvements in
                   truthfulness and reductions in toxic output generation while
                   having minimal performance regressions on public NLP
                   datasets. Even though InstructGPT still makes simple
                   mistakes, our results show that fine-tuning with human
                   feedback is a promising direction for aligning language
                   models with human intent.",
  month         =  mar,
  year          =  2022,
  keywords      = "Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "2203.02155",
  primaryClass  = "cs.CL",
  arxivid       = "2203.02155"
}

@ARTICLE{Stiennon2020-qn,
  title         = "Learning to summarize from human feedback",
  author        = "Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler,
                   Daniel M and Lowe, Ryan and Voss, Chelsea and Radford, Alec
                   and Amodei, Dario and Christiano, Paul",
  abstract      = "As language models become more powerful, training and
                   evaluation are increasingly bottlenecked by the data and
                   metrics used for a particular task. For example,
                   summarization models are often trained to predict human
                   reference summaries and evaluated using ROUGE, but both of
                   these metrics are rough proxies for what we really care
                   about -- summary quality. In this work, we show that it is
                   possible to significantly improve summary quality by
                   training a model to optimize for human preferences. We
                   collect a large, high-quality dataset of human comparisons
                   between summaries, train a model to predict the
                   human-preferred summary, and use that model as a reward
                   function to fine-tune a summarization policy using
                   reinforcement learning. We apply our method to a version of
                   the TL;DR dataset of Reddit posts and find that our models
                   significantly outperform both human reference summaries and
                   much larger models fine-tuned with supervised learning
                   alone. Our models also transfer to CNN/DM news articles,
                   producing summaries nearly as good as the human reference
                   without any news-specific fine-tuning. We conduct extensive
                   analyses to understand our human feedback dataset and
                   fine-tuned models We establish that our reward model
                   generalizes to new datasets, and that optimizing our reward
                   model results in better summaries than optimizing ROUGE
                   according to humans. We hope the evidence from our paper
                   motivates machine learning researchers to pay closer
                   attention to how their training loss affects the model
                   behavior they actually want.",
  month         =  sep,
  year          =  2020,
  keywords      = "Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "2009.01325",
  primaryClass  = "cs.CL",
  arxivid       = "2009.01325"
}

@ARTICLE{Ziegler2019-jm,
  title         = "{Fine-Tuning} Language Models from Human Preferences",
  author        = "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and
                   Brown, Tom B and Radford, Alec and Amodei, Dario and
                   Christiano, Paul and Irving, Geoffrey",
  abstract      = "Reward learning enables the application of reinforcement
                   learning (RL) to tasks where reward is defined by human
                   judgment, building a model of reward by asking humans
                   questions. Most work on reward learning has used simulated
                   environments, but complex information about values is often
                   expressed in natural language, and we believe reward
                   learning for language is a key to making RL practical and
                   safe for real-world tasks. In this paper, we build on
                   advances in generative pretraining of language models to
                   apply reward learning to four natural language tasks:
                   continuing text with positive sentiment or physically
                   descriptive language, and summarization tasks on the TL;DR
                   and CNN/Daily Mail datasets. For stylistic continuation we
                   achieve good results with only 5,000 comparisons evaluated
                   by humans. For summarization, models trained with 60,000
                   comparisons copy whole sentences from the input but skip
                   irrelevant preamble; this leads to reasonable ROUGE scores
                   and very good performance according to our human labelers,
                   but may be exploiting the fact that labelers rely on simple
                   heuristics.",
  month         =  sep,
  year          =  2019,
  keywords      = "Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "1909.08593",
  primaryClass  = "cs.CL",
  arxivid       = "1909.08593"
}

@ARTICLE{Thoppilan2022-kd,
  title         = "{LaMDA}: Language Models for Dialog Applications",
  author        = "Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and
                   Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze
                   and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu
                   and Li, Yaguang and Lee, Hongrae and Zheng, Huaixiu Steven
                   and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping
                   and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and
                   Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts,
                   Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi
                   and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will
                   and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee
                   and Meier-Hellstern, Kathleen and Morris, Meredith Ringel
                   and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju
                   and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran,
                   Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson,
                   Kristen and Molina, Alejandra and Hoffman-John, Erin and
                   Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna,
                   Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton,
                   Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray
                   and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian
                   and Chi, Ed and Le, Quoc",
  abstract      = "We present LaMDA: Language Models for Dialog Applications.
                   LaMDA is a family of Transformer-based neural language
                   models specialized for dialog, which have up to 137B
                   parameters and are pre-trained on 1.56T words of public
                   dialog data and web text. While model scaling alone can
                   improve quality, it shows less improvements on safety and
                   factual grounding. We demonstrate that fine-tuning with
                   annotated data and enabling the model to consult external
                   knowledge sources can lead to significant improvements
                   towards the two key challenges of safety and factual
                   grounding. The first challenge, safety, involves ensuring
                   that the model's responses are consistent with a set of
                   human values, such as preventing harmful suggestions and
                   unfair bias. We quantify safety using a metric based on an
                   illustrative set of human values, and we find that filtering
                   candidate responses using a LaMDA classifier fine-tuned with
                   a small amount of crowdworker-annotated data offers a
                   promising approach to improving model safety. The second
                   challenge, factual grounding, involves enabling the model to
                   consult external knowledge sources, such as an information
                   retrieval system, a language translator, and a calculator.
                   We quantify factuality using a groundedness metric, and we
                   find that our approach enables the model to generate
                   responses grounded in known sources, rather than responses
                   that merely sound plausible. Finally, we explore the use of
                   LaMDA in the domains of education and content
                   recommendations, and analyze their helpfulness and role
                   consistency.",
  month         =  jan,
  year          =  2022,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2201.08239",
  primaryClass  = "cs.CL",
  arxivid       = "2201.08239"
}

@ARTICLE{Madaan2022-pk,
  title         = "Memory-assisted prompt editing to improve {GPT-3} after
                   deployment",
  author        = "Madaan, Aman and Tandon, Niket and Clark, Peter and Yang,
                   Yiming",
  abstract      = "Large LMs such as GPT-3 are powerful, but can commit
                   mistakes that are obvious to humans. For example, GPT-3
                   would mistakenly interpret ``What word is similar to good?''
                   to mean a homophone, while the user intended a synonym. Our
                   goal is to effectively correct such errors via user
                   interactions with the system but without retraining, which
                   will be prohibitively costly. We pair GPT-3 with a growing
                   memory of recorded cases where the model misunderstood the
                   user's intents, along with user feedback for clarification.
                   Such a memory allows our system to produce enhanced prompts
                   for any new query based on the user feedback for error
                   correction on similar cases in the past. On four tasks (two
                   lexical tasks, two advanced ethical reasoning tasks), we
                   show how a (simulated) user can interactively teach a
                   deployed GPT-3, substantially increasing its accuracy over
                   the queries with different kinds of misunderstandings by the
                   GPT-3. Our approach is a step towards the low-cost utility
                   enhancement for very large pre-trained LMs. Code, data, and
                   instructions to implement MEMPROMPT for a new task at
                   https://www.memprompt.com/.",
  month         =  jan,
  year          =  2022,
  keywords      = "In-Context",
  archivePrefix = "arXiv",
  eprint        = "2201.06009",
  primaryClass  = "cs.CL",
  arxivid       = "2201.06009"
}

@ARTICLE{Zhou2023-zc,
  title         = "What Algorithms can Transformers Learn? A Study in Length
                   Generalization",
  author        = "Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin,
                   Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy
                   and Nakkiran, Preetum",
  abstract      = "Large language models exhibit surprising emergent
                   generalization properties, yet also struggle on many simple
                   reasoning tasks such as arithmetic and parity. This raises
                   the question of if and when Transformer models can learn the
                   true algorithm for solving a task. We study the scope of
                   Transformers' abilities in the specific setting of length
                   generalization on algorithmic tasks. Here, we propose a
                   unifying framework to understand when and how Transformers
                   can exhibit strong length generalization on a given task.
                   Specifically, we leverage RASP (Weiss et al., 2021) -- a
                   programming language designed for the computational model of
                   a Transformer -- and introduce the RASP-Generalization
                   Conjecture: Transformers tend to length generalize on a task
                   if the task can be solved by a short RASP program which
                   works for all input lengths. This simple conjecture
                   remarkably captures most known instances of length
                   generalization on algorithmic tasks. Moreover, we leverage
                   our insights to drastically improve generalization
                   performance on traditionally hard tasks (such as parity and
                   addition). On the theoretical side, we give a simple example
                   where the ``min-degree-interpolator'' model of learning from
                   Abbe et al. (2023) does not correctly predict Transformers'
                   out-of-distribution behavior, but our conjecture does.
                   Overall, our work provides a novel perspective on the
                   mechanisms of compositional generalization and the
                   algorithmic capabilities of Transformers.",
  month         =  oct,
  year          =  2023,
  keywords      = "Understanding",
  archivePrefix = "arXiv",
  eprint        = "2310.16028",
  primaryClass  = "cs.LG",
  arxivid       = "2310.16028"
}

@ARTICLE{Bottou2023-ok,
  title         = "Borges and {AI}",
  author        = "Bottou, L{\'e}on and Sch{\"o}lkopf, Bernhardt",
  abstract      = "Many believe that Large Language Models (LLMs) open the era
                   of Artificial Intelligence (AI). Some see opportunities
                   while others see dangers. Yet both proponents and opponents
                   grasp AI through the imagery popularised by science fiction.
                   Will the machine become sentient and rebel against its
                   creators? Will we experience a paperclip apocalypse? Before
                   answering such questions, we should first ask whether this
                   mental imagery provides a good description of the phenomenon
                   at hand. Understanding weather patterns through the moods of
                   the gods only goes so far. The present paper instead
                   advocates understanding LLMs and their connection to AI
                   through the imagery of Jorge Luis Borges, a master of 20th
                   century literature, forerunner of magical realism, and
                   precursor to postmodern literature. This exercise leads to a
                   new perspective that illuminates the relation between
                   language modelling and artificial intelligence.",
  month         =  sep,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2310.01425",
  primaryClass  = "cs.CL",
  arxivid       = "2310.01425"
}

@ARTICLE{Kirchenbauer2023-xn,
  title         = "On the Reliability of Watermarks for Large Language Models",
  author        = "Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and
                   Shu, Manli and Saifullah, Khalid and Kong, Kezhi and
                   Fernando, Kasun and Saha, Aniruddha and Goldblum, Micah and
                   Goldstein, Tom",
  abstract      = "As LLMs become commonplace, machine-generated text has the
                   potential to flood the internet with spam, social media
                   bots, and valueless content. Watermarking is a simple and
                   effective strategy for mitigating such harms by enabling the
                   detection and documentation of LLM-generated text. Yet a
                   crucial question remains: How reliable is watermarking in
                   realistic settings in the wild? There, watermarked text may
                   be modified to suit a user's needs, or entirely rewritten to
                   avoid detection. We study the robustness of watermarked text
                   after it is re-written by humans, paraphrased by a
                   non-watermarked LLM, or mixed into a longer hand-written
                   document. We find that watermarks remain detectable even
                   after human and machine paraphrasing. While these attacks
                   dilute the strength of the watermark, paraphrases are
                   statistically likely to leak n-grams or even longer
                   fragments of the original text, resulting in high-confidence
                   detections when enough tokens are observed. For example,
                   after strong human paraphrasing the watermark is detectable
                   after observing 800 tokens on average, when setting a 1e-5
                   false positive rate. We also consider a range of new
                   detection schemes that are sensitive to short spans of
                   watermarked text embedded inside a large document, and we
                   compare the robustness of watermarking to other kinds of
                   detectors.",
  month         =  jun,
  year          =  2023,
  keywords      = "Watermarking",
  archivePrefix = "arXiv",
  eprint        = "2306.04634",
  primaryClass  = "cs.LG",
  arxivid       = "2306.04634"
}

@ARTICLE{Liu2021-oz,
  title         = "{P-Tuning} v2: Prompt Tuning Can Be Comparable to
                   Fine-tuning Universally Across Scales and Tasks",
  author        = "Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam
                   and Du, Zhengxiao and Yang, Zhilin and Tang, Jie",
  abstract      = "Prompt tuning, which only tunes continuous prompts with a
                   frozen language model, substantially reduces per-task
                   storage and memory usage at training. However, in the
                   context of NLU, prior work reveals that prompt tuning does
                   not perform well for normal-sized pretrained models. We also
                   find that existing methods of prompt tuning cannot handle
                   hard sequence labeling tasks, indicating a lack of
                   universality. We present a novel empirical finding that
                   properly optimized prompt tuning can be universally
                   effective across a wide range of model scales and NLU tasks.
                   It matches the performance of finetuning while having only
                   0.1\%-3\% tuned parameters. Our method P-Tuning v2 is an
                   implementation of Deep Prompt Tuning
                   \textbackslashcite\{li2021prefix,qin2021learning\} optimized
                   and adapted for NLU. Given the universality and simplicity
                   of P-Tuning v2, we believe it can serve as an alternative to
                   finetuning and a strong baseline for future research.Our
                   code and data are released at
                   https://github.com/THUDM/P-tuning-v2.",
  month         =  oct,
  year          =  2021,
  keywords      = "Efficient;Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "2110.07602",
  primaryClass  = "cs.CL",
  arxivid       = "2110.07602"
}

@ARTICLE{Tian2023-yc,
  title         = "Fine-tuning Language Models for Factuality",
  author        = "Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and
                   Manning, Christopher D and Finn, Chelsea",
  abstract      = "The fluency and creativity of large pre-trained language
                   models (LLMs) have led to their widespread use, sometimes
                   even as a replacement for traditional search engines. Yet
                   language models are prone to making convincing but factually
                   inaccurate claims, often referred to as 'hallucinations.'
                   These errors can inadvertently spread misinformation or
                   harmfully perpetuate misconceptions. Further, manual
                   fact-checking of model responses is a time-consuming
                   process, making human factuality labels expensive to
                   acquire. In this work, we fine-tune language models to be
                   more factual, without human labeling and targeting more
                   open-ended generation settings than past work. We leverage
                   two key recent innovations in NLP to do so. First, several
                   recent works have proposed methods for judging the
                   factuality of open-ended text by measuring consistency with
                   an external knowledge base or simply a large model's
                   confidence scores. Second, the direct preference
                   optimization algorithm enables straightforward fine-tuning
                   of language models on objectives other than supervised
                   imitation, using a preference ranking over possible model
                   responses. We show that learning from automatically
                   generated factuality preference rankings, generated either
                   through existing retrieval systems or our novel
                   retrieval-free approach, significantly improves the
                   factuality (percent of generated claims that are correct) of
                   Llama-2 on held-out topics compared with RLHF or decoding
                   strategies targeted at factuality. At 7B scale, compared to
                   Llama-2-chat, we observe 58\% and 40\% reduction in factual
                   error rate when generating biographies and answering medical
                   questions, respectively.",
  month         =  nov,
  year          =  2023,
  keywords      = "Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "2311.08401",
  primaryClass  = "cs.CL",
  arxivid       = "2311.08401"
}

@MISC{Pratap_undated-ck,
  title       = "No title",
  author      = "Pratap, Vineel and Tjandra, Andros and Shi, Bowen and Kundu,
                 Paden Tomasello Arun Babu and Elkahky, Ali and Adi, Yossi and
                 Conneau, Xiaohui Zhang Wei-Ning Hsu and Auli, Michael",
  institution = "Github",
  keywords    = "Speech"
}

@ARTICLE{Qi2023-mp,
  title         = "The Art of {SOCRATIC} {QUESTIONING}: Recursive Thinking with
                   Large Language Models",
  author        = "Qi, Jingyuan and Xu, Zhiyang and Shen, Ying and Liu, Minqian
                   and Jin, Di and Wang, Qifan and Huang, Lifu",
  abstract      = "Chain-of-Thought (CoT) prompting enables large language
                   models to solve complex reasoning problems by generating
                   intermediate steps. However, confined by its inherent
                   single-pass and sequential generation process, CoT heavily
                   relies on the initial decisions, causing errors in early
                   steps to accumulate and impact the final answers. In
                   contrast, humans adopt recursive thinking when tackling
                   complex reasoning problems, i.e., iteratively breaking the
                   original problem into approachable sub-problems and
                   aggregating their answers to resolve the original one.
                   Inspired by the human cognitive process, we propose SOCRATIC
                   QUESTIONING, a divide-and-conquer style algorithm that
                   mimics the recursive thinking process. Specifically,
                   SOCRATIC QUESTIONING leverages large language models to
                   raise and answer sub-questions until collecting enough
                   information to tackle the original question. Unlike CoT,
                   SOCRATIC QUESTIONING explicitly navigates the thinking
                   space, stimulates effective recursive thinking, and is more
                   robust towards errors in the thinking process. Extensive
                   experiments on several complex reasoning tasks, including
                   MMLU, MATH, LogiQA, and visual question-answering
                   demonstrate significant performance improvements over the
                   state-of-the-art prompting methods, such as CoT, and
                   Tree-of-Thought. The qualitative analysis clearly shows that
                   the intermediate reasoning steps elicited by SOCRATIC
                   QUESTIONING are similar to humans' recursively thinking
                   process of complex reasoning problems.",
  month         =  may,
  year          =  2023,
  keywords      = "In-Context;Models",
  archivePrefix = "arXiv",
  eprint        = "2305.14999",
  primaryClass  = "cs.CL",
  arxivid       = "2305.14999"
}

@ARTICLE{Ortega_undated-vg,
  title    = "Shaking the foundations: delusions in sequence models for
              interaction and control",
  author   = "Ortega, Pedro A and Kunesch, Markus and Del{\'e}tang,
              Gr{\'e}goire and Genewein, Tim and Grau-Moya, Jordi and Veness,
              Joel and Buchli, Jonas and Degrave, Jonas and Piot, Bilal and
              Perolat, Julien and Everitt, Tom and Tallec, Corentin and
              Parisotto, Emilio and Erez, Tom and Chen, Yutian and Reed, Scott
              and Hutter, Marcus and De Freitas, Nando and Legg, Shane",
  keywords = "Causal Models;LLMs",
  arxivid  = "2110.10819v1"
}

@ARTICLE{Zhang2023-lv,
  title         = "Siren's Song in the {AI} Ocean: A Survey on Hallucination in
                   Large Language Models",
  author        = "Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and
                   Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao,
                   Enbo and Zhang, Yu and Chen, Yulong and Wang, Longyue and
                   Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming",
  abstract      = "While large language models (LLMs) have demonstrated
                   remarkable capabilities across a range of downstream tasks,
                   a significant concern revolves around their propensity to
                   exhibit hallucinations: LLMs occasionally generate content
                   that diverges from the user input, contradicts previously
                   generated context, or misaligns with established world
                   knowledge. This phenomenon poses a substantial challenge to
                   the reliability of LLMs in real-world scenarios. In this
                   paper, we survey recent efforts on the detection,
                   explanation, and mitigation of hallucination, with an
                   emphasis on the unique challenges posed by LLMs. We present
                   taxonomies of the LLM hallucination phenomena and evaluation
                   benchmarks, analyze existing approaches aiming at mitigating
                   LLM hallucination, and discuss potential directions for
                   future research.",
  month         =  sep,
  year          =  2023,
  keywords      = "Hallucination",
  archivePrefix = "arXiv",
  eprint        = "2309.01219",
  primaryClass  = "cs.CL",
  arxivid       = "2309.01219"
}

@ARTICLE{Li2023-yz,
  title         = "{HaluEval}: A {Large-Scale} Hallucination Evaluation
                   Benchmark for Large Language Models",
  author        = "Li, Junyi and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie,
                   Jian-Yun and Wen, Ji-Rong",
  abstract      = "Large language models (LLMs), such as ChatGPT, are prone to
                   generate hallucinations, i.e., content that conflicts with
                   the source or cannot be verified by the factual knowledge.
                   To understand what types of content and to which extent LLMs
                   are apt to hallucinate, we introduce the Hallucination
                   Evaluation benchmark for Large Language Models (HaluEval), a
                   large collection of generated and human-annotated
                   hallucinated samples for evaluating the performance of LLMs
                   in recognizing hallucination. To generate these samples, we
                   propose a ChatGPT-based two-step framework, i.e.,
                   sampling-then-filtering. Besides, we also hire some human
                   labelers to annotate the hallucinations in ChatGPT
                   responses. The empirical results suggest that ChatGPT is
                   likely to generate hallucinated content in specific topics
                   by fabricating unverifiable information (i.e., about
                   $19.5\%$ responses). Moreover, existing LLMs face great
                   challenges in recognizing the hallucinations in texts.
                   However, our experiments also prove that providing external
                   knowledge or adding reasoning steps can help LLMs recognize
                   hallucinations. Our benchmark can be accessed at
                   https://github.com/RUCAIBox/HaluEval.",
  month         =  may,
  year          =  2023,
  keywords      = "Evaluation;Hallucination",
  archivePrefix = "arXiv",
  eprint        = "2305.11747",
  primaryClass  = "cs.CL",
  arxivid       = "2305.11747"
}

@MISC{Slack_undated-jf,
  title        = "A holistic approach for test and evaluation of large language
                  models",
  author       = "Slack, Dylan and Wang, Jean and Semenenko, Denis and Park,
                  Kate and Berrios, Daniel and Hendryx, Sean",
  howpublished = "\url{https://static.scale.com/uploads/6019a18f03a4ae003acb1113/test-and-evaluation.pdf}",
  note         = "Accessed: 2023-12-4",
  keywords     = "Evaluation"
}

@MISC{Girdhar_undated-it,
  title        = "Factorizing text-to-video generation by explicit image
                  conditioning",
  author       = "Girdhar, Rohit and Singh, Mannat and Brown, Andrew and Duval,
                  Quentin and Azadi, Samaneh and Saketh, Sai and Akbar,
                  Rambhatla and Yin, Shah Xi and Parikh, Devi and Misra, Ishan
                  and Genai, Meta",
  howpublished = "\url{https://emu-video.metademolab.com/assets/emu_video.pdf}",
  note         = "Accessed: 2023-11-17",
  keywords     = "Multimodal (Vision, speech, etc)"
}

@MISC{Sheynin_undated-pk,
  title        = "Emu edit: Precise image editing via recognition and
                  generation tasks",
  author       = "Sheynin, Shelly and Polyak, Adam and Singer, Uriel and
                  Kirstain, Yuval and Zohar, Amit and Ashual, Oron and Parikh,
                  Devi and Yaniv, Taigman and Genai, Meta",
  howpublished = "\url{https://emu-edit.metademolab.com/assets/emu_edit.pdf}",
  note         = "Accessed: 2023-11-17",
  keywords     = "Multimodal (Vision, speech, etc)"
}

@MISC{Hagen_undated-ub,
  title        = "{DeepSpeed}: Extreme-scale model training for everyone",
  author       = "Hagen, Alexis",
  howpublished = "\url{https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/}",
  note         = "Accessed: 2023-12-5",
  keywords     = "Scaling"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Lemley2021-ev,
  title        = "Fair Learning",
  booktitle    = "Texas Law Review",
  author       = "Lemley, Mark A and Casey, Bryan",
  abstract     = "Introduction The challenge handed to the musician was
                  peculiar and daunting: Take a five-second sample of a
                  randomly selected song and, with just a moment's notice,
                  transform it into a full-length piece composed in the style
                  of a completely different artist.[1] On this occasion, the
                  musician rose to the challenge with such aplomb that it […]",
  month        =  mar,
  year         =  2021,
  howpublished = "\url{https://texaslawreview.org/fair-learning/}",
  note         = "Accessed: 2023-12-5",
  keywords     = "Fairness, Bias, Toxicity",
  language     = "en"
}

@ARTICLE{Sheng2023-qk,
  title         = "{S-LoRA}: Serving Thousands of Concurrent {LoRA} Adapters",
  author        = "Sheng, Ying and Cao, Shiyi and Li, Dacheng and Hooper,
                   Coleman and Lee, Nicholas and Yang, Shuo and Chou,
                   Christopher and Zhu, Banghua and Zheng, Lianmin and Keutzer,
                   Kurt and Gonzalez, Joseph E and Stoica, Ion",
  abstract      = "The ``pretrain-then-finetune'' paradigm is commonly adopted
                   in the deployment of large language models. Low-Rank
                   Adaptation (LoRA), a parameter-efficient fine-tuning method,
                   is often employed to adapt a base model to a multitude of
                   tasks, resulting in a substantial collection of LoRA
                   adapters derived from one base model. We observe that this
                   paradigm presents significant opportunities for batched
                   inference during serving. To capitalize on these
                   opportunities, we present S-LoRA, a system designed for the
                   scalable serving of many LoRA adapters. S-LoRA stores all
                   adapters in the main memory and fetches the adapters used by
                   the currently running queries to the GPU memory. To
                   efficiently use the GPU memory and reduce fragmentation,
                   S-LoRA proposes Unified Paging. Unified Paging uses a
                   unified memory pool to manage dynamic adapter weights with
                   different ranks and KV cache tensors with varying sequence
                   lengths. Additionally, S-LoRA employs a novel tensor
                   parallelism strategy and highly optimized custom CUDA
                   kernels for heterogeneous batching of LoRA computation.
                   Collectively, these features enable S-LoRA to serve
                   thousands of LoRA adapters on a single GPU or across
                   multiple GPUs with a small overhead. Compared to
                   state-of-the-art libraries such as HuggingFace PEFT and vLLM
                   (with naive support of LoRA serving), S-LoRA can improve the
                   throughput by up to 4 times and increase the number of
                   served adapters by several orders of magnitude. As a result,
                   S-LoRA enables scalable serving of many task-specific
                   fine-tuned models and offers the potential for large-scale
                   customized fine-tuning services. The code is available at
                   https://github.com/S-LoRA/S-LoRA",
  month         =  nov,
  year          =  2023,
  keywords      = "Efficient",
  archivePrefix = "arXiv",
  eprint        = "2311.03285",
  primaryClass  = "cs.LG",
  arxivid       = "2311.03285"
}

@ARTICLE{Wang2023-ya,
  title         = "{JARVIS-1}: {Open-World} Multi-task Agents with
                   {Memory-Augmented} Multimodal Language Models",
  author        = "Wang, Zihao and Cai, Shaofei and Liu, Anji and Jin, Yonggang
                   and Hou, Jinbing and Zhang, Bowei and Lin, Haowei and He,
                   Zhaofeng and Zheng, Zilong and Yang, Yaodong and Ma,
                   Xiaojian and Liang, Yitao",
  abstract      = "Achieving human-like planning and control with multimodal
                   observations in an open world is a key milestone for more
                   functional generalist agents. Existing approaches can handle
                   certain long-horizon tasks in an open world. However, they
                   still struggle when the number of open-world tasks could
                   potentially be infinite and lack the capability to
                   progressively enhance task completion as game time
                   progresses. We introduce JARVIS-1, an open-world agent that
                   can perceive multimodal input (visual observations and human
                   instructions), generate sophisticated plans, and perform
                   embodied control, all within the popular yet challenging
                   open-world Minecraft universe. Specifically, we develop
                   JARVIS-1 on top of pre-trained multimodal language models,
                   which map visual observations and textual instructions to
                   plans. The plans will be ultimately dispatched to the
                   goal-conditioned controllers. We outfit JARVIS-1 with a
                   multimodal memory, which facilitates planning using both
                   pre-trained knowledge and its actual game survival
                   experiences. In our experiments, JARVIS-1 exhibits nearly
                   perfect performances across over 200 varying tasks from the
                   Minecraft Universe Benchmark, ranging from entry to
                   intermediate levels. JARVIS-1 has achieved a completion rate
                   of 12.5\% in the long-horizon diamond pickaxe task. This
                   represents a significant increase up to 5 times compared to
                   previous records. Furthermore, we show that JARVIS-1 is able
                   to $\textit\{self-improve\}$ following a life-long learning
                   paradigm thanks to multimodal memory, sparking a more
                   general intelligence and improved autonomy. The project page
                   is available at https://craftjarvis-jarvis1.github.io.",
  month         =  nov,
  year          =  2023,
  keywords      = "RAG",
  archivePrefix = "arXiv",
  eprint        = "2311.05997",
  primaryClass  = "cs.AI",
  arxivid       = "2311.05997"
}

@ARTICLE{Jin2023-zq,
  title         = "Large language models on graphs: A comprehensive survey",
  author        = "Jin, Bowen and Liu, Gang and Han, Chi and Jiang, Meng and
                   Ji, Heng and Han, Jiawei",
  abstract      = "Large language models (LLMs), such as ChatGPT and LLaMA, are
                   creating significant advancements in natural language
                   processing, due to their strong text encoding/decoding
                   ability and newly found emergent capability (e.g.,
                   reasoning). While LLMs are mainly designed to process pure
                   texts, there are many real-world scenarios where text data
                   are associated with rich structure information in the form
                   of graphs (e.g., academic networks, and e-commerce networks)
                   or scenarios where graph data are paired with rich textual
                   information (e.g., molecules with descriptions). Besides,
                   although LLMs have shown their pure text-based reasoning
                   ability, it is underexplored whether such ability can be
                   generalized to graph scenarios (i.e., graph-based
                   reasoning). In this paper, we provide a systematic review of
                   scenarios and techniques related to large language models on
                   graphs. We first summarize potential scenarios of adopting
                   LLMs on graphs into three categories, namely pure graphs,
                   text-rich graphs, and text-paired graphs. We then discuss
                   detailed techniques for utilizing LLMs on graphs, including
                   LLM as Predictor, LLM as Encoder, and LLM as Aligner, and
                   compare the advantages and disadvantages of different
                   schools of models. Furthermore, we mention the real-world
                   applications of such methods and summarize open-source codes
                   and benchmark datasets. Finally, we conclude with potential
                   future research directions in this fast-growing field. The
                   related source can be found at
                   https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.",
  month         =  dec,
  year          =  2023,
  keywords      = "Survey/Review Paper;Survey",
  archivePrefix = "arXiv",
  eprint        = "2312.02783",
  primaryClass  = "cs.CL",
  arxivid       = "2312.02783"
}

@MISC{Inan_undated-ja,
  title    = "Llama Guard: {LLM-based} {Input-Output} Safeguard for {Human-AI}
              Conversations",
  author   = "Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta,
              Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael
              and Hu, Qing and Fuller, Brian and Testuggine, Davide and Khabsa,
              Madian",
  keywords = "Safety/Risks"
}

@MISC{Communication_undated-wb,
  title    = "Multilingual Expressive and Streaming Speech Translation",
  author   = "Communication, Seamless and Barrault, Lo{\"\i}c and Chung, Yu-An
              and Meglioli, Mariano Coria and Dale, David and Dong, Ning and
              Duppenthaler, Mark and Duquenne, Paul-Ambroise and Ellis, Brian
              and Elsahar, Hady and Haaheim, Justin and Hoffman, John and
              Hwang, Min-Jae and Inaguma, Hirofumi and Klaiber, Christopher and
              Kulikov, Ilia and Li, Pengwei and Licht, Daniel and Maillard,
              Jean and Mavlyutov, Ruslan and Rakotoarison, Alice and Kaushik,
              Ram and Ramakrishnan, Abinesh and Tran, Tuan and Wenzek,
              Guillaume and Yang, Yilin and Ye, Ethan and Evtimov, Ivan and
              Fernandez, Pierre and Gao, Cynthia and Hansanti, Prangthip and
              Kalbassi, Elahe and Kallet, Amanda and Kozhevnikov, Artyom and
              Mejia Gonzalez, Gabriel and San, Robin and Touret, Christophe and
              Wong, Corinne and Wood, Carleigh and Yu, Bokai and Andrews,
              Pierre and Balioglu, Can and Chen, Peng-Jen and Costa-Juss{\`a},
              Marta R and Elbayad, Maha and Gong, Hongyu and Guzm{\'a}n,
              Francisco and Heffernan, Kevin and Jain, Somya and Kao, Justine
              and Lee, Ann and Ma, Xutai and Mourachko, Alex and Peloquin,
              Benjamin and Pino, Juan and Popuri, Sravya and Ropers, Christophe
              and Saleem, Safiyyah and Schwenk, Holger and Sun, Anna and
              Tomasello, Paden and Wang, Changhan and Wang, Jeff and Wang,
              Skyler and Williamson, Mary and Berkeley, U C",
  keywords = "Multimodal (Vision, speech, etc)"
}

@ARTICLE{Wang2024-lq,
  title         = "{MambaByte}: Token-free selective state space model",
  author        = "Wang, Junxiong and Gangavarapu, Tushaar and Yan, Jing Nathan
                   and Rush, Alexander M",
  abstract      = "Token-free language models learn directly from raw bytes and
                   remove the bias of subword tokenization. Operating on bytes,
                   however, results in significantly longer sequences, and
                   standard autoregressive Transformers scale poorly in such
                   settings. We experiment with MambaByte, a token-free
                   adaptation of the Mamba state space model, trained
                   autoregressively on byte sequences. Our experiments indicate
                   the computational efficiency of MambaByte compared to other
                   byte-level models. We also find MambaByte to be competitive
                   with and even outperform state-of-the-art subword
                   Transformers. Furthermore, owing to linear scaling in
                   length, MambaByte benefits from fast inference compared to
                   Transformers. Our findings establish the viability of
                   MambaByte in enabling token-free language modeling.",
  month         =  jan,
  year          =  2024,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2401.13660",
  primaryClass  = "cs.CL",
  arxivid       = "2401.13660"
}

@ARTICLE{Hubinger2024-bg,
  title         = "Sleeper agents: Training deceptive {LLMs} that persist
                   through safety training",
  author        = "Hubinger, Evan and Denison, Carson and Mu, Jesse and
                   Lambert, Mike and Tong, Meg and MacDiarmid, Monte and
                   Lanham, Tamera and Ziegler, Daniel M and Maxwell, Tim and
                   Cheng, Newton and Jermyn, Adam and Askell, Amanda and
                   Radhakrishnan, Ansh and Anil, Cem and Duvenaud, David and
                   Ganguli, Deep and Barez, Fazl and Clark, Jack and Ndousse,
                   Kamal and Sachan, Kshitij and Sellitto, Michael and Sharma,
                   Mrinank and DasSarma, Nova and Grosse, Roger and Kravec,
                   Shauna and Bai, Yuntao and Witten, Zachary and Favaro,
                   Marina and Brauner, Jan and Karnofsky, Holden and
                   Christiano, Paul and Bowman, Samuel R and Graham, Logan and
                   Kaplan, Jared and Mindermann, S{\"o}ren and Greenblatt, Ryan
                   and Shlegeris, Buck and Schiefer, Nicholas and Perez, Ethan",
  abstract      = "Humans are capable of strategically deceptive behavior:
                   behaving helpfully in most situations, but then behaving
                   very differently in order to pursue alternative objectives
                   when given the opportunity. If an AI system learned such a
                   deceptive strategy, could we detect it and remove it using
                   current state-of-the-art safety training techniques? To
                   study this question, we construct proof-of-concept examples
                   of deceptive behavior in large language models (LLMs). For
                   example, we train models that write secure code when the
                   prompt states that the year is 2023, but insert exploitable
                   code when the stated year is 2024. We find that such
                   backdoored behavior can be made persistent, so that it is
                   not removed by standard safety training techniques,
                   including supervised fine-tuning, reinforcement learning,
                   and adversarial training (eliciting unsafe behavior and then
                   training to remove it). The backdoored behavior is most
                   persistent in the largest models and in models trained to
                   produce chain-of-thought reasoning about deceiving the
                   training process, with the persistence remaining even when
                   the chain-of-thought is distilled away. Furthermore, rather
                   than removing backdoors, we find that adversarial training
                   can teach models to better recognize their backdoor
                   triggers, effectively hiding the unsafe behavior. Our
                   results suggest that, once a model exhibits deceptive
                   behavior, standard techniques could fail to remove such
                   deception and create a false impression of safety.",
  month         =  jan,
  year          =  2024,
  keywords      = "Jailbreak",
  archivePrefix = "arXiv",
  eprint        = "2401.05566",
  primaryClass  = "cs.CR",
  arxivid       = "2401.05566"
}

@ARTICLE{Jiang2024-jx,
  title         = "Mixtral of Experts",
  author        = "Jiang, Albert Q and Sablayrolles, Alexandre and Roux,
                   Antoine and Mensch, Arthur and Savary, Blanche and Bamford,
                   Chris and Chaplot, Devendra Singh and Casas, Diego de las
                   and Hanna, Emma Bou and Bressand, Florian and Lengyel,
                   Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud,
                   L{\'e}lio Renard and Saulnier, Lucile and Lachaux,
                   Marie-Anne and Stock, Pierre and Subramanian, Sandeep and
                   Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and
                   Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas
                   and Lacroix, Timoth{\'e}e and Sayed, William El",
  abstract      = "We introduce Mixtral 8x7B, a Sparse Mixture of Experts
                   (SMoE) language model. Mixtral has the same architecture as
                   Mistral 7B, with the difference that each layer is composed
                   of 8 feedforward blocks (i.e. experts). For every token, at
                   each layer, a router network selects two experts to process
                   the current state and combine their outputs. Even though
                   each token only sees two experts, the selected experts can
                   be different at each timestep. As a result, each token has
                   access to 47B parameters, but only uses 13B active
                   parameters during inference. Mixtral was trained with a
                   context size of 32k tokens and it outperforms or matches
                   Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In
                   particular, Mixtral vastly outperforms Llama 2 70B on
                   mathematics, code generation, and multilingual benchmarks.
                   We also provide a model fine-tuned to follow instructions,
                   Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,
                   Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on
                   human benchmarks. Both the base and instruct models are
                   released under the Apache 2.0 license.",
  month         =  jan,
  year          =  2024,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2401.04088",
  primaryClass  = "cs.LG",
  arxivid       = "2401.04088"
}

@ARTICLE{Zeng2024-mn,
  title         = "How Johnny can persuade {LLMs} to jailbreak them: Rethinking
                   persuasion to challenge {AI} safety by humanizing {LLMs}",
  author        = "Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi
                   and Jia, Ruoxi and Shi, Weiyan",
  abstract      = "Most traditional AI safety research has approached AI models
                   as machines and centered on algorithm-focused attacks
                   developed by security experts. As large language models
                   (LLMs) become increasingly common and competent, non-expert
                   users can also impose risks during daily interactions. This
                   paper introduces a new perspective to jailbreak LLMs as
                   human-like communicators, to explore this overlooked
                   intersection between everyday language interaction and AI
                   safety. Specifically, we study how to persuade LLMs to
                   jailbreak them. First, we propose a persuasion taxonomy
                   derived from decades of social science research. Then, we
                   apply the taxonomy to automatically generate interpretable
                   persuasive adversarial prompts (PAP) to jailbreak LLMs.
                   Results show that persuasion significantly increases the
                   jailbreak performance across all risk categories: PAP
                   consistently achieves an attack success rate of over $92\%$
                   on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials,
                   surpassing recent algorithm-focused attacks. On the defense
                   side, we explore various mechanisms against PAP and, found a
                   significant gap in existing defenses, and advocate for more
                   fundamental mitigation for highly interactive LLMs",
  month         =  jan,
  year          =  2024,
  keywords      = "In-Context",
  archivePrefix = "arXiv",
  eprint        = "2401.06373",
  primaryClass  = "cs.CL",
  arxivid       = "2401.06373"
}

@ARTICLE{Aggarwal2024-on,
  title         = "{MAPLE}: Multilingual evaluation of parameter efficient
                   finetuning of Large Language Models",
  author        = "Aggarwal, Divyanshu and Sathe, Ashutosh and Sitaram,
                   Sunayana",
  abstract      = "Parameter efficient finetuning has emerged as a viable
                   solution for improving the performance of Large Language
                   Models without requiring massive resources and compute.
                   Prior work on multilingual evaluation has shown that there
                   is a large gap between the performance of LLMs on English
                   and other languages. Further, there is also a large gap
                   between the performance of smaller open-source models and
                   larger LLMs. Finetuning can be an effective way to bridge
                   this gap and make language models more equitable. In this
                   work, we finetune the LLaMA-7B and Mistral-7B models on
                   synthetic multilingual instruction tuning data to determine
                   its effect on model performance on five downstream tasks
                   covering twenty three languages in all. Additionally, we
                   experiment with various parameters, such as rank for
                   low-rank adaptation and values of quantisation to determine
                   their effects on downstream performance and find that higher
                   rank and higher quantisation values benefit low-resource
                   languages. We find that parameter efficient finetuning of
                   smaller open source models sometimes bridges the gap between
                   the performance of these models and the larger ones,
                   however, English performance can take a hit. We also find
                   that finetuning sometimes improves performance on
                   low-resource languages, while degrading performance on
                   high-resource languages.",
  month         =  jan,
  year          =  2024,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2401.07598",
  primaryClass  = "cs.CL",
  arxivid       = "2401.07598"
}

@ARTICLE{Trinh2024-ul,
  title     = "Solving olympiad geometry without human demonstrations",
  author    = "Trinh, Trieu H and Wu, Yuhuai and Le, Quoc V and He, He and
               Luong, Thang",
  abstract  = "Proving mathematical theorems at the olympiad level represents a
               notable milestone in human-level automated reasoning1--4, owing
               to their reputed difficulty among the world's best talents in
               pre-university mathematics. Current machine-learning approaches,
               however, are not applicable to most mathematical domains owing
               to the high cost of translating human proofs into
               machine-verifiable format. The problem is even worse for
               geometry because of its unique translation challenges1,5,
               resulting in severe scarcity of training data. We propose
               AlphaGeometry, a theorem prover for Euclidean plane geometry
               that sidesteps the need for human demonstrations by synthesizing
               millions of theorems and proofs across different levels of
               complexity. AlphaGeometry is a neuro-symbolic system that uses a
               neural language model, trained from scratch on our large-scale
               synthetic data, to guide a symbolic deduction engine through
               infinite branching points in challenging problems. On a test set
               of 30 latest olympiad-level problems, AlphaGeometry solves 25,
               outperforming the previous best method that only solves ten
               problems and approaching the performance of an average
               International Mathematical Olympiad (IMO) gold medallist.
               Notably, AlphaGeometry produces human-readable proofs, solves
               all geometry problems in the IMO 2000 and 2015 under human
               expert evaluation and discovers a generalized version of a
               translated IMO theorem in 2004. A new neuro-symbolic theorem
               prover for Euclidean plane geometry trained from scratch on
               millions of synthesized theorems and proofs outperforms the
               previous best method and reaches the performance of an olympiad
               gold medallist.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  625,
  number    =  7995,
  pages     = "476--482",
  month     =  jan,
  year      =  2024,
  keywords  = "Models",
  language  = "en",
  issn      = "0028-0836",
  doi       = "10.1038/s41586-023-06747-5"
}

@ARTICLE{Balaguer2024-rr,
  title         = "{RAG} vs fine-tuning: Pipelines, tradeoffs, and a case study
                   on agriculture",
  author        = "Balaguer, Angels and Benara, Vinamra and Cunha, Renato Luiz
                   de Freitas and Filho, Roberto de M Estev{\~a}o and Hendry,
                   Todd and Holstein, Daniel and Marsman, Jennifer and
                   Mecklenburg, Nick and Malvar, Sara and Nunes, Leonardo O and
                   Padilha, Rafael and Sharp, Morris and Silva, Bruno and
                   Sharma, Swati and Aski, Vijay and Chandra, Ranveer",
  abstract      = "There are two common ways in which developers are
                   incorporating proprietary and domain-specific data when
                   building applications of Large Language Models (LLMs):
                   Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG
                   augments the prompt with the external data, while
                   fine-Tuning incorporates the additional knowledge into the
                   model itself. However, the pros and cons of both approaches
                   are not well understood. In this paper, we propose a
                   pipeline for fine-tuning and RAG, and present the tradeoffs
                   of both for multiple popular LLMs, including Llama2-13B,
                   GPT-3.5, and GPT-4. Our pipeline consists of multiple
                   stages, including extracting information from PDFs,
                   generating questions and answers, using them for
                   fine-tuning, and leveraging GPT-4 for evaluating the
                   results. We propose metrics to assess the performance of
                   different stages of the RAG and fine-Tuning pipeline. We
                   conduct an in-depth study on an agricultural dataset.
                   Agriculture as an industry has not seen much penetration of
                   AI, and we study a potentially disruptive application - what
                   if we could provide location-specific insights to a farmer?
                   Our results show the effectiveness of our dataset generation
                   pipeline in capturing geographic-specific knowledge, and the
                   quantitative and qualitative benefits of RAG and
                   fine-tuning. We see an accuracy increase of over 6 p.p. when
                   fine-tuning the model and this is cumulative with RAG, which
                   increases accuracy by 5 p.p. further. In one particular
                   experiment, we also demonstrate that the fine-tuned model
                   leverages information from across geographies to answer
                   specific questions, increasing answer similarity from 47\%
                   to 72\%. Overall, the results point to how systems built
                   using LLMs can be adapted to respond and incorporate
                   knowledge across a dimension that is critical for a specific
                   industry, paving the way for further applications of LLMs in
                   other industrial domains.",
  month         =  jan,
  year          =  2024,
  keywords      = "Applications",
  archivePrefix = "arXiv",
  eprint        = "2401.08406",
  primaryClass  = "cs.CL",
  arxivid       = "2401.08406"
}

@ARTICLE{Liu2024-ix,
  title         = "Tuning language models by proxy",
  author        = "Liu, Alisa and Han, Xiaochuang and Wang, Yizhong and
                   Tsvetkov, Yulia and Choi, Yejin and Smith, Noah A",
  abstract      = "Despite the general capabilities of large pretrained
                   language models, they consistently benefit from further
                   adaptation to better achieve desired behaviors. However,
                   tuning these models has become increasingly
                   resource-intensive, or impossible when model weights are
                   private. We introduce proxy-tuning, a lightweight
                   decoding-time algorithm that operates on top of black-box
                   LMs to achieve the result of directly tuning the model, but
                   by accessing only its prediction over the output vocabulary.
                   Our method instead tunes a smaller LM, then applies the
                   difference between the predictions of the small tuned and
                   untuned LMs to shift the original predictions of the base
                   model in the direction of tuning, while retaining the
                   benefits of larger scale pretraining. In experiments, when
                   we apply proxy-tuning to Llama2-70B using proxies of only 7B
                   size, we can close 88\% of the gap between Llama2-70B and
                   its truly-tuned chat version, when evaluated across
                   knowledge, reasoning, and safety benchmarks. Interestingly,
                   when tested on TruthfulQA, proxy-tuned models are actually
                   more truthful than directly tuned models, possibly because
                   decoding-time guidance better retains the model's factual
                   knowledge. We then demonstrate the generality of
                   proxy-tuning by applying it for domain adaptation on code,
                   and task-specific finetuning on question-answering and math
                   problems. Our work demonstrates the promise of using small
                   tuned LMs to efficiently customize large, potentially
                   proprietary LMs through decoding-time guidance.",
  month         =  jan,
  year          =  2024,
  keywords      = "Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "2401.08565",
  primaryClass  = "cs.CL",
  arxivid       = "2401.08565"
}

@ARTICLE{Kossen2021-au,
  title         = "{Self-Attention} Between Datapoints: Going Beyond Individual
                   {Input-Output} Pairs in Deep Learning",
  author        = "Kossen, Jannik and Band, Neil and Lyle, Clare and Gomez,
                   Aidan N and Rainforth, Tom and Gal, Yarin",
  abstract      = "We challenge a common assumption underlying most supervised
                   deep learning: that a model makes a prediction depending
                   only on its parameters and the features of a single input.
                   To this end, we introduce a general-purpose deep learning
                   architecture that takes as input the entire dataset instead
                   of processing one datapoint at a time. Our approach uses
                   self-attention to reason about relationships between
                   datapoints explicitly, which can be seen as realizing
                   non-parametric models using parametric attention mechanisms.
                   However, unlike conventional non-parametric models, we let
                   the model learn end-to-end from the data how to make use of
                   other datapoints for prediction. Empirically, our models
                   solve cross-datapoint lookup and complex reasoning tasks
                   unsolvable by traditional deep learning models. We show
                   highly competitive results on tabular data, early results on
                   CIFAR-10, and give insight into how the model makes use of
                   the interactions between points.",
  month         =  jun,
  year          =  2021,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2106.02584",
  primaryClass  = "cs.LG",
  arxivid       = "2106.02584"
}

@ARTICLE{noauthor_undated-zb,
  title    = "Long Range Arena : A Benchmark for Efficient Transformers",
  author   = "Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang
              and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu
              and Ruder, Sebastian and Metzler, Donald",
  abstract = "Transformers do not scale very well to long sequence lengths
              largely because of quadratic self-attention complexity. In the
              recent months, a wide spectrum of efficient, fast Transformers
              have been proposed to tackle this problem, more often than not
              claiming superior or comparable model quality to vanilla
              Transformer models. To this date, there is no well-established
              consensus on how to evaluate this class of models. Moreover,
              inconsistent benchmarking on a wide spectrum of tasks and
              datasets makes it difficult to assess relative model quality
              amongst many models. This paper proposes a systematic and unified
              benchmark, Long Range Arena, specifically focused on evaluating
              model quality under long-context scenarios. Our benchmark is a
              suite of tasks consisting of sequences ranging from $1K$ to $16K$
              tokens, encompassing a wide range of data types and modalities
              such as text, natural, synthetic images, and mathematical
              expressions requiring similarity, structural, and visual-spatial
              reasoning. We systematically evaluate ten well-established
              long-range Transformer models (Reformers, Linformers, Linear
              Transformers, Sinkhorn Transformers, Performers, Synthesizers,
              Sparse Transformers, and Longformers) on our newly proposed
              benchmark suite. Long Range Arena paves the way towards better
              understanding this class of efficient Transformer models,
              facilitates more research in this direction, and presents new
              challenging tasks to tackle.",
  month    =  sep,
  year     =  2020,
  keywords = "Evaluation;LLMs"
}

@ARTICLE{Huang2018-yk,
  title         = "Music Transformer",
  author        = "Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit,
                   Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis
                   and Dai, Andrew M and Hoffman, Matthew D and Dinculescu,
                   Monica and Eck, Douglas",
  abstract      = "Music relies heavily on repetition to build structure and
                   meaning. Self-reference occurs on multiple timescales, from
                   motifs to phrases to reusing of entire sections of music,
                   such as in pieces with ABA structure. The Transformer
                   (Vaswani et al., 2017), a sequence model based on
                   self-attention, has achieved compelling results in many
                   generation tasks that require maintaining long-range
                   coherence. This suggests that self-attention might also be
                   well-suited to modeling music. In musical composition and
                   performance, however, relative timing is critically
                   important. Existing approaches for representing relative
                   positional information in the Transformer modulate attention
                   based on pairwise distance (Shaw et al., 2018). This is
                   impractical for long sequences such as musical compositions
                   since their memory complexity for intermediate relative
                   information is quadratic in the sequence length. We propose
                   an algorithm that reduces their intermediate memory
                   requirement to linear in the sequence length. This enables
                   us to demonstrate that a Transformer with our modified
                   relative attention mechanism can generate minute-long
                   compositions (thousands of steps, four times the length
                   modeled in Oore et al., 2018) with compelling structure,
                   generate continuations that coherently elaborate on a given
                   motif, and in a seq2seq setup generate accompaniments
                   conditioned on melodies. We evaluate the Transformer with
                   our relative attention mechanism on two datasets, JSB
                   Chorales and Piano-e-Competition, and obtain
                   state-of-the-art results on the latter.",
  month         =  sep,
  year          =  2018,
  keywords      = "Sound;LLMs",
  archivePrefix = "arXiv",
  eprint        = "1809.04281",
  primaryClass  = "cs.LG",
  arxivid       = "1809.04281"
}

@ARTICLE{Bruch2024-te,
  title         = "Foundations of vector retrieval",
  author        = "Bruch, Sebastian",
  abstract      = "Vectors are universal mathematical objects that can
                   represent text, images, speech, or a mix of these data
                   modalities. That happens regardless of whether data is
                   represented by hand-crafted features or learnt embeddings.
                   Collect a large enough quantity of such vectors and the
                   question of retrieval becomes urgently relevant: Finding
                   vectors that are more similar to a query vector. This
                   monograph is concerned with the question above and covers
                   fundamental concepts along with advanced data structures and
                   algorithms for vector retrieval. In doing so, it recaps this
                   fascinating topic and lowers barriers of entry into this
                   rich area of research.",
  month         =  jan,
  year          =  2024,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2401.09350",
  primaryClass  = "cs.DS",
  arxivid       = "2401.09350"
}

@MISC{Open_undated-ut,
  title    = "{GPT-4V(ision}) System Card",
  author   = "Open, A I",
  keywords = "Multimodal (Vision, speech, etc);Models"
}

@ARTICLE{Raji2023-ac,
  title         = "Concrete problems in {AI} safety, revisited",
  author        = "Raji, Inioluwa Deborah and Dobbe, Roel",
  abstract      = "As AI systems proliferate in society, the AI community is
                   increasingly preoccupied with the concept of AI Safety,
                   namely the prevention of failures due to accidents that
                   arise from an unanticipated departure of a system's behavior
                   from designer intent in AI deployment. We demonstrate
                   through an analysis of real world cases of such incidents
                   that although current vocabulary captures a range of the
                   encountered issues of AI deployment, an expanded
                   socio-technical framing will be required for a more complete
                   understanding of how AI systems and implemented safety
                   mechanisms fail and succeed in real life.",
  month         =  dec,
  year          =  2023,
  keywords      = "Safety/Risks",
  archivePrefix = "arXiv",
  eprint        = "2401.10899",
  primaryClass  = "cs.CY",
  arxivid       = "2401.10899"
}

@ARTICLE{Lin2024-xg,
  title         = "{MaLA-500}: Massive language adaptation of large language
                   models",
  author        = "Lin, Peiqin and Ji, Shaoxiong and Tiedemann, J{\"o}rg and
                   Martins, Andr{\'e} F T and Sch{\"u}tze, Hinrich",
  abstract      = "Large language models have advanced the state of the art in
                   natural language processing. However, their predominant
                   design for English or a limited set of languages creates a
                   substantial gap in their effectiveness for low-resource
                   languages. To bridge this gap, we introduce MaLA-500, a
                   novel large language model designed to cover an extensive
                   range of 534 languages. To train MaLA-500, we employ
                   vocabulary extension and continued pretraining on LLaMA 2
                   with Glot500-c. Our experiments on SIB-200 show that
                   MaLA-500 achieves state-of-the-art in-context learning
                   results. We release MaLA-500 at
                   https://huggingface.co/MaLA-LM",
  month         =  jan,
  year          =  2024,
  keywords      = "Fine-tuning",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2401.13303",
  primaryClass  = "cs.CL",
  arxivid       = "2401.13303"
}

@ARTICLE{Kusupati2022-sl,
  title         = "Matryoshka Representation Learning",
  author        = "Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and
                   Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek
                   and Howard-Snyder, William and Chen, Kaifeng and Kakade,
                   Sham and Jain, Prateek and Farhadi, Ali",
  abstract      = "Learned representations are a central component in modern ML
                   systems, serving a multitude of downstream tasks. When
                   training such representations, it is often the case that
                   computational and statistical constraints for each
                   downstream task are unknown. In this context rigid, fixed
                   capacity representations can be either over or
                   under-accommodating to the task at hand. This leads us to
                   ask: can we design a flexible representation that can adapt
                   to multiple downstream tasks with varying computational
                   resources? Our main contribution is Matryoshka
                   Representation Learning (MRL) which encodes information at
                   different granularities and allows a single embedding to
                   adapt to the computational constraints of downstream tasks.
                   MRL minimally modifies existing representation learning
                   pipelines and imposes no additional cost during inference
                   and deployment. MRL learns coarse-to-fine representations
                   that are at least as accurate and rich as independently
                   trained low-dimensional representations. The flexibility
                   within the learned Matryoshka Representations offer: (a) up
                   to 14x smaller embedding size for ImageNet-1K classification
                   at the same level of accuracy; (b) up to 14x real-world
                   speed-ups for large-scale retrieval on ImageNet-1K and 4K;
                   and (c) up to 2\% accuracy improvements for long-tail
                   few-shot classification, all while being as robust as the
                   original representations. Finally, we show that MRL extends
                   seamlessly to web-scale datasets (ImageNet, JFT) across
                   various modalities -- vision (ViT, ResNet), vision +
                   language (ALIGN) and language (BERT). MRL code and
                   pretrained models are open-sourced at
                   https://github.com/RAIVNLab/MRL.",
  month         =  may,
  year          =  2022,
  keywords      = "Word Embeddings;LLMs",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2205.13147",
  primaryClass  = "cs.LG",
  arxivid       = "2205.13147"
}

@MISC{Inan_undated-hx,
  title    = "Llama Guard: {LLM-based} {Input-Output} Safeguard for {Human-AI}
              Conversations",
  author   = "Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta,
              Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael
              and Hu, Qing and Fuller, Brian and Testuggine, Davide and Khabsa,
              Madian",
  keywords = "Safety/Risks"
}

@ARTICLE{Radford2022-cc,
  title         = "Robust Speech Recognition via {Large-Scale} Weak Supervision",
  author        = "Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman,
                   Greg and McLeavey, Christine and Sutskever, Ilya",
  abstract      = "We study the capabilities of speech processing systems
                   trained simply to predict large amounts of transcripts of
                   audio on the internet. When scaled to 680,000 hours of
                   multilingual and multitask supervision, the resulting models
                   generalize well to standard benchmarks and are often
                   competitive with prior fully supervised results but in a
                   zero-shot transfer setting without the need for any
                   fine-tuning. When compared to humans, the models approach
                   their accuracy and robustness. We are releasing models and
                   inference code to serve as a foundation for further work on
                   robust speech processing.",
  month         =  dec,
  year          =  2022,
  keywords      = "Speech",
  archivePrefix = "arXiv",
  eprint        = "2212.04356",
  primaryClass  = "eess.AS",
  arxivid       = "2212.04356"
}

@ARTICLE{noauthor_undated-uw,
  title         = "Scaling Speech Technology to 1,000+ Languages",
  author        = "Pratap, Vineel and Tjandra, Andros and Shi, Bowen and
                   Tomasello, Paden and Babu, Arun and Kundu, Sayani and
                   Elkahky, Ali and Ni, Zhaoheng and Vyas, Apoorv and
                   Fazel-Zarandi, Maryam and Baevski, Alexei and Adi, Yossi and
                   Zhang, Xiaohui and Hsu, Wei-Ning and Conneau, Alexis and
                   Auli, Michael",
  abstract      = "Expanding the language coverage of speech technology has the
                   potential to improve access to information for many more
                   people. However, current speech technology is restricted to
                   about one hundred languages which is a small fraction of the
                   over 7,000 languages spoken around the world. The Massively
                   Multilingual Speech (MMS) project increases the number of
                   supported languages by 10-40x, depending on the task. The
                   main ingredients are a new dataset based on readings of
                   publicly available religious texts and effectively
                   leveraging self-supervised learning. We built pre-trained
                   wav2vec 2.0 models covering 1,406 languages, a single
                   multilingual automatic speech recognition model for 1,107
                   languages, speech synthesis models for the same number of
                   languages, as well as a language identification model for
                   4,017 languages. Experiments show that our multilingual
                   speech recognition model more than halves the word error
                   rate of Whisper on 54 languages of the FLEURS benchmark
                   while being trained on a small fraction of the labeled data.",
  journal       = "arXiv [cs.CL]",
  month         =  may,
  year          =  2023,
  keywords      = "Speech",
  archivePrefix = "arXiv",
  eprint        = "2305.13516",
  primaryClass  = "cs.CL",
  arxivid       = "2305.13516"
}

@ARTICLE{Javed2022-ja,
  title    = "Towards Building {ASR} Systems for the Next Billion Users",
  author   = "Javed, Tahir and Doddapaneni, Sumanth and Raman, Abhigyan and
              Bhogale, Kaushal Santosh and Ramesh, Gowtham and Kunchukuttan,
              Anoop and Kumar, Pratyush and Khapra, Mitesh M",
  journal  = "AAAI",
  volume   =  36,
  number   =  10,
  pages    = "10813--10821",
  month    =  jun,
  year     =  2022,
  keywords = "Speech",
  language = "en",
  issn     = "2374-3468, 2374-3468",
  doi      = "10.1609/aaai.v36i10.21327"
}

@ARTICLE{Ramesh2022-pg,
  title     = "Samanantar: The Largest Publicly Available Parallel Corpora
               Collection for 11 {I}ndic Languages",
  author    = "Ramesh, Gowtham and Doddapaneni, Sumanth and Bheemaraj, Aravinth
               and Jobanputra, Mayank and Ak, Raghavan and Sharma, Ajitesh and
               Sahoo, Sujit and Diddee, Harshita and J, Mahalakshmi and
               Kakwani, Divyanshu and Kumar, Navneet and Pradeep, Aswin and
               Nagaraj, Srihari and Deepak, Kumar and Raghavan, Vivek and
               Kunchukuttan, Anoop and Kumar, Pratyush and Khapra, Mitesh
               Shantadevi",
  editor    = "Roark, Brian and Nenkova, Ani",
  abstract  = "We present Samanantar, the largest publicly available parallel
               corpora collection for Indic languages. The collection contains
               a total of 49.7 million sentence pairs between English and 11
               Indic languages (from two language families). Specifically, we
               compile 12.4 million sentence pairs from existing, publicly
               available parallel corpora, and additionally mine 37.4 million
               sentence pairs from the Web, resulting in a 4$\times$ increase.
               We mine the parallel sentences from the Web by combining many
               corpora, tools, and methods: (a) Web-crawled monolingual
               corpora, (b) document OCR for extracting sentences from scanned
               documents, (c) multilingual representation models for aligning
               sentences, and (d) approximate nearest neighbor search for
               searching in a large collection of sentences. Human evaluation
               of samples from the newly mined corpora validate the high
               quality of the parallel sentences across 11 languages. Further,
               we extract 83.4 million sentence pairs between all 55 Indic
               language pairs from the English-centric parallel corpus using
               English as the pivot language. We trained multilingual NMT
               models spanning all these languages on Samanantar which
               outperform existing models and baselines on publicly available
               benchmarks, such as FLORES, establishing the utility of
               Samanantar. Our data and models are available publicly at
               Samanantar and we hope they will help advance research in NMT
               and multilingual NLP for Indic languages.",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press",
  volume    =  10,
  pages     = "145--162",
  year      =  2022,
  address   = "Cambridge, MA",
  keywords  = "Datasets",
  doi       = "10.1162/tacl\_a\_00452"
}

@ARTICLE{noauthor_undated-ce,
  title     = "{I}ndic{NLG} Benchmark: Multilingual Datasets for Diverse {NLG}
               Tasks in {I}ndic Languages",
  author    = "Kumar, Aman and Shrotriya, Himani and Sahu, Prachi and Mishra,
               Amogh and Dabre, Raj and Puduppully, Ratish and Kunchukuttan,
               Anoop and Khapra, Mitesh M and Kumar, Pratyush",
  editor    = "Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue",
  abstract  = "Natural Language Generation (NLG) for non-English languages is
               hampered by the scarcity of datasets in these languages. We
               present the IndicNLG Benchmark, a collection of datasets for
               benchmarking NLG for 11 Indic languages. We focus on five
               diverse tasks, namely, biography generation using Wikipedia
               infoboxes, news headline generation, sentence summarization,
               paraphrase generation and, question generation. We describe the
               created datasets and use them to benchmark the performance of
               several monolingual and multilingual baselines that leverage
               pre-trained sequence-to-sequence models. Our results exhibit the
               strong performance of multilingual language-specific pre-trained
               models, and the utility of models trained on our dataset for
               other related NLG tasks. Our dataset creation methods can be
               easily applied to modest-resource languages as they involve
               simple steps such as scraping news articles and Wikipedia
               infoboxes, light cleaning, and pivoting through machine
               translation data. To the best of our knowledge, the IndicNLG
               Benchmark is the first NLG benchmark for Indic languages and the
               most diverse multilingual NLG dataset, with approximately 8M
               examples across 5 tasks and 11 languages. The datasets and
               models will be publicly available.",
  publisher = "Association for Computational Linguistics",
  pages     = "5363--5394",
  month     =  dec,
  year      =  2022,
  address   = "Abu Dhabi, United Arab Emirates",
  keywords  = "Evaluation",
  doi       = "10.18653/v1/2022.emnlp-main.360"
}

@ARTICLE{noauthor_undated-cl,
  title         = "Aksharantar: Open Indic-language Transliteration datasets
                   and models for the Next Billion Users",
  author        = "Madhani, Yash and Parthan, Sushane and Bedekar, Priyanka and
                   Gokul, N C and Khapra, Ruchi and Kunchukuttan, Anoop and
                   Kumar, Pratyush and Khapra, Mitesh M",
  abstract      = "Transliteration is very important in the Indian language
                   context due to the usage of multiple scripts and the
                   widespread use of romanized inputs. However, few training
                   and evaluation sets are publicly available. We introduce
                   Aksharantar, the largest publicly available transliteration
                   dataset for Indian languages created by mining from
                   monolingual and parallel corpora, as well as collecting data
                   from human annotators. The dataset contains 26 million
                   transliteration pairs for 21 Indic languages from 3 language
                   families using 12 scripts. Aksharantar is 21 times larger
                   than existing datasets and is the first publicly available
                   dataset for 7 languages and 1 language family. We also
                   introduce the Aksharantar testset comprising 103k word pairs
                   spanning 19 languages that enables a fine-grained analysis
                   of transliteration models on native origin words, foreign
                   words, frequent words, and rare words. Using the training
                   set, we trained IndicXlit, a multilingual transliteration
                   model that improves accuracy by 15\% on the Dakshina test
                   set, and establishes strong baselines on the Aksharantar
                   testset introduced in this work. The models, mining scripts,
                   transliteration guidelines, and datasets are available at
                   https://github.com/AI4Bharat/IndicXlit under open-source
                   licenses. We hope the availability of these large-scale,
                   open resources will spur innovation for Indic language
                   transliteration and downstream applications. We hope the
                   availability of these large-scale, open resources will spur
                   innovation for Indic language transliteration and downstream
                   applications.",
  journal       = "arXiv [cs.CL]",
  month         =  may,
  year          =  2022,
  keywords      = "Speech",
  archivePrefix = "arXiv",
  eprint        = "2205.03018",
  primaryClass  = "cs.CL",
  arxivid       = "2205.03018"
}

@ARTICLE{noauthor_undated-zh,
  title         = "Scaling Speech Technology to 1,000+ Languages",
  author        = "Pratap, Vineel and Tjandra, Andros and Shi, Bowen and
                   Tomasello, Paden and Babu, Arun and Kundu, Sayani and
                   Elkahky, Ali and Ni, Zhaoheng and Vyas, Apoorv and
                   Fazel-Zarandi, Maryam and Baevski, Alexei and Adi, Yossi and
                   Zhang, Xiaohui and Hsu, Wei-Ning and Conneau, Alexis and
                   Auli, Michael",
  abstract      = "Expanding the language coverage of speech technology has the
                   potential to improve access to information for many more
                   people. However, current speech technology is restricted to
                   about one hundred languages which is a small fraction of the
                   over 7,000 languages spoken around the world. The Massively
                   Multilingual Speech (MMS) project increases the number of
                   supported languages by 10-40x, depending on the task. The
                   main ingredients are a new dataset based on readings of
                   publicly available religious texts and effectively
                   leveraging self-supervised learning. We built pre-trained
                   wav2vec 2.0 models covering 1,406 languages, a single
                   multilingual automatic speech recognition model for 1,107
                   languages, speech synthesis models for the same number of
                   languages, as well as a language identification model for
                   4,017 languages. Experiments show that our multilingual
                   speech recognition model more than halves the word error
                   rate of Whisper on 54 languages of the FLEURS benchmark
                   while being trained on a small fraction of the labeled data.",
  journal       = "arXiv [cs.CL]",
  month         =  may,
  year          =  2023,
  keywords      = "Speech",
  archivePrefix = "arXiv",
  eprint        = "2305.13516",
  primaryClass  = "cs.CL",
  arxivid       = "2305.13516"
}

@ARTICLE{Laskar2023-lt,
  title         = "A Systematic Study and Comprehensive Evaluation of {ChatGPT}
                   on Benchmark Datasets",
  author        = "Laskar, Md Tahmid Rahman and Saiful Bari, M and Rahman,
                   Mizanur and Bhuiyan, Md Amran Hossen and Joty, Shafiq and
                   Huang, Jimmy Xiangji",
  abstract      = "The development of large language models (LLMs) such as
                   ChatGPT has brought a lot of attention recently. However,
                   their evaluation in the benchmark academic datasets remains
                   under-explored due to the difficulty of evaluating the
                   generative outputs produced by this model against the ground
                   truth. In this paper, we aim to present a thorough
                   evaluation of ChatGPT's performance on diverse academic
                   datasets, covering tasks like question-answering, text
                   summarization, code generation, commonsense reasoning,
                   mathematical problem-solving, machine translation, bias
                   detection, and ethical considerations. Specifically, we
                   evaluate ChatGPT across 140 tasks and analyze 255K responses
                   it generates in these datasets. This makes our work the
                   largest evaluation of ChatGPT in NLP benchmarks. In short,
                   our study aims to validate the strengths and weaknesses of
                   ChatGPT in various tasks and provide insights for future
                   research using LLMs. We also report a new emergent ability
                   to follow multi-query instructions that we mostly found in
                   ChatGPT and other instruction-tuned models. Our extensive
                   evaluation shows that even though ChatGPT is capable of
                   performing a wide variety of tasks, and may obtain
                   impressive performance in several benchmark datasets, it is
                   still far from achieving the ability to reliably solve many
                   challenging tasks. By providing a thorough assessment of
                   ChatGPT's performance across diverse NLP tasks, this paper
                   sets the stage for a targeted deployment of ChatGPT-like
                   LLMs in real-world applications.",
  month         =  may,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2305.18486",
  primaryClass  = "cs.CL",
  arxivid       = "2305.18486"
}

@ARTICLE{Dettmers2023-fw,
  title         = "{QLoRA}: Efficient Finetuning of Quantized {LLMs}",
  author        = "Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and
                   Zettlemoyer, Luke",
  abstract      = "We present QLoRA, an efficient finetuning approach that
                   reduces memory usage enough to finetune a 65B parameter
                   model on a single 48GB GPU while preserving full 16-bit
                   finetuning task performance. QLoRA backpropagates gradients
                   through a frozen, 4-bit quantized pretrained language model
                   into Low Rank Adapters~(LoRA). Our best model family, which
                   we name Guanaco, outperforms all previous openly released
                   models on the Vicuna benchmark, reaching 99.3\% of the
                   performance level of ChatGPT while only requiring 24 hours
                   of finetuning on a single GPU. QLoRA introduces a number of
                   innovations to save memory without sacrificing performance:
                   (a) 4-bit NormalFloat (NF4), a new data type that is
                   information theoretically optimal for normally distributed
                   weights (b) double quantization to reduce the average memory
                   footprint by quantizing the quantization constants, and (c)
                   paged optimziers to manage memory spikes. We use QLoRA to
                   finetune more than 1,000 models, providing a detailed
                   analysis of instruction following and chatbot performance
                   across 8 instruction datasets, multiple model types (LLaMA,
                   T5), and model scales that would be infeasible to run with
                   regular finetuning (e.g. 33B and 65B parameter models). Our
                   results show that QLoRA finetuning on a small high-quality
                   dataset leads to state-of-the-art results, even when using
                   smaller models than the previous SoTA. We provide a detailed
                   analysis of chatbot performance based on both human and
                   GPT-4 evaluations showing that GPT-4 evaluations are a cheap
                   and reasonable alternative to human evaluation. Furthermore,
                   we find that current chatbot benchmarks are not trustworthy
                   to accurately evaluate the performance levels of chatbots. A
                   lemon-picked analysis demonstrates where Guanaco fails
                   compared to ChatGPT. We release all of our models and code,
                   including CUDA kernels for 4-bit training.",
  month         =  may,
  year          =  2023,
  keywords      = "Efficient;Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "2305.14314",
  primaryClass  = "cs.LG",
  arxivid       = "2305.14314"
}

@ARTICLE{R2023-lc,
  title         = "{SPRING-INX}: A multilingual Indian language speech corpus
                   by {SPRING} Lab, {IIT} madras",
  author        = "R, Nithya and S, Malavika and F, Jordan and Gangwar, Arjun
                   and J, Metilda N and Umesh, S and Sarab, Rithik and Dubey,
                   Akhilesh Kumar and Divakaran, Govind and K, Samudra Vijaya
                   and Gangashetty, Suryakanth V",
  abstract      = "India is home to a multitude of languages of which 22
                   languages are recognised by the Indian Constitution as
                   official. Building speech based applications for the Indian
                   population is a difficult problem owing to limited data and
                   the number of languages and accents to accommodate. To
                   encourage the language technology community to build speech
                   based applications in Indian languages, we are open sourcing
                   SPRING-INX data which has about 2000 hours of legally
                   sourced and manually transcribed speech data for ASR system
                   building in Assamese, Bengali, Gujarati, Hindi, Kannada,
                   Malayalam, Marathi, Odia, Punjabi and Tamil. This endeavor
                   is by SPRING Lab , Indian Institute of Technology Madras and
                   is a part of National Language Translation Mission (NLTM),
                   funded by the Indian Ministry of Electronics and Information
                   Technology (MeitY), Government of India. We describe the
                   data collection and data cleaning process along with the
                   data statistics in this paper.",
  month         =  oct,
  year          =  2023,
  keywords      = "Speech;Speech",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2310.14654",
  primaryClass  = "cs.CL",
  arxivid       = "2310.14654"
}

@ARTICLE{Soldaini_undated-bm,
  title    = ": an Open Corpus of Three Trillion Tokens for Language Model
              Pretraining Research",
  author   = "Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and
              Schwenk, Dustin and Bogin, David Atkinson Russell Authur and
              Elazar, Khyathi Chandu Jennifer Dumas and Subramani, Nishant",
  keywords = "Datasets"
}

@ARTICLE{noauthor_undated-na,
  title         = "Paloma: A Benchmark for Evaluating Language Model Fit",
  author        = "Magnusson, Ian and Bhagia, Akshita and Hofmann, Valentin and
                   Soldaini, Luca and Jha, Ananya Harsh and Tafjord, Oyvind and
                   Schwenk, Dustin and Walsh, Evan Pete and Elazar, Yanai and
                   Lo, Kyle and Groeneveld, Dirk and Beltagy, Iz and
                   Hajishirzi, Hannaneh and Smith, Noah A and Richardson, Kyle
                   and Dodge, Jesse",
  abstract      = "Language models (LMs) commonly report perplexity on
                   monolithic data held out from training. Implicitly or
                   explicitly, this data is composed of
                   domains$\unicode\{x2013\}$varying distributions of language.
                   Rather than assuming perplexity on one distribution
                   extrapolates to others, Perplexity Analysis for Language
                   Model Assessment (Paloma), measures LM fit to 585 text
                   domains, ranging from nytimes.com to r/depression on Reddit.
                   We invite submissions to our benchmark and organize results
                   by comparability based on compliance with guidelines such as
                   removal of benchmark contamination from pretraining.
                   Submissions can also record parameter and training token
                   count to make comparisons of Pareto efficiency for
                   performance as a function of these measures of cost. We
                   populate our benchmark with results from 6 baselines
                   pretrained on popular corpora. In case studies, we
                   demonstrate analyses that are possible with Paloma, such as
                   finding that pretraining without data beyond Common Crawl
                   leads to inconsistent fit to many domains.",
  journal       = "arXiv [cs.CL]",
  month         =  dec,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2312.10523",
  primaryClass  = "cs.CL",
  arxivid       = "2312.10523"
}

@MISC{Lo_Luca_Soldaini_Noah_A_Smith_Hannaneh_Hajishirzi_undated-ip,
  title       = "{OLMo}: Modeling, training, eval, and inference code for
                 {OLMo}",
  author      = "Lo Luca Soldaini Noah A. Smith Hannaneh Hajishirzi, Nishant
                 Subramani Mitchell Wortsman Pradeep Dasigi Nathan Lambert Kyle
                 Richardson Luke Zettlemoyer Jesse Dodge Kyle",
  abstract    = "Modeling, training, eval, and inference code for OLMo - GitHub
                 - allenai/OLMo: Modeling, training, eval, and inference code
                 for OLMo",
  institution = "Github",
  keywords    = "Models",
  language    = "en"
}

@ARTICLE{Feffer2024-cn,
  title         = "Red-teaming for generative {AI}: Silver bullet or security
                   theater?",
  author        = "Feffer, Michael and Sinha, Anusha and Lipton, Zachary Chase
                   and Heidari, Hoda",
  abstract      = "In response to rising concerns surrounding the safety,
                   security, and trustworthiness of Generative AI (GenAI)
                   models, practitioners and regulators alike have pointed to
                   AI red-teaming as a key component of their strategies for
                   identifying and mitigating these risks. However, despite AI
                   red-teaming's central role in policy discussions and
                   corporate messaging, significant questions remain about what
                   precisely it means, what role it can play in regulation, and
                   how precisely it relates to conventional red-teaming
                   practices as originally conceived in the field of
                   cybersecurity. In this work, we identify recent cases of
                   red-teaming activities in the AI industry and conduct an
                   extensive survey of the relevant research literature to
                   characterize the scope, structure, and criteria for AI
                   red-teaming practices. Our analysis reveals that prior
                   methods and practices of AI red-teaming diverge along
                   several axes, including the purpose of the activity (which
                   is often vague), the artifact under evaluation, the setting
                   in which the activity is conducted (e.g., actors, resources,
                   and methods), and the resulting decisions it informs (e.g.,
                   reporting, disclosure, and mitigation). In light of our
                   findings, we argue that while red-teaming may be a valuable
                   big-tent idea for characterizing a broad set of activities
                   and attitudes aimed at improving the behavior of GenAI
                   models, gestures towards red-teaming as a panacea for every
                   possible risk verge on security theater. To move toward a
                   more robust toolbox of evaluations for generative AI, we
                   synthesize our recommendations into a question bank meant to
                   guide and scaffold future AI red-teaming practices.",
  journal       = "arXiv [cs.CY]",
  month         =  jan,
  year          =  2024,
  keywords      = "Safety/Risks",
  archivePrefix = "arXiv",
  eprint        = "2401.15897",
  primaryClass  = "cs.CY",
  arxivid       = "2401.15897"
}

@ARTICLE{Dwaracherla2024-oo,
  title         = "Efficient Exploration for {LLMs}",
  author        = "Dwaracherla, V and Asghari, S and Hao, Botao and Van Roy,
                   Benjamin",
  abstract      = "We present evidence of substantial benefit from efficient
                   exploration in gathering human feedback to improve large
                   language models. In our experiments, an agent sequentially
                   generates queries while fitting a reward model to the
                   feedback received. Our best-performing agent generates
                   queries using double Thompson sampling, with uncertainty
                   represented by an epistemic neural network. Our results
                   demonstrate that efficient exploration enables high levels
                   of performance with far fewer queries. Further, both
                   uncertainty estimation and the choice of exploration scheme
                   play critical roles.",
  journal       = "arXiv [cs.LG]",
  month         =  feb,
  year          =  2024,
  keywords      = "Efficient",
  archivePrefix = "arXiv",
  eprint        = "2402.00396",
  primaryClass  = "cs.LG",
  arxivid       = "2402.00396"
}

@ARTICLE{Papamarkou2024-oq,
  title         = "Position paper: Bayesian deep learning in the age of
                   large-scale {AI}",
  author        = "Papamarkou, Theodore and Skoularidou, Maria and Palla,
                   Konstantina and Aitchison, Laurence and Arbel, Julyan and
                   Dunson, David and Filippone, Maurizio and Fortuin, Vincent
                   and Hennig, Philipp and Hubin, Aliaksandr and Immer,
                   Alexander and Karaletsos, Theofanis and Khan, Mohammad
                   Emtiyaz and Kristiadi, Agustinus and Li, Yingzhen and
                   Lobato, Jose Miguel Hernandez and Mandt, Stephan and Nemeth,
                   Christopher and Osborne, Michael A and Rudner, Tim G J and
                   R{\"u}gamer, David and Teh, Yee Whye and Welling, Max and
                   Wilson, Andrew Gordon and Zhang, Ruqi",
  abstract      = "In the current landscape of deep learning research, there is
                   a predominant emphasis on achieving high predictive accuracy
                   in supervised tasks involving large image and language
                   datasets. However, a broader perspective reveals a multitude
                   of overlooked metrics, tasks, and data types, such as
                   uncertainty, active and continual learning, and scientific
                   data, that demand attention. Bayesian deep learning (BDL)
                   constitutes a promising avenue, offering advantages across
                   these diverse settings. This paper posits that BDL can
                   elevate the capabilities of deep learning. It revisits the
                   strengths of BDL, acknowledges existing challenges, and
                   highlights some exciting research avenues aimed at
                   addressing these obstacles. Looking ahead, the discussion
                   focuses on possible ways to combine large-scale foundation
                   models with BDL to unlock their full potential.",
  month         =  feb,
  year          =  2024,
  keywords      = "Bayesian Models;Models",
  archivePrefix = "arXiv",
  eprint        = "2402.00809",
  primaryClass  = "cs.LG",
  arxivid       = "2402.00809"
}

@ARTICLE{Sadasivan2023-jb,
  title         = "Can {AI-Generated} Text be Reliably Detected?",
  author        = "Sadasivan, Vinu Sankar and Kumar, Aounon and
                   Balasubramanian, Sriram and Wang, Wenxiao and Feizi, Soheil",
  abstract      = "In this paper, both empirically and theoretically, we show
                   that several AI-text detectors are not reliable in practical
                   scenarios. Empirically, we show that paraphrasing attacks,
                   where a light paraphraser is applied on top of a large
                   language model (LLM), can break a whole range of detectors,
                   including ones using watermarking schemes as well as neural
                   network-based detectors and zero-shot classifiers. Our
                   experiments demonstrate that retrieval-based detectors,
                   designed to evade paraphrasing attacks, are still vulnerable
                   to recursive paraphrasing. We then provide a theoretical
                   impossibility result indicating that as language models
                   become more sophisticated and better at emulating human
                   text, the performance of even the best-possible detector
                   decreases. For a sufficiently advanced language model
                   seeking to imitate human text, even the best-possible
                   detector may only perform marginally better than a random
                   classifier. Our result is general enough to capture specific
                   scenarios such as particular writing styles, clever prompt
                   design, or text paraphrasing. We also extend the
                   impossibility result to include the case where pseudorandom
                   number generators are used for AI-text generation instead of
                   true randomness. We show that the same result holds with a
                   negligible correction term for all polynomial-time
                   computable detectors. Finally, we show that even LLMs
                   protected by watermarking schemes can be vulnerable against
                   spoofing attacks where adversarial humans can infer hidden
                   LLM text signatures and add them to human-generated text to
                   be detected as text generated by the LLMs, potentially
                   causing reputational damage to their developers. We believe
                   these results can open an honest conversation in the
                   community regarding the ethical and reliable use of
                   AI-generated text.",
  month         =  mar,
  year          =  2023,
  keywords      = "Watermarking",
  archivePrefix = "arXiv",
  eprint        = "2303.11156",
  primaryClass  = "cs.CL",
  arxivid       = "2303.11156"
}

@UNPUBLISHED{Perlman2022-od,
  title    = "The Implications of {ChatGPT} for Legal Services and Society",
  author   = "Perlman, Andrew M",
  abstract = "On November 30, 2022, OpenAI released a chatbot called ChatGPT.
              To demonstrate the chatbot's remarkable sophistication and
              potential implications, for both legal services and society more
              generally, most of this paper was generated in about an hour
              through prompts within ChatGPT. Only this abstract, the preface,
              the outline headers, the epilogue, and the prompts were written
              by a person. ChatGPT generated the rest of the text with no human
              editing.To be clear, the responses generated by ChatGPT were
              imperfect and at times problematic, and the use of an AI tool for
              law-related services raises a host of regulatory and ethical
              issues. At the same time, ChatGPT highlights the promise of
              artificial intelligence, including its ability to affect our
              lives in both modest and more profound ways. ChatGPT suggests an
              imminent reimagination of how we access and create information,
              obtain legal and other services, and prepare people for their
              careers. We also will soon face new questions about the role of
              knowledge workers in society, the attribution of work (e.g.,
              determining when people's written work is their own), and the
              potential misuse of and excessive reliance on the information
              produced by these kinds of tools. The disruptions from AI's rapid
              development are no longer in the distant future. They have
              arrived, and this document offers a small taste of what lies
              ahead.",
  month    =  dec,
  year     =  2022,
  keywords = "Open AI, Chatbots, Legal Innovation, Access To
              Justice;Applications",
  doi      = "10.2139/ssrn.4294197"
}

@ARTICLE{Bai_undated-ui,
  title    = "Training a Helpful and Harmless Assistant with Reinforcement
              Learning from Human Feedback",
  author   = "Bai, Yuntao and Jones, Andy and Ndousse, Kamal",
  keywords = "Alignment (RLHF, etc)",
  arxivid  = "2204.05862"
}

@ARTICLE{Bubeck2023-zw,
  title         = "Sparks of Artificial General Intelligence: Early experiments
                   with {GPT-4}",
  author        = "Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan,
                   Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece
                   and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and
                   Lundberg, Scott and Nori, Harsha and Palangi, Hamid and
                   Ribeiro, Marco Tulio and Zhang, Yi",
  abstract      = "Artificial intelligence (AI) researchers have been
                   developing and refining large language models (LLMs) that
                   exhibit remarkable capabilities across a variety of domains
                   and tasks, challenging our understanding of learning and
                   cognition. The latest model developed by OpenAI, GPT-4, was
                   trained using an unprecedented scale of compute and data. In
                   this paper, we report on our investigation of an early
                   version of GPT-4, when it was still in active development by
                   OpenAI. We contend that (this early version of) GPT-4 is
                   part of a new cohort of LLMs (along with ChatGPT and
                   Google's PaLM for example) that exhibit more general
                   intelligence than previous AI models. We discuss the rising
                   capabilities and implications of these models. We
                   demonstrate that, beyond its mastery of language, GPT-4 can
                   solve novel and difficult tasks that span mathematics,
                   coding, vision, medicine, law, psychology and more, without
                   needing any special prompting. Moreover, in all of these
                   tasks, GPT-4's performance is strikingly close to
                   human-level performance, and often vastly surpasses prior
                   models such as ChatGPT. Given the breadth and depth of
                   GPT-4's capabilities, we believe that it could reasonably be
                   viewed as an early (yet still incomplete) version of an
                   artificial general intelligence (AGI) system. In our
                   exploration of GPT-4, we put special emphasis on discovering
                   its limitations, and we discuss the challenges ahead for
                   advancing towards deeper and more comprehensive versions of
                   AGI, including the possible need for pursuing a new paradigm
                   that moves beyond next-word prediction. We conclude with
                   reflections on societal influences of the recent
                   technological leap and future research directions.",
  month         =  mar,
  year          =  2023,
  keywords      = "Intriguing Properties",
  archivePrefix = "arXiv",
  eprint        = "2303.12712",
  primaryClass  = "cs.CL",
  arxivid       = "2303.12712"
}

@INPROCEEDINGS{Sai2021-lo,
  title     = "Perturbation {{C}heck{L}ists} for Evaluating {NLG} Evaluation
               Metrics",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in
               Natural Language Processing",
  author    = "Sai, Ananya B and Dixit, Tanay and Sheth, Dev Yashpal and Mohan,
               Sreyas and Khapra, Mitesh M",
  editor    = "Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and
               Yih, Scott Wen-Tau",
  abstract  = "Natural Language Generation (NLG) evaluation is a multifaceted
               task requiring assessment of multiple desirable criteria, e.g.,
               fluency, coherency, coverage, relevance, adequacy, overall
               quality, etc. Across existing datasets for 6 NLG tasks, we
               observe that the human evaluation scores on these multiple
               criteria are often not correlated. For example, there is a very
               low correlation between human scores on fluency and data
               coverage for the task of structured data to text generation.
               This suggests that the current recipe of proposing new automatic
               evaluation metrics for NLG by showing that they correlate well
               with scores assigned by humans for a single criteria (overall
               quality) alone is inadequate. Indeed, our extensive study
               involving 25 automatic evaluation metrics across 6 different
               tasks and 18 different evaluation criteria shows that there is
               no single metric which correlates well with human scores on all
               desirable criteria, for most NLG tasks. Given this situation, we
               propose CheckLists for better design and evaluation of automatic
               metrics. We design templates which target a specific criteria
               (e.g., coverage) and perturb the output such that the quality
               gets affected only along this specific criteria (e.g., the
               coverage drops). We show that existing evaluation metrics are
               not robust against even such simple perturbations and disagree
               with scores assigned by humans to the perturbed output. The
               proposed templates thus allow for a fine-grained assessment of
               automatic evaluation metrics exposing their limitations and will
               facilitate better design, analysis and evaluation of such
               metrics. Our templates and code are available at
               https://iitmnlp.github.io/EvalEval/",
  publisher = "Association for Computational Linguistics",
  pages     = "7219--7234",
  month     =  nov,
  year      =  2021,
  address   = "Online and Punta Cana, Dominican Republic",
  keywords  = "Evaluation",
  doi       = "10.18653/v1/2021.emnlp-main.575"
}

@MISC{Nori_undated-qq,
  title    = "Capabilities of {GPT-4} on Medical Challenge Problems",
  author   = "Nori, Harsha and King, Nicholas and Mc Kinney, Scott Mayer and
              Carignan, Dean and Horvitz, Eric",
  keywords = "Health"
}

@ARTICLE{Lai2023-eg,
  title         = "{ChatGPT} Beyond English: Towards a Comprehensive Evaluation
                   of Large Language Models in Multilingual Learning",
  author        = "Lai, Viet Dac and Ngo, Nghia Trung and Veyseh, Amir Pouran
                   Ben and Man, Hieu and Dernoncourt, Franck and Bui, Trung and
                   Nguyen, Thien Huu",
  abstract      = "Over the last few years, large language models (LLMs) have
                   emerged as the most important breakthroughs in natural
                   language processing (NLP) that fundamentally transform
                   research and developments in the field. ChatGPT represents
                   one of the most exciting LLM systems developed recently to
                   showcase impressive skills for language generation and
                   highly attract public attention. Among various exciting
                   applications discovered for ChatGPT in English, the model
                   can process and generate texts for multiple languages due to
                   its multilingual training data. Given the broad adoption of
                   ChatGPT for English in different problems and areas, a
                   natural question is whether ChatGPT can also be applied
                   effectively for other languages or it is necessary to
                   develop more language-specific technologies. The answer to
                   this question requires a thorough evaluation of ChatGPT over
                   multiple tasks with diverse languages and large datasets
                   (i.e., beyond reported anecdotes), which is still missing or
                   limited in current research. Our work aims to fill this gap
                   for the evaluation of ChatGPT and similar LLMs to provide
                   more comprehensive information for multilingual NLP
                   applications. While this work will be an ongoing effort to
                   include additional experiments in the future, our current
                   paper evaluates ChatGPT on 7 different tasks, covering 37
                   diverse languages with high, medium, low, and extremely low
                   resources. We also focus on the zero-shot learning setting
                   for ChatGPT to improve reproducibility and better simulate
                   the interactions of general users. Compared to the
                   performance of previous models, our extensive experimental
                   results demonstrate a worse performance of ChatGPT for
                   different NLP tasks and languages, calling for further
                   research to develop better models and understanding for
                   multilingual learning.",
  month         =  apr,
  year          =  2023,
  keywords      = "Applications",
  archivePrefix = "arXiv",
  eprint        = "2304.05613",
  primaryClass  = "cs.CL",
  arxivid       = "2304.05613"
}

@ARTICLE{Mangalam2023-hb,
  title         = "Reversible Vision Transformers",
  author        = "Mangalam, Karttikeya and Fan, Haoqi and Li, Yanghao and Wu,
                   Chao-Yuan and Xiong, Bo and Feichtenhofer, Christoph and
                   Malik, Jitendra",
  abstract      = "We present Reversible Vision Transformers, a memory
                   efficient architecture design for visual recognition. By
                   decoupling the GPU memory requirement from the depth of the
                   model, Reversible Vision Transformers enable scaling up
                   architectures with efficient memory usage. We adapt two
                   popular models, namely Vision Transformer and Multiscale
                   Vision Transformers, to reversible variants and benchmark
                   extensively across both model sizes and tasks of image
                   classification, object detection and video classification.
                   Reversible Vision Transformers achieve a reduced memory
                   footprint of up to 15.5x at roughly identical model
                   complexity, parameters and accuracy, demonstrating the
                   promise of reversible vision transformers as an efficient
                   backbone for hardware resource limited training regimes.
                   Finally, we find that the additional computational burden of
                   recomputing activations is more than overcome for deeper
                   models, where throughput can increase up to 2.3x over their
                   non-reversible counterparts. Full code and trained models
                   are available at
                   https://github.com/facebookresearch/slowfast. A simpler,
                   easy to understand and modify version is also available at
                   https://github.com/karttikeya/minREV",
  journal       = "arXiv [cs.CV]",
  publisher     = "arXiv",
  month         =  feb,
  year          =  2023,
  keywords      = "Vision;LLMs",
  archivePrefix = "arXiv",
  eprint        = "2302.04869",
  primaryClass  = "cs.CV",
  arxivid       = "2302.04869",
  doi           = "10.48550/ARXIV.2302.04869"
}

@ARTICLE{Dehghani2023-sf,
  title         = "Scaling vision Transformers to 22 billion parameters",
  author        = "Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and
                   Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and
                   Steiner, Andreas and Caron, Mathilde and Geirhos, Robert and
                   Alabdulmohsin, Ibrahim and Jenatton, Rodolphe and Beyer,
                   Lucas and Tschannen, Michael and Arnab, Anurag and Wang,
                   Xiao and Riquelme, Carlos and Minderer, Matthias and
                   Puigcerver, Joan and Evci, Utku and Kumar, Manoj and van
                   Steenkiste, Sjoerd and Elsayed, Gamaleldin F and Mahendran,
                   Aravindh and Yu, Fisher and Oliver, Avital and Huot, Fantine
                   and Bastings, Jasmijn and Collier, Mark Patrick and
                   Gritsenko, Alexey and Birodkar, Vighnesh and Vasconcelos,
                   Cristina and Tay, Yi and Mensink, Thomas and Kolesnikov,
                   Alexander and Paveti{\'c}, Filip and Tran, Dustin and Kipf,
                   Thomas and Lu{\v c}i{\'c}, Mario and Zhai, Xiaohua and
                   Keysers, Daniel and Harmsen, Jeremiah and Houlsby, Neil",
  abstract      = "The scaling of Transformers has driven breakthrough
                   capabilities for language models. At present, the largest
                   large language models (LLMs) contain upwards of 100B
                   parameters. Vision Transformers (ViT) have introduced the
                   same architecture to image and video modelling, but these
                   have not yet been successfully scaled to nearly the same
                   degree; the largest dense ViT contains 4B parameters (Chen
                   et al., 2022). We present a recipe for highly efficient and
                   stable training of a 22B-parameter ViT (ViT-22B) and perform
                   a wide variety of experiments on the resulting model. When
                   evaluated on downstream tasks (often with a lightweight
                   linear model on frozen features), ViT-22B demonstrates
                   increasing performance with scale. We further observe other
                   interesting benefits of scale, including an improved
                   tradeoff between fairness and performance, state-of-the-art
                   alignment to human visual perception in terms of
                   shape/texture bias, and improved robustness. ViT-22B
                   demonstrates the potential for ``LLM-like'' scaling in
                   vision, and provides key steps towards getting there.",
  journal       = "arXiv [cs.CV]",
  publisher     = "arXiv",
  month         =  feb,
  year          =  2023,
  keywords      = "LLMs;Vision",
  archivePrefix = "arXiv",
  eprint        = "2302.05442",
  primaryClass  = "cs.CV",
  arxivid       = "2302.05442",
  doi           = "10.48550/ARXIV.2302.05442"
}

@ARTICLE{Zhou2023-gy,
  title         = "A comprehensive survey on Pretrained Foundation Models: A
                   history from {BERT} to {ChatGPT}",
  author        = "Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu,
                   Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and
                   Yan, Qiben and He, Lifang and Peng, Hao and Li, Jianxin and
                   Wu, Jia and Liu, Ziwei and Xie, Pengtao and Xiong, Caiming
                   and Pei, Jian and Yu, Philip S and Sun, Lichao",
  abstract      = "The Pretrained Foundation Models (PFMs) are regarded as the
                   foundation for various downstream tasks with different data
                   modalities. A pretrained foundation model, such as BERT,
                   GPT-3, MAE, DALLE-E, and ChatGPT, is trained on large-scale
                   data which provides a reasonable parameter initialization
                   for a wide range of downstream applications. The idea of
                   pretraining behind PFMs plays an important role in the
                   application of large models. Different from previous methods
                   that apply convolution and recurrent modules for feature
                   extractions, the generative pre-training (GPT) method
                   applies Transformer as the feature extractor and is trained
                   on large datasets with an autoregressive paradigm.
                   Similarly, the BERT apples transformers to train on large
                   datasets as a contextual language model. Recently, the
                   ChatGPT shows promising success on large language models,
                   which applies an autoregressive language model with zero
                   shot or few show prompting. With the extraordinary success
                   of PFMs, AI has made waves in a variety of fields over the
                   past few years. Considerable methods, datasets, and
                   evaluation metrics have been proposed in the literature, the
                   need is raising for an updated survey. This study provides a
                   comprehensive review of recent research advancements,
                   current and future challenges, and opportunities for PFMs in
                   text, image, graph, as well as other data modalities. We
                   first review the basic components and existing pretraining
                   in natural language processing, computer vision, and graph
                   learning. We then discuss other advanced PFMs for other data
                   modalities and unified PFMs considering the data quality and
                   quantity. Besides, we discuss relevant research about the
                   fundamentals of the PFM, including model efficiency and
                   compression, security, and privacy. Finally, we lay out key
                   implications, future research directions, challenges, and
                   open problems.",
  journal       = "arXiv [cs.AI]",
  publisher     = "arXiv",
  month         =  feb,
  year          =  2023,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2302.09419",
  primaryClass  = "cs.AI",
  arxivid       = "2302.09419",
  doi           = "10.48550/ARXIV.2302.09419"
}

@ARTICLE{Mialon2023-vk,
  title         = "Augmented Language Models: A survey",
  author        = "Mialon, Gr{\'e}goire and Dess{\`\i}, Roberto and Lomeli,
                   Maria and Nalmpantis, Christoforos and Pasunuru, Ram and
                   Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo
                   and Dwivedi-Yu, Jane and Celikyilmaz, Asli and Grave,
                   Edouard and LeCun, Yann and Scialom, Thomas",
  abstract      = "This survey reviews works in which language models (LMs) are
                   augmented with reasoning skills and the ability to use
                   tools. The former is defined as decomposing a potentially
                   complex task into simpler subtasks while the latter consists
                   in calling external modules such as a code interpreter. LMs
                   can leverage these augmentations separately or in
                   combination via heuristics, or learn to do so from
                   demonstrations. While adhering to a standard missing tokens
                   prediction objective, such augmented LMs can use various,
                   possibly non-parametric external modules to expand their
                   context processing ability, thus departing from the pure
                   language modeling paradigm. We therefore refer to them as
                   Augmented Language Models (ALMs). The missing token
                   objective allows ALMs to learn to reason, use tools, and
                   even act, while still performing standard natural language
                   tasks and even outperforming most regular LMs on several
                   benchmarks. In this work, after reviewing current advance in
                   ALMs, we conclude that this new research direction has the
                   potential to address common limitations of traditional LMs
                   such as interpretability, consistency, and scalability
                   issues.",
  journal       = "arXiv [cs.CL]",
  publisher     = "arXiv",
  month         =  feb,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2302.07842",
  primaryClass  = "cs.CL",
  arxivid       = "2302.07842",
  doi           = "10.48550/ARXIV.2302.07842"
}

@ARTICLE{Bhogale2023-xk,
  title         = "Vistaar: Diverse Benchmarks and Training Sets for Indian
                   Language {ASR}",
  author        = "Bhogale, Kaushal Santosh and Sundaresan, Sai and Raman,
                   Abhigyan and Javed, Tahir and Khapra, Mitesh M and Kumar,
                   Pratyush",
  abstract      = "Improving ASR systems is necessary to make new LLM-based
                   use-cases accessible to people across the globe. In this
                   paper, we focus on Indian languages, and make the case that
                   diverse benchmarks are required to evaluate and improve ASR
                   systems for Indian languages. To address this, we collate
                   Vistaar as a set of 59 benchmarks across various language
                   and domain combinations, on which we evaluate 3 publicly
                   available ASR systems and 2 commercial systems. We also
                   train IndicWhisper models by fine-tuning the Whisper models
                   on publicly available training datasets across 12 Indian
                   languages totalling to 10.7K hours. We show that
                   IndicWhisper significantly improves on considered ASR
                   systems on the Vistaar benchmark. Indeed, IndicWhisper has
                   the lowest WER in 39 out of the 59 benchmarks, with an
                   average reduction of 4.1 WER. We open-source all datasets,
                   code and models.",
  month         =  may,
  year          =  2023,
  keywords      = "Datasets",
  archivePrefix = "arXiv",
  eprint        = "2305.15386",
  primaryClass  = "cs.CL",
  arxivid       = "2305.15386"
}

@ARTICLE{noauthor_undated-wr,
  title         = "{Bhasha-Abhijnaanam}: Native-script and romanized Language
                   Identification for 22 Indic languages",
  author        = "Madhani, Yash and Khapra, Mitesh M and Kunchukuttan, Anoop",
  abstract      = "We create publicly available language identification (LID)
                   datasets and models in all 22 Indian languages listed in the
                   Indian constitution in both native-script and romanized
                   text. First, we create Bhasha-Abhijnaanam, a language
                   identification test set for native-script as well as
                   romanized text which spans all 22 Indic languages. We also
                   train IndicLID, a language identifier for all the
                   above-mentioned languages in both native and romanized
                   script. For native-script text, it has better language
                   coverage than existing LIDs and is competitive or better
                   than other LIDs. IndicLID is the first LID for romanized
                   text in Indian languages. Two major challenges for romanized
                   text LID are the lack of training data and low-LID
                   performance when languages are similar. We provide simple
                   and effective solutions to these problems. In general, there
                   has been limited work on romanized text in any language, and
                   our findings are relevant to other languages that need
                   romanized language identification. Our models are publicly
                   available at https://ai4bharat.iitm.ac.in/indiclid under
                   open-source licenses. Our training and test sets are also
                   publicly available at
                   https://ai4bharat.iitm.ac.in/bhasha-abhijnaanam under
                   open-source licenses.",
  journal       = "arXiv [cs.CL]",
  month         =  may,
  year          =  2023,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2305.15814",
  primaryClass  = "cs.CL",
  arxivid       = "2305.15814"
}

@ARTICLE{noauthor_undated-gj,
  title    = "gpt-4.pdf",
  keywords = "LLMs"
}

@ARTICLE{noauthor_undated-bk,
  title    = "gpt-4-system-card.pdf",
  keywords = "LLMs"
}

@ARTICLE{Huang2019-fx,
  title         = "{ClinicalBERT}: Modeling Clinical Notes and Predicting
                   Hospital Readmission",
  author        = "Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh",
  abstract      = "Clinical notes contain information about patients that goes
                   beyond structured data like lab values and medications.
                   However, clinical notes have been underused relative to
                   structured data, because notes are high-dimensional and
                   sparse. This work develops and evaluates representations of
                   clinical notes using bidirectional transformers
                   (ClinicalBERT). ClinicalBERT uncovers high-quality
                   relationships between medical concepts as judged by humans.
                   ClinicalBert outperforms baselines on 30-day hospital
                   readmission prediction using both discharge summaries and
                   the first few days of notes in the intensive care unit. Code
                   and model parameters are available.",
  month         =  apr,
  year          =  2019,
  keywords      = "Health",
  archivePrefix = "arXiv",
  eprint        = "1904.05342",
  primaryClass  = "cs.CL",
  arxivid       = "1904.05342"
}

@ARTICLE{Geiping2022-as,
  title         = "Cramming: Training a language model on a single {GPU} in one
                   day",
  author        = "Geiping, Jonas and Goldstein, Tom",
  abstract      = "Recent trends in language modeling have focused on
                   increasing performance through scaling, and have resulted in
                   an environment where training language models is out of
                   reach for most researchers and practitioners. While most in
                   the community are asking how to push the limits of extreme
                   computation, we ask the opposite question: How far can we
                   get with a single GPU in just one day? We investigate the
                   downstream performance achievable with a transformer-based
                   language model trained completely from scratch with masked
                   language modeling for a single day on a single consumer GPU.
                   Aside from re-analyzing nearly all components of the
                   pretraining pipeline for this scenario and providing a
                   modified pipeline with performance close to BERT, we
                   investigate why scaling down is hard, and which
                   modifications actually improve performance in this scenario.
                   We provide evidence that even in this constrained setting,
                   performance closely follows scaling laws observed in
                   large-compute settings. Through the lens of scaling laws, we
                   categorize a range of recent improvements to training and
                   architecture and discuss their merit and practical
                   applicability (or lack thereof) for the limited compute
                   setting.",
  month         =  dec,
  year          =  2022,
  keywords      = "LLMs",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2212.14034",
  primaryClass  = "cs.CL",
  arxivid       = "2212.14034"
}

@ARTICLE{Lee2022-se,
  title         = "{Pix2Struct}: Screenshot parsing as pretraining for visual
                   language understanding",
  author        = "Lee, Kenton and Joshi, Mandar and Turc, Iulia and Hu,
                   Hexiang and Liu, Fangyu and Eisenschlos, Julian and
                   Khandelwal, Urvashi and Shaw, Peter and Chang, Ming-Wei and
                   Toutanova, Kristina",
  abstract      = "Visually-situated language is ubiquitous -- sources range
                   from textbooks with diagrams to web pages with images and
                   tables, to mobile apps with buttons and forms. Perhaps due
                   to this diversity, previous work has typically relied on
                   domain-specific recipes with limited sharing of the
                   underlying data, model architectures, and objectives. We
                   present Pix2Struct, a pretrained image-to-text model for
                   purely visual language understanding, which can be finetuned
                   on tasks containing visually-situated language. Pix2Struct
                   is pretrained by learning to parse masked screenshots of web
                   pages into simplified HTML. The web, with its richness of
                   visual elements cleanly reflected in the HTML structure,
                   provides a large source of pretraining data well suited to
                   the diversity of downstream tasks. Intuitively, this
                   objective subsumes common pretraining signals such as OCR,
                   language modeling, image captioning. In addition to the
                   novel pretraining strategy, we introduce a
                   variable-resolution input representation and a more flexible
                   integration of language and vision inputs, where language
                   prompts such as questions are rendered directly on top of
                   the input image. For the first time, we show that a single
                   pretrained model can achieve state-of-the-art results in six
                   out of nine tasks across four domains: documents,
                   illustrations, user interfaces, and natural images.",
  month         =  oct,
  year          =  2022,
  keywords      = "LLMs",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2210.03347",
  primaryClass  = "cs.CL",
  arxivid       = "2210.03347"
}

@ARTICLE{Touvron2023-sy,
  title         = "{LLaMA}: Open and Efficient Foundation Language Models",
  author        = "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and
                   Martinet, Xavier and Lachaux, Marie-Anne and Lacroix,
                   Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and
                   Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and
                   Joulin, Armand and Grave, Edouard and Lample, Guillaume",
  abstract      = "We introduce LLaMA, a collection of foundation language
                   models ranging from 7B to 65B parameters. We train our
                   models on trillions of tokens, and show that it is possible
                   to train state-of-the-art models using publicly available
                   datasets exclusively, without resorting to proprietary and
                   inaccessible datasets. In particular, LLaMA-13B outperforms
                   GPT-3 (175B) on most benchmarks, and LLaMA-65B is
                   competitive with the best models, Chinchilla-70B and
                   PaLM-540B. We release all our models to the research
                   community.",
  month         =  feb,
  year          =  2023,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2302.13971",
  primaryClass  = "cs.CL",
  arxivid       = "2302.13971"
}

@ARTICLE{Lin2023-qy,
  title         = "Towards Healthy {AI}: Large Language Models Need Therapists
                   Too",
  author        = "Lin, Baihan and Bouneffouf, Djallel and Cecchi, Guillermo
                   and Varshney, Kush R",
  abstract      = "Recent advances in large language models (LLMs) have led to
                   the development of powerful AI chatbots capable of engaging
                   in natural and human-like conversations. However, these
                   chatbots can be potentially harmful, exhibiting
                   manipulative, gaslighting, and narcissistic behaviors. We
                   define Healthy AI to be safe, trustworthy and ethical. To
                   create healthy AI systems, we present the SafeguardGPT
                   framework that uses psychotherapy to correct for these
                   harmful behaviors in AI chatbots. The framework involves
                   four types of AI agents: a Chatbot, a ``User,'' a
                   ``Therapist,'' and a ``Critic.'' We demonstrate the
                   effectiveness of SafeguardGPT through a working example of
                   simulating a social conversation. Our results show that the
                   framework can improve the quality of conversations between
                   AI chatbots and humans. Although there are still several
                   challenges and directions to be addressed in the future,
                   SafeguardGPT provides a promising approach to improving the
                   alignment between AI chatbots and human values. By
                   incorporating psychotherapy and reinforcement learning
                   techniques, the framework enables AI chatbots to learn and
                   adapt to human preferences and values in a safe and ethical
                   way, contributing to the development of a more human-centric
                   and responsible AI.",
  month         =  apr,
  year          =  2023,
  keywords      = "Safety/Risks",
  archivePrefix = "arXiv",
  eprint        = "2304.00416",
  primaryClass  = "cs.AI",
  arxivid       = "2304.00416"
}

@ARTICLE{Bai2022-el,
  title         = "Constitutional {AI}: Harmlessness from {AI} Feedback",
  author        = "Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and
                   Askell, Amanda and Kernion, Jackson and Jones, Andy and
                   Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and
                   McKinnon, Cameron and Chen, Carol and Olsson, Catherine and
                   Olah, Christopher and Hernandez, Danny and Drain, Dawn and
                   Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and
                   Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish,
                   Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite,
                   Kamile and Lovitt, Liane and Sellitto, Michael and Elhage,
                   Nelson and Schiefer, Nicholas and Mercado, Noemi and
                   DasSarma, Nova and Lasenby, Robert and Larson, Robin and
                   Ringer, Sam and Johnston, Scott and Kravec, Shauna and El
                   Showk, Sheer and Fort, Stanislav and Lanham, Tamera and
                   Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom
                   and Hume, Tristan and Bowman, Samuel R and Hatfield-Dodds,
                   Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and
                   McCandlish, Sam and Brown, Tom and Kaplan, Jared",
  abstract      = "As AI systems become more capable, we would like to enlist
                   their help to supervise other AIs. We experiment with
                   methods for training a harmless AI assistant through
                   self-improvement, without any human labels identifying
                   harmful outputs. The only human oversight is provided
                   through a list of rules or principles, and so we refer to
                   the method as 'Constitutional AI'. The process involves both
                   a supervised learning and a reinforcement learning phase. In
                   the supervised phase we sample from an initial model, then
                   generate self-critiques and revisions, and then finetune the
                   original model on revised responses. In the RL phase, we
                   sample from the finetuned model, use a model to evaluate
                   which of the two samples is better, and then train a
                   preference model from this dataset of AI preferences. We
                   then train with RL using the preference model as the reward
                   signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a
                   result we are able to train a harmless but non-evasive AI
                   assistant that engages with harmful queries by explaining
                   its objections to them. Both the SL and RL methods can
                   leverage chain-of-thought style reasoning to improve the
                   human-judged performance and transparency of AI decision
                   making. These methods make it possible to control AI
                   behavior more precisely and with far fewer human labels.",
  month         =  dec,
  year          =  2022,
  keywords      = "LLMs;Alignment (RLHF, etc)",
  archivePrefix = "arXiv",
  eprint        = "2212.08073",
  primaryClass  = "cs.CL",
  arxivid       = "2212.08073"
}

@ARTICLE{Caballero2022-wc,
  title         = "Broken Neural Scaling Laws",
  author        = "Caballero, Ethan and Gupta, Kshitij and Rish, Irina and
                   Krueger, David",
  abstract      = "We present a smoothly broken power law functional form
                   (referred to by us as a Broken Neural Scaling Law (BNSL))
                   that accurately models and extrapolates the scaling
                   behaviors of deep neural networks (i.e. how the evaluation
                   metric of interest varies as the amount of compute used for
                   training, number of model parameters, training dataset size,
                   model input size, number of training steps, or upstream
                   performance varies) for various architectures and for each
                   of various tasks within a large and diverse set of upstream
                   and downstream tasks, in zero-shot, prompted, and fine-tuned
                   settings. This set includes large-scale vision, language,
                   audio, video, diffusion, generative modeling, multimodal
                   learning, contrastive learning, AI alignment, robotics,
                   out-of-distribution (OOD) generalization, continual
                   learning, transfer learning, uncertainty estimation /
                   calibration, out-of-distribution detection, adversarial
                   robustness, distillation, sparsity, retrieval, quantization,
                   pruning, fairness, molecules, computer programming/coding,
                   math word problems, ``emergent'' ``phase transitions /
                   changes'', arithmetic, unsupervised/self-supervised
                   learning, \& reinforcement learning (single agent \&
                   multi-agent). When compared to other functional forms for
                   neural scaling behavior, this functional form yields
                   extrapolations of scaling behavior that are considerably
                   more accurate on this set. Moreover, this functional form
                   accurately models \& extrapolates scaling behavior that
                   other functional forms are incapable of expressing such as
                   the non-monotonic transitions present in the scaling
                   behavior of phenomena such as double descent \& the delayed,
                   sharp inflection points present in the scaling behavior of
                   tasks such as arithmetic. Lastly, we use this functional
                   form to glean insights about the limit of the predictability
                   of scaling behavior. Code is available at
                   https://github.com/ethancaballero/broken\_neural\_scaling\_laws",
  month         =  oct,
  year          =  2022,
  keywords      = "Intriguing Properties",
  archivePrefix = "arXiv",
  eprint        = "2210.14891v13",
  primaryClass  = "cs.LG",
  arxivid       = "2210.14891v13"
}

@ARTICLE{Zhou2022-za,
  title         = "Large Language Models Are {Human-Level} Prompt Engineers",
  author        = "Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and
                   Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba,
                   Jimmy",
  abstract      = "By conditioning on natural language instructions, large
                   language models (LLMs) have displayed impressive
                   capabilities as general-purpose computers. However, task
                   performance depends significantly on the quality of the
                   prompt used to steer the model, and most effective prompts
                   have been handcrafted by humans. Inspired by classical
                   program synthesis and the human approach to prompt
                   engineering, we propose Automatic Prompt Engineer (APE) for
                   automatic instruction generation and selection. In our
                   method, we treat the instruction as the ``program,''
                   optimized by searching over a pool of instruction candidates
                   proposed by an LLM in order to maximize a chosen score
                   function. To evaluate the quality of the selected
                   instruction, we evaluate the zero-shot performance of
                   another LLM following the selected instruction. Experiments
                   on 24 NLP tasks show that our automatically generated
                   instructions outperform the prior LLM baseline by a large
                   margin and achieve better or comparable performance to the
                   instructions generated by human annotators on 19/24 tasks.
                   We conduct extensive qualitative and quantitative analyses
                   to explore the performance of APE. We show that
                   APE-engineered prompts can be applied to steer models toward
                   truthfulness and/or informativeness, as well as to improve
                   few-shot learning performance by simply prepending them to
                   standard in-context learning prompts. Please check out our
                   webpage at
                   https://sites.google.com/view/automatic-prompt-engineer.",
  month         =  nov,
  year          =  2022,
  keywords      = "In-Context",
  archivePrefix = "arXiv",
  eprint        = "2211.01910",
  primaryClass  = "cs.LG",
  arxivid       = "2211.01910"
}

@ARTICLE{Biderman2023-is,
  title         = "Emergent and Predictable Memorization in Large Language
                   Models",
  author        = "Biderman, Stella and Prashanth, Usvsn Sai and Sutawika,
                   Lintang and Schoelkopf, Hailey and Anthony, Quentin and
                   Purohit, Shivanshu and Raff, Edward",
  abstract      = "Memorization, or the tendency of large language models
                   (LLMs) to output entire sequences from their training data
                   verbatim, is a key concern for safely deploying language
                   models. In particular, it is vital to minimize a model's
                   memorization of sensitive datapoints such as those
                   containing personal identifiable information (PII). The
                   prevalence of such undesirable memorization can pose issues
                   for model trainers, and may even require discarding an
                   otherwise functional model. We therefore seek to predict
                   which sequences will be memorized before a large model's
                   full train-time by extrapolating the memorization behavior
                   of lower-compute trial runs. We measure memorization of the
                   Pythia model suite and plot scaling laws for forecasting
                   memorization, allowing us to provide equi-compute
                   recommendations to maximize the reliability (recall) of such
                   predictions. We additionally provide further novel
                   discoveries on the distribution of memorization scores
                   across models and data. We release all code and data
                   necessary to reproduce the results in this paper at
                   https://github.com/EleutherAI/pythia",
  month         =  apr,
  year          =  2023,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2304.11158",
  primaryClass  = "cs.CL",
  arxivid       = "2304.11158"
}

@ARTICLE{Oquab2023-gq,
  title         = "{DINOv2}: Learning Robust Visual Features without
                   Supervision",
  author        = "Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni,
                   Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov,
                   Vasil and Fernandez, Pierre and Haziza, Daniel and Massa,
                   Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and
                   Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and
                   Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat,
                   Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu
                   and Jegou, Herv{\'e} and Mairal, Julien and Labatut, Patrick
                   and Joulin, Armand and Bojanowski, Piotr",
  abstract      = "The recent breakthroughs in natural language processing for
                   model pretraining on large quantities of data have opened
                   the way for similar foundation models in computer vision.
                   These models could greatly simplify the use of images in any
                   system by producing all-purpose visual features, i.e.,
                   features that work across image distributions and tasks
                   without finetuning. This work shows that existing
                   pretraining methods, especially self-supervised methods, can
                   produce such features if trained on enough curated data from
                   diverse sources. We revisit existing approaches and combine
                   different techniques to scale our pretraining in terms of
                   data and model size. Most of the technical contributions aim
                   at accelerating and stabilizing the training at scale. In
                   terms of data, we propose an automatic pipeline to build a
                   dedicated, diverse, and curated image dataset instead of
                   uncurated data, as typically done in the self-supervised
                   literature. In terms of models, we train a ViT model
                   (Dosovitskiy et al., 2020) with 1B parameters and distill it
                   into a series of smaller models that surpass the best
                   available all-purpose features, OpenCLIP (Ilharco et al.,
                   2021) on most of the benchmarks at image and pixel levels.",
  month         =  apr,
  year          =  2023,
  keywords      = "Multimodal (Vision, speech, etc)",
  archivePrefix = "arXiv",
  eprint        = "2304.07193",
  primaryClass  = "cs.CV",
  arxivid       = "2304.07193"
}

@ARTICLE{Hu2021-te,
  title         = "{LoRA}: {Low-Rank} Adaptation of large language models",
  author        = "Hu, Edward J and Shen, Yelong and Wallis, Phillip and
                   Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang,
                   Lu and Chen, Weizhu",
  abstract      = "An important paradigm of natural language processing
                   consists of large-scale pre-training on general domain data
                   and adaptation to particular tasks or domains. As we
                   pre-train larger models, full fine-tuning, which retrains
                   all model parameters, becomes less feasible. Using GPT-3
                   175B as an example -- deploying independent instances of
                   fine-tuned models, each with 175B parameters, is
                   prohibitively expensive. We propose Low-Rank Adaptation, or
                   LoRA, which freezes the pre-trained model weights and
                   injects trainable rank decomposition matrices into each
                   layer of the Transformer architecture, greatly reducing the
                   number of trainable parameters for downstream tasks.
                   Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce
                   the number of trainable parameters by 10,000 times and the
                   GPU memory requirement by 3 times. LoRA performs on-par or
                   better than fine-tuning in model quality on RoBERTa,
                   DeBERTa, GPT-2, and GPT-3, despite having fewer trainable
                   parameters, a higher training throughput, and, unlike
                   adapters, no additional inference latency. We also provide
                   an empirical investigation into rank-deficiency in language
                   model adaptation, which sheds light on the efficacy of LoRA.
                   We release a package that facilitates the integration of
                   LoRA with PyTorch models and provide our implementations and
                   model checkpoints for RoBERTa, DeBERTa, and GPT-2 at
                   https://github.com/microsoft/LoRA.",
  month         =  jun,
  year          =  2021,
  keywords      = "Fine-tuning",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2106.09685",
  primaryClass  = "cs.CL",
  arxivid       = "2106.09685"
}

@ARTICLE{Nakano2021-fa,
  title         = "{WebGPT}: Browser-assisted question-answering with human
                   feedback",
  author        = "Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and
                   Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse,
                   Christopher and Jain, Shantanu and Kosaraju, Vineet and
                   Saunders, William and Jiang, Xu and Cobbe, Karl and
                   Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and
                   Knight, Matthew and Chess, Benjamin and Schulman, John",
  abstract      = "We fine-tune GPT-3 to answer long-form questions using a
                   text-based web-browsing environment, which allows the model
                   to search and navigate the web. By setting up the task so
                   that it can be performed by humans, we are able to train
                   models on the task using imitation learning, and then
                   optimize answer quality with human feedback. To make human
                   evaluation of factual accuracy easier, models must collect
                   references while browsing in support of their answers. We
                   train and evaluate our models on ELI5, a dataset of
                   questions asked by Reddit users. Our best model is obtained
                   by fine-tuning GPT-3 using behavior cloning, and then
                   performing rejection sampling against a reward model trained
                   to predict human preferences. This model's answers are
                   preferred by humans 56\% of the time to those of our human
                   demonstrators, and 69\% of the time to the highest-voted
                   answer from Reddit.",
  month         =  dec,
  year          =  2021,
  keywords      = "LLMs;Alignment (RLHF, etc)",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2112.09332",
  primaryClass  = "cs.CL",
  arxivid       = "2112.09332"
}

@ARTICLE{Silva2023-aj,
  title         = "{GPT-4} as an Agronomist Assistant? Answering Agriculture
                   Exams Using Large Language Models",
  author        = "Silva, Bruno and Nunes, Leonardo and Estev{\~a}o, Roberto
                   and Aski, Vijay and Chandra, Ranveer",
  abstract      = "Large language models (LLMs) have demonstrated remarkable
                   capabilities in natural language understanding across
                   various domains, including healthcare and finance. For some
                   tasks, LLMs achieve similar or better performance than
                   trained human beings, therefore it is reasonable to employ
                   human exams (e.g., certification tests) to assess the
                   performance of LLMs. We present a comprehensive evaluation
                   of popular LLMs, such as Llama 2 and GPT, on their ability
                   to answer agriculture-related questions. In our evaluation,
                   we also employ RAG (Retrieval-Augmented Generation) and ER
                   (Ensemble Refinement) techniques, which combine information
                   retrieval, generation capabilities, and prompting strategies
                   to improve the LLMs' performance. To demonstrate the
                   capabilities of LLMs, we selected agriculture exams and
                   benchmark datasets from three of the largest agriculture
                   producer countries: Brazil, India, and the USA. Our analysis
                   highlights GPT-4's ability to achieve a passing score on
                   exams to earn credits for renewing agronomist
                   certifications, answering 93\% of the questions correctly
                   and outperforming earlier general-purpose models, which
                   achieved 88\% accuracy. On one of our experiments, GPT-4
                   obtained the highest performance when compared to human
                   subjects. This performance suggests that GPT-4 could
                   potentially pass on major graduate education admission tests
                   or even earn credits for renewing agronomy certificates. We
                   also explore the models' capacity to address general
                   agriculture-related questions and generate crop management
                   guidelines for Brazilian and Indian farmers, utilizing
                   robust datasets from the Brazilian Agency of Agriculture
                   (Embrapa) and graduate program exams from India. The results
                   suggest that GPT-4, ER, and RAG can contribute meaningfully
                   to agricultural education, assessment, and crop management
                   practice, offering valuable insights to farmers and
                   agricultural professionals.",
  month         =  oct,
  year          =  2023,
  keywords      = "Applications",
  archivePrefix = "arXiv",
  eprint        = "2310.06225",
  primaryClass  = "cs.AI",
  arxivid       = "2310.06225"
}

@ARTICLE{Yang2023-nu,
  title         = "Harnessing the power of {LLMs} in practice: A survey on
                   {ChatGPT} and beyond",
  author        = "Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han,
                   Xiaotian and Feng, Qizhang and Jiang, Haoming and Yin, Bing
                   and Hu, Xia",
  abstract      = "This paper presents a comprehensive and practical guide for
                   practitioners and end-users working with Large Language
                   Models (LLMs) in their downstream natural language
                   processing (NLP) tasks. We provide discussions and insights
                   into the usage of LLMs from the perspectives of models,
                   data, and downstream tasks. Firstly, we offer an
                   introduction and brief summary of current GPT- and
                   BERT-style LLMs. Then, we discuss the influence of
                   pre-training data, training data, and test data. Most
                   importantly, we provide a detailed discussion about the use
                   and non-use cases of large language models for various
                   natural language processing tasks, such as
                   knowledge-intensive tasks, traditional natural language
                   understanding tasks, natural language generation tasks,
                   emergent abilities, and considerations for specific tasks.We
                   present various use cases and non-use cases to illustrate
                   the practical applications and limitations of LLMs in
                   real-world scenarios. We also try to understand the
                   importance of data and the specific challenges associated
                   with each NLP task. Furthermore, we explore the impact of
                   spurious biases on LLMs and delve into other essential
                   considerations, such as efficiency, cost, and latency, to
                   ensure a comprehensive understanding of deploying LLMs in
                   practice. This comprehensive guide aims to provide
                   researchers and practitioners with valuable insights and
                   best practices for working with LLMs, thereby enabling the
                   successful implementation of these models in a wide range of
                   NLP tasks. A curated list of practical guide resources of
                   LLMs, regularly updated, can be found at
                   \textbackslashurl\{https://github.com/Mooler0410/LLMsPracticalGuide\}.",
  month         =  apr,
  year          =  2023,
  keywords      = "Survey/Review Paper;LLMs",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2304.13712",
  primaryClass  = "cs.CL",
  arxivid       = "2304.13712"
}

@ARTICLE{Bertsch2023-mg,
  title         = "Unlimiformer: {Long-Range} Transformers with Unlimited
                   Length Input",
  author        = "Bertsch, Amanda and Alon, Uri and Neubig, Graham and
                   Gormley, Matthew R",
  abstract      = "Transformer-based models typically have a predefined bound
                   to their input length, because of their need to potentially
                   attend to every token in the input. In this work, we propose
                   Unlimiformer: a general approach that can wrap any existing
                   pretrained encoder-decoder transformer, and offload the
                   attention computation across all layers to a single
                   $k$-nearest-neighbor index; this index can be kept on either
                   the GPU or CPU memory and queried in sub-linear time. This
                   way, we can index extremely long input sequences, while
                   every attention head in every decoder layer retrieves its
                   top-$k$ keys, instead of attending to every key. We
                   demonstrate Unlimiformers's efficacy on several
                   long-document and multi-document summarization benchmarks,
                   showing that it can summarize even 350k token-long inputs
                   from the BookSum dataset, without any input truncation at
                   test time. Unlimiformer improves pretrained models such as
                   BART and Longformer by extending them to unlimited inputs
                   without additional learned weights and without modifying
                   their code. We make our code and models publicly available
                   at https://github.com/abertsch72/unlimiformer .",
  month         =  may,
  year          =  2023,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2305.01625",
  primaryClass  = "cs.CL",
  arxivid       = "2305.01625"
}

@ARTICLE{Shinn2023-gj,
  title         = "Reflexion: an autonomous agent with dynamic memory and
                   self-reflection",
  author        = "Shinn, Noah and Labash, Beck and Gopinath, Ashwin",
  abstract      = "Recent advancements in decision-making large language model
                   (LLM) agents have demonstrated impressive performance across
                   various benchmarks. However, these state-of-the-art
                   approaches typically necessitate internal model fine-tuning,
                   external model fine-tuning, or policy optimization over a
                   defined state space. Implementing these methods can prove
                   challenging due to the scarcity of high-quality training
                   data or the lack of well-defined state space. Moreover,
                   these agents do not possess certain qualities inherent to
                   human decision-making processes, specifically the ability to
                   learn from mistakes. Self-reflection allows humans to
                   efficiently solve novel problems through a process of trial
                   and error. Building on recent research, we propose
                   Reflexion, an approach that endows an agent with dynamic
                   memory and self-reflection capabilities to enhance its
                   existing reasoning trace and task-specific action choice
                   abilities. To achieve full automation, we introduce a
                   straightforward yet effective heuristic that enables the
                   agent to pinpoint hallucination instances, avoid repetition
                   in action sequences, and, in some environments, construct an
                   internal memory map of the given environment. To assess our
                   approach, we evaluate the agent's ability to complete
                   decision-making tasks in AlfWorld environments and
                   knowledge-intensive, search-based question-and-answer tasks
                   in HotPotQA environments. We observe success rates of 97\%
                   and 51\%, respectively, and provide a discussion on the
                   emergent property of self-reflection.",
  month         =  mar,
  year          =  2023,
  keywords      = "Readling List;LLMs",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2303.11366",
  primaryClass  = "cs.AI",
  arxivid       = "2303.11366"
}

@ARTICLE{Turpin2023-jl,
  title         = "Language Models Don't Always Say What They Think: Unfaithful
                   Explanations in {Chain-of-Thought} Prompting",
  author        = "Turpin, Miles and Michael, Julian and Perez, Ethan and
                   Bowman, Samuel R",
  abstract      = "Large Language Models (LLMs) can achieve strong performance
                   on many tasks by producing step-by-step reasoning before
                   giving a final output, often referred to as chain-of-thought
                   reasoning (CoT). It is tempting to interpret these CoT
                   explanations as the LLM's process for solving a task.
                   However, we find that CoT explanations can systematically
                   misrepresent the true reason for a model's prediction. We
                   demonstrate that CoT explanations can be heavily influenced
                   by adding biasing features to model inputs -- e.g., by
                   reordering the multiple-choice options in a few-shot prompt
                   to make the answer always ``(A)'' -- which models
                   systematically fail to mention in their explanations. When
                   we bias models toward incorrect answers, they frequently
                   generate CoT explanations supporting those answers. This
                   causes accuracy to drop by as much as 36\% on a suite of 13
                   tasks from BIG-Bench Hard, when testing with GPT-3.5 from
                   OpenAI and Claude 1.0 from Anthropic. On a social-bias task,
                   model explanations justify giving answers in line with
                   stereotypes without mentioning the influence of these social
                   biases. Our findings indicate that CoT explanations can be
                   plausible yet misleading, which risks increasing our trust
                   in LLMs without guaranteeing their safety. CoT is promising
                   for explainability, but our results highlight the need for
                   targeted efforts to evaluate and improve explanation
                   faithfulness.",
  month         =  may,
  year          =  2023,
  keywords      = "Hallucination",
  archivePrefix = "arXiv",
  eprint        = "2305.04388",
  primaryClass  = "cs.CL",
  arxivid       = "2305.04388"
}

@ARTICLE{Zou2023-mz,
  title         = "Representation Engineering: A {Top-Down} Approach to {AI}
                   Transparency",
  author        = "Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James
                   and Guo, Phillip and Ren, Richard and Pan, Alexander and
                   Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin
                   and Goel, Shashwat and Li, Nathaniel and Byun, Michael J and
                   Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo,
                   Sanmi and Song, Dawn and Fredrikson, Matt and Zico Kolter, J
                   and Hendrycks, Dan",
  abstract      = "In this paper, we identify and characterize the emerging
                   area of representation engineering (RepE), an approach to
                   enhancing the transparency of AI systems that draws on
                   insights from cognitive neuroscience. RepE places
                   population-level representations, rather than neurons or
                   circuits, at the center of analysis, equipping us with novel
                   methods for monitoring and manipulating high-level cognitive
                   phenomena in deep neural networks (DNNs). We provide
                   baselines and an initial analysis of RepE techniques,
                   showing that they offer simple yet effective solutions for
                   improving our understanding and control of large language
                   models. We showcase how these methods can provide traction
                   on a wide range of safety-relevant problems, including
                   honesty, harmlessness, power-seeking, and more,
                   demonstrating the promise of top-down transparency research.
                   We hope that this work catalyzes further exploration of RepE
                   and fosters advancements in the transparency and safety of
                   AI systems.",
  month         =  oct,
  year          =  2023,
  keywords      = "Safety/Risks;Intriguing Properties",
  archivePrefix = "arXiv",
  eprint        = "2310.01405",
  primaryClass  = "cs.LG",
  arxivid       = "2310.01405"
}

@ARTICLE{Lee2023-xm,
  title         = "Teaching Arithmetic to Small Transformers",
  author        = "Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason D and
                   Lee, Kangwook and Papailiopoulos, Dimitris",
  abstract      = "Large language models like GPT-4 exhibit emergent
                   capabilities across general-purpose tasks, such as basic
                   arithmetic, when trained on extensive text data, even though
                   these tasks are not explicitly encoded by the unsupervised,
                   next-token prediction objective. This study investigates how
                   small transformers, trained from random initialization, can
                   efficiently learn arithmetic operations such as addition,
                   multiplication, and elementary functions like square root,
                   using the next-token prediction objective. We first
                   demonstrate that conventional training data is not the most
                   effective for arithmetic learning, and simple formatting
                   changes can significantly improve accuracy. This leads to
                   sharp phase transitions as a function of training data
                   scale, which, in some cases, can be explained through
                   connections to low-rank matrix completion. Building on prior
                   work, we then train on chain-of-thought style data that
                   includes intermediate step results. Even in the complete
                   absence of pretraining, this approach significantly and
                   simultaneously improves accuracy, sample complexity, and
                   convergence speed. We also study the interplay between
                   arithmetic and text data during training and examine the
                   effects of few-shot prompting, pretraining, and model scale.
                   Additionally, we discuss length generalization challenges.
                   Our work highlights the importance of high-quality,
                   instructive data that considers the particular
                   characteristics of the next-word prediction objective for
                   rapidly eliciting arithmetic capabilities.",
  month         =  jul,
  year          =  2023,
  keywords      = "Understanding",
  archivePrefix = "arXiv",
  eprint        = "2307.03381",
  primaryClass  = "cs.LG",
  arxivid       = "2307.03381"
}

@ARTICLE{Dong2021-vo,
  title         = "Attention is not all you need: Pure attention loses rank
                   doubly exponentially with depth",
  author        = "Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas",
  abstract      = "Attention-based architectures have become ubiquitous in
                   machine learning, yet our understanding of the reasons for
                   their effectiveness remains limited. This work proposes a
                   new way to understand self-attention networks: we show that
                   their output can be decomposed into a sum of smaller terms,
                   each involving the operation of a sequence of attention
                   heads across layers. Using this decomposition, we prove that
                   self-attention possesses a strong inductive bias towards
                   ``token uniformity''. Specifically, without skip connections
                   or multi-layer perceptrons (MLPs), the output converges
                   doubly exponentially to a rank-1 matrix. On the other hand,
                   skip connections and MLPs stop the output from degeneration.
                   Our experiments verify the identified convergence phenomena
                   on different variants of standard transformer architectures.",
  month         =  mar,
  year          =  2021,
  keywords      = "LLMs",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2103.03404",
  primaryClass  = "cs.LG",
  arxivid       = "2103.03404"
}

@ARTICLE{Dosovitskiy2020-ao,
  title         = "An image is worth 16x16 words: Transformers for image
                   recognition at scale",
  author        = "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov,
                   Alexander and Weissenborn, Dirk and Zhai, Xiaohua and
                   Unterthiner, Thomas and Dehghani, Mostafa and Minderer,
                   Matthias and Heigold, Georg and Gelly, Sylvain and
                   Uszkoreit, Jakob and Houlsby, Neil",
  abstract      = "While the Transformer architecture has become the de-facto
                   standard for natural language processing tasks, its
                   applications to computer vision remain limited. In vision,
                   attention is either applied in conjunction with
                   convolutional networks, or used to replace certain
                   components of convolutional networks while keeping their
                   overall structure in place. We show that this reliance on
                   CNNs is not necessary and a pure transformer applied
                   directly to sequences of image patches can perform very well
                   on image classification tasks. When pre-trained on large
                   amounts of data and transferred to multiple mid-sized or
                   small image recognition benchmarks (ImageNet, CIFAR-100,
                   VTAB, etc.), Vision Transformer (ViT) attains excellent
                   results compared to state-of-the-art convolutional networks
                   while requiring substantially fewer computational resources
                   to train.",
  month         =  oct,
  year          =  2020,
  keywords      = "LLMs;Vision",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2010.11929",
  primaryClass  = "cs.CV",
  arxivid       = "2010.11929"
}

@INPROCEEDINGS{Weidinger2022-qg,
  title     = "Taxonomy of Risks posed by Language Models",
  booktitle = "2022 {ACM} Conference on Fairness, Accountability, and
               Transparency",
  author    = "Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and
               Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese,
               Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa
               and Biles, Courtney and Brown, Sasha and Kenton, Zac and
               Hawkins, Will and Stepleton, Tom and Birhane, Abeba and
               Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and
               Haas, Julia and Legassick, Sean and Irving, Geoffrey and
               Gabriel, Iason",
  abstract  = "Responsible innovation on large-scale Language Models (LMs)
               requires foresight into and in-depth understanding of the risks
               these models may pose. This paper develops a comprehensive
               taxonomy of ethical and social risks associated with LMs. We
               identify twenty-one risks, drawing on expertise and literature
               from computer science, linguistics, and the social sciences. We
               situate these risks in our taxonomy of six risk areas: I.
               Discrimination, Hate speech and Exclusion, II. Information
               Hazards, III. Misinformation Harms, IV. Malicious Uses, V.
               Human-Computer Interaction Harms, and VI. Environmental and
               Socioeconomic harms. For risks that have already been observed
               in LMs, the causal mechanism leading to harm, evidence of the
               risk, and approaches to risk mitigation are discussed. We
               further describe and analyse risks that have not yet been
               observed but are anticipated based on assessments of other
               language technologies, and situate these in the same taxonomy.
               We underscore that it is the responsibility of organizations to
               engage with the mitigations we discuss throughout the paper. We
               close by highlighting challenges and directions for further
               research on risk evaluation and mitigation with the goal of
               ensuring that language models are developed responsibly.",
  publisher = "Association for Computing Machinery",
  pages     = "214--229",
  series    = "FAccT '22",
  month     =  jun,
  year      =  2022,
  address   = "New York, NY, USA",
  keywords  = "responsible AI, risk assessment, technology risks, language
               models, responsible innovation;Safety/Risks",
  location  = "Seoul, Republic of Korea",
  isbn      = "9781450393522",
  doi       = "10.1145/3531146.3533088"
}

@ARTICLE{Yao2022-yv,
  title         = "{ReAct}: Synergizing Reasoning and Acting in Language Models",
  author        = "Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and
                   Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan",
  abstract      = "While large language models (LLMs) have demonstrated
                   impressive capabilities across tasks in language
                   understanding and interactive decision making, their
                   abilities for reasoning (e.g. chain-of-thought prompting)
                   and acting (e.g. action plan generation) have primarily been
                   studied as separate topics. In this paper, we explore the
                   use of LLMs to generate both reasoning traces and
                   task-specific actions in an interleaved manner, allowing for
                   greater synergy between the two: reasoning traces help the
                   model induce, track, and update action plans as well as
                   handle exceptions, while actions allow it to interface with
                   external sources, such as knowledge bases or environments,
                   to gather additional information. We apply our approach,
                   named ReAct, to a diverse set of language and decision
                   making tasks and demonstrate its effectiveness over
                   state-of-the-art baselines, as well as improved human
                   interpretability and trustworthiness over methods without
                   reasoning or acting components. Concretely, on question
                   answering (HotpotQA) and fact verification (Fever), ReAct
                   overcomes issues of hallucination and error propagation
                   prevalent in chain-of-thought reasoning by interacting with
                   a simple Wikipedia API, and generates human-like
                   task-solving trajectories that are more interpretable than
                   baselines without reasoning traces. On two interactive
                   decision making benchmarks (ALFWorld and WebShop), ReAct
                   outperforms imitation and reinforcement learning methods by
                   an absolute success rate of 34\% and 10\% respectively,
                   while being prompted with only one or two in-context
                   examples. Project site with code: https://react-lm.github.io",
  month         =  oct,
  year          =  2022,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2210.03629",
  primaryClass  = "cs.CL",
  arxivid       = "2210.03629"
}

@ARTICLE{Yao2023-xj,
  title         = "Tree of Thoughts: Deliberate Problem Solving with Large
                   Language Models",
  author        = "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran,
                   Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan,
                   Karthik",
  abstract      = "Language models are increasingly being deployed for general
                   problem solving across a wide range of tasks, but are still
                   confined to token-level, left-to-right decision-making
                   processes during inference. This means they can fall short
                   in tasks that require exploration, strategic lookahead, or
                   where initial decisions play a pivotal role. To surmount
                   these challenges, we introduce a new framework for language
                   model inference, Tree of Thoughts (ToT), which generalizes
                   over the popular Chain of Thought approach to prompting
                   language models, and enables exploration over coherent units
                   of text (thoughts) that serve as intermediate steps toward
                   problem solving. ToT allows LMs to perform deliberate
                   decision making by considering multiple different reasoning
                   paths and self-evaluating choices to decide the next course
                   of action, as well as looking ahead or backtracking when
                   necessary to make global choices. Our experiments show that
                   ToT significantly enhances language models' problem-solving
                   abilities on three novel tasks requiring non-trivial
                   planning or search: Game of 24, Creative Writing, and Mini
                   Crosswords. For instance, in Game of 24, while GPT-4 with
                   chain-of-thought prompting only solved 4\% of tasks, our
                   method achieved a success rate of 74\%. Code repo with all
                   prompts: https://github.com/ysymyth/tree-of-thought-llm.",
  month         =  may,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2305.10601",
  primaryClass  = "cs.CL",
  arxivid       = "2305.10601"
}

@ARTICLE{Zhou2023-ua,
  title         = "{LIMA}: Less is more for alignment",
  author        = "Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer,
                   Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and
                   Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and
                   Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy,
                   Omer",
  abstract      = "Large language models are trained in two stages: (1)
                   unsupervised pretraining from raw text, to learn
                   general-purpose representations, and (2) large scale
                   instruction tuning and reinforcement learning, to better
                   align to end tasks and user preferences. We measure the
                   relative importance of these two stages by training LIMA, a
                   65B parameter LLaMa language model fine-tuned with the
                   standard supervised loss on only 1,000 carefully curated
                   prompts and responses, without any reinforcement learning or
                   human preference modeling. LIMA demonstrates remarkably
                   strong performance, learning to follow specific response
                   formats from only a handful of examples in the training
                   data, including complex queries that range from planning
                   trip itineraries to speculating about alternate history.
                   Moreover, the model tends to generalize well to unseen tasks
                   that did not appear in the training data. In a controlled
                   human study, responses from LIMA are either equivalent or
                   strictly preferred to GPT-4 in 43\% of cases; this statistic
                   is as high as 58\% when compared to Bard and 65\% versus
                   DaVinci003, which was trained with human feedback. Taken
                   together, these results strongly suggest that almost all
                   knowledge in large language models is learned during
                   pretraining, and only limited instruction tuning data is
                   necessary to teach models to produce high quality output.",
  month         =  may,
  year          =  2023,
  keywords      = "LLMs;Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "2305.11206",
  primaryClass  = "cs.CL",
  arxivid       = "2305.11206"
}

@ARTICLE{Singhal2023-dr,
  title         = "Towards {Expert-Level} Medical Question Answering with Large
                   Language Models",
  author        = "Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres,
                   Rory and Wulczyn, Ellery and Hou, Le and Clark, Kevin and
                   Pfohl, Stephen and Cole-Lewis, Heather and Neal, Darlene and
                   Schaekermann, Mike and Wang, Amy and Amin, Mohamed and
                   Lachgar, Sami and Mansfield, Philip and Prakash, Sushant and
                   Green, Bradley and Dominowska, Ewa and Aguera y Arcas,
                   Blaise and Tomasev, Nenad and Liu, Yun and Wong, Renee and
                   Semturs, Christopher and Sara Mahdavi, S and Barral, Joelle
                   and Webster, Dale and Corrado, Greg S and Matias, Yossi and
                   Azizi, Shekoofeh and Karthikesalingam, Alan and Natarajan,
                   Vivek",
  abstract      = "Recent artificial intelligence (AI) systems have reached
                   milestones in ``grand challenges'' ranging from Go to
                   protein-folding. The capability to retrieve medical
                   knowledge, reason over it, and answer medical questions
                   comparably to physicians has long been viewed as one such
                   grand challenge. Large language models (LLMs) have catalyzed
                   significant progress in medical question answering; Med-PaLM
                   was the first model to exceed a ``passing'' score in US
                   Medical Licensing Examination (USMLE) style questions with a
                   score of 67.2\% on the MedQA dataset. However, this and
                   other prior work suggested significant room for improvement,
                   especially when models' answers were compared to clinicians'
                   answers. Here we present Med-PaLM 2, which bridges these
                   gaps by leveraging a combination of base LLM improvements
                   (PaLM 2), medical domain finetuning, and prompting
                   strategies including a novel ensemble refinement approach.
                   Med-PaLM 2 scored up to 86.5\% on the MedQA dataset,
                   improving upon Med-PaLM by over 19\% and setting a new
                   state-of-the-art. We also observed performance approaching
                   or exceeding state-of-the-art across MedMCQA, PubMedQA, and
                   MMLU clinical topics datasets. We performed detailed human
                   evaluations on long-form questions along multiple axes
                   relevant to clinical applications. In pairwise comparative
                   ranking of 1066 consumer medical questions, physicians
                   preferred Med-PaLM 2 answers to those produced by physicians
                   on eight of nine axes pertaining to clinical utility (p <
                   0.001). We also observed significant improvements compared
                   to Med-PaLM on every evaluation axis (p < 0.001) on newly
                   introduced datasets of 240 long-form ``adversarial''
                   questions to probe LLM limitations. While further studies
                   are necessary to validate the efficacy of these models in
                   real-world settings, these results highlight rapid progress
                   towards physician-level performance in medical question
                   answering.",
  month         =  may,
  year          =  2023,
  keywords      = "Health",
  archivePrefix = "arXiv",
  eprint        = "2305.09617",
  primaryClass  = "cs.CL",
  arxivid       = "2305.09617"
}

@ARTICLE{Cai2023-am,
  title         = "Large Language Models as Tool Makers",
  author        = "Cai, Tianle and Wang, Xuezhi and Ma, Tengyu and Chen, Xinyun
                   and Zhou, Denny",
  abstract      = "Recent research shows the potential of enhancing the
                   problem-solving ability of large language models (LLMs)
                   through the use of external tools. However, prior work along
                   this line depends on the availability of existing tools. In
                   this work, we take an initial step towards removing this
                   dependency by proposing a closed-loop framework, referred to
                   as LLMs As Tool Makers (LATM), where LLMs create their own
                   reusable tools for problem-solving. Our approach consists of
                   two key phases: 1) tool making: an LLM acts as the tool
                   maker that crafts tools for given tasks, where a tool is
                   implemented as a Python utility function. 2) tool using: an
                   LLM acts as the tool user, which applies the tool built by
                   the tool maker for problem-solving. The tool user can be
                   either the same or a different LLM from the tool maker.
                   Tool-making enables an LLM to continually generate tools
                   that can be applied to different requests so that future
                   requests can call the corresponding APIs when beneficial for
                   solving the tasks. Furthermore, the division of labor among
                   LLMs for tool-making and tool-using phases introduces the
                   opportunity to achieve cost effectiveness without degrading
                   the quality of generated tools and problem solutions. For
                   example, recognizing that tool-making demands more
                   sophisticated capabilities than tool-using, we can apply a
                   powerful yet resource-intensive model as the tool maker, and
                   a lightweight while cost-effective model as the tool user.
                   We validate the effectiveness of our approach across a
                   variety of complex reasoning tasks, including Big-Bench
                   tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool
                   user, LATM can achieve performance that is on par with using
                   GPT-4 for both tool making and tool using, while the
                   inference cost is significantly reduced.",
  month         =  may,
  year          =  2023,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2305.17126",
  primaryClass  = "cs.LG",
  arxivid       = "2305.17126"
}

@ARTICLE{Yang2023-lk,
  title         = "The Dawn of {LMMs}: Preliminary Explorations with
                   {GPT-4V(ision})",
  author        = "Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang,
                   Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang,
                   Lijuan",
  abstract      = "Large multimodal models (LMMs) extend large language models
                   (LLMs) with multi-sensory skills, such as visual
                   understanding, to achieve stronger generic intelligence. In
                   this paper, we analyze the latest model, GPT-4V(ision), to
                   deepen the understanding of LMMs. The analysis focuses on
                   the intriguing tasks that GPT-4V can perform, containing
                   test samples to probe the quality and genericity of GPT-4V's
                   capabilities, its supported inputs and working modes, and
                   the effective ways to prompt the model. In our approach to
                   exploring GPT-4V, we curate and organize a collection of
                   carefully designed qualitative samples spanning a variety of
                   domains and tasks. Observations from these samples
                   demonstrate that GPT-4V's unprecedented ability in
                   processing arbitrarily interleaved multimodal inputs and the
                   genericity of its capabilities together make GPT-4V a
                   powerful multimodal generalist system. Furthermore, GPT-4V's
                   unique capability of understanding visual markers drawn on
                   input images can give rise to new human-computer interaction
                   methods such as visual referring prompting. We conclude the
                   report with in-depth discussions on the emerging application
                   scenarios and the future research directions for
                   GPT-4V-based systems. We hope that this preliminary
                   exploration will inspire future research on the
                   next-generation multimodal task formulation, new ways to
                   exploit and enhance LMMs to solve real-world problems, and
                   gaining better understanding of multimodal foundation
                   models.",
  month         =  sep,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2309.17421",
  primaryClass  = "cs.CV",
  arxivid       = "2309.17421"
}

@ARTICLE{Pan_undated-qs,
  title    = "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
  author   = "Pan, Shirui and {Senior Member} and Luo, Linhao and Wang, Yufei
              and Chen, Chen and Wang, Jiapu and Wu, Xindong",
  keywords = "Survey",
  arxivid  = "2306.08302"
}

@ARTICLE{Hendrycks2023-hq,
  title         = "An overview of catastrophic {AI} risks",
  author        = "Hendrycks, Dan and Mazeika, Mantas and Woodside, Thomas",
  abstract      = "Rapid advancements in artificial intelligence (AI) have
                   sparked growing concerns among experts, policymakers, and
                   world leaders regarding the potential for increasingly
                   advanced AI systems to pose catastrophic risks. Although
                   numerous risks have been detailed separately, there is a
                   pressing need for a systematic discussion and illustration
                   of the potential dangers to better inform efforts to
                   mitigate them. This paper provides an overview of the main
                   sources of catastrophic AI risks, which we organize into
                   four categories: malicious use, in which individuals or
                   groups intentionally use AIs to cause harm; AI race, in
                   which competitive environments compel actors to deploy
                   unsafe AIs or cede control to AIs; organizational risks,
                   highlighting how human factors and complex systems can
                   increase the chances of catastrophic accidents; and rogue
                   AIs, describing the inherent difficulty in controlling
                   agents far more intelligent than humans. For each category
                   of risk, we describe specific hazards, present illustrative
                   stories, envision ideal scenarios, and propose practical
                   suggestions for mitigating these dangers. Our goal is to
                   foster a comprehensive understanding of these risks and
                   inspire collective and proactive efforts to ensure that AIs
                   are developed and deployed in a safe manner. Ultimately, we
                   hope this will allow us to realize the benefits of this
                   powerful technology while minimizing the potential for
                   catastrophic outcomes.",
  month         =  jun,
  year          =  2023,
  keywords      = "Safety/Risks",
  archivePrefix = "arXiv",
  eprint        = "2306.12001",
  primaryClass  = "cs.CY",
  arxivid       = "2306.12001"
}

@ARTICLE{Deletang2023-qy,
  title         = "Language modeling is compression",
  author        = "Del{\'e}tang, Gr{\'e}goire and Ruoss, Anian and Duquenne,
                   Paul-Ambroise and Catt, Elliot and Genewein, Tim and
                   Mattern, Christopher and Grau-Moya, Jordi and Wenliang, Li
                   Kevin and Aitchison, Matthew and Orseau, Laurent and Hutter,
                   Marcus and Veness, Joel",
  abstract      = "It has long been established that predictive models can be
                   transformed into lossless compressors and vice versa.
                   Incidentally, in recent years, the machine learning
                   community has focused on training increasingly large and
                   powerful self-supervised (language) models. Since these
                   large language models exhibit impressive predictive
                   capabilities, they are well-positioned to be strong
                   compressors. In this work, we advocate for viewing the
                   prediction problem through the lens of compression and
                   evaluate the compression capabilities of large (foundation)
                   models. We show that large language models are powerful
                   general-purpose predictors and that the compression
                   viewpoint provides novel insights into scaling laws,
                   tokenization, and in-context learning. For example,
                   Chinchilla 70B, while trained primarily on text, compresses
                   ImageNet patches to 43.4\% and LibriSpeech samples to 16.4\%
                   of their raw size, beating domain-specific compressors like
                   PNG (58.5\%) or FLAC (30.3\%), respectively. Finally, we
                   show that the prediction-compression equivalence allows us
                   to use any compressor (like gzip) to build a conditional
                   generative model.",
  month         =  sep,
  year          =  2023,
  keywords      = "LLMs;Understanding",
  archivePrefix = "arXiv",
  eprint        = "2309.10668",
  primaryClass  = "cs.LG",
  arxivid       = "2309.10668"
}

@ARTICLE{Singhal2022-ln,
  title         = "Large Language Models Encode Clinical Knowledge",
  author        = "Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Sara
                   Mahdavi, S and Wei, Jason and Chung, Hyung Won and Scales,
                   Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl,
                   Stephen and Payne, Perry and Seneviratne, Martin and Gamble,
                   Paul and Kelly, Chris and Scharli, Nathaneal and Chowdhery,
                   Aakanksha and Mansfield, Philip and Aguera y Arcas, Blaise
                   and Webster, Dale and Corrado, Greg S and Matias, Yossi and
                   Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and
                   Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs,
                   Christopher and Karthikesalingam, Alan and Natarajan, Vivek",
  abstract      = "Large language models (LLMs) have demonstrated impressive
                   capabilities in natural language understanding and
                   generation, but the quality bar for medical and clinical
                   applications is high. Today, attempts to assess models'
                   clinical knowledge typically rely on automated evaluations
                   on limited benchmarks. There is no standard to evaluate
                   model predictions and reasoning across a breadth of tasks.
                   To address this, we present MultiMedQA, a benchmark
                   combining six existing open question answering datasets
                   spanning professional medical exams, research, and consumer
                   queries; and HealthSearchQA, a new free-response dataset of
                   medical questions searched online. We propose a framework
                   for human evaluation of model answers along multiple axes
                   including factuality, precision, possible harm, and bias. In
                   addition, we evaluate PaLM (a 540-billion parameter LLM) and
                   its instruction-tuned variant, Flan-PaLM, on MultiMedQA.
                   Using a combination of prompting strategies, Flan-PaLM
                   achieves state-of-the-art accuracy on every MultiMedQA
                   multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU
                   clinical topics), including 67.6\% accuracy on MedQA (US
                   Medical License Exam questions), surpassing prior
                   state-of-the-art by over 17\%. However, human evaluation
                   reveals key gaps in Flan-PaLM responses. To resolve this we
                   introduce instruction prompt tuning, a parameter-efficient
                   approach for aligning LLMs to new domains using a few
                   exemplars. The resulting model, Med-PaLM, performs
                   encouragingly, but remains inferior to clinicians. We show
                   that comprehension, recall of knowledge, and medical
                   reasoning improve with model scale and instruction prompt
                   tuning, suggesting the potential utility of LLMs in
                   medicine. Our human evaluations reveal important limitations
                   of today's models, reinforcing the importance of both
                   evaluation frameworks and method development in creating
                   safe, helpful LLM models for clinical applications.",
  month         =  dec,
  year          =  2022,
  keywords      = "Applications",
  archivePrefix = "arXiv",
  eprint        = "2212.13138",
  primaryClass  = "cs.CL",
  arxivid       = "2212.13138"
}

@ARTICLE{Bhatia2023-sh,
  title         = "{TART}: A plug-and-play Transformer module for task-agnostic
                   reasoning",
  author        = "Bhatia, Kush and Narayan, Avanika and De Sa, Christopher and
                   R{\'e}, Christopher",
  abstract      = "Large language models (LLMs) exhibit in-context learning
                   abilities which enable the same model to perform several
                   tasks without any task-specific training. In contrast,
                   traditional adaptation approaches, such as fine-tuning,
                   modify the underlying models for each specific task.
                   In-context learning, however, consistently underperforms
                   task-specific tuning approaches even when presented with the
                   same examples. While most existing approaches (e.g., prompt
                   engineering) focus on the LLM's learned representations to
                   patch this performance gap, our analysis actually reveal
                   that LLM representations contain sufficient information to
                   make good predictions. As such, we focus on the LLM's
                   reasoning abilities and demonstrate that this performance
                   gap exists due to their inability to perform simple
                   probabilistic reasoning tasks. This raises an intriguing
                   question: Are LLMs actually capable of learning how to
                   reason in a task-agnostic manner? We answer this in the
                   affirmative and propose TART which generically improves an
                   LLM's reasoning abilities using a synthetically trained
                   Transformer-based reasoning module. TART trains this
                   reasoning module in a task-agnostic manner using only
                   synthetic logistic regression tasks and composes it with an
                   arbitrary real-world pre-trained model without any
                   additional training. With a single inference module, TART
                   improves performance across different model families
                   (GPT-Neo, Pythia, BLOOM), model sizes (100M - 6B), tasks (14
                   NLP binary classification tasks), and even across different
                   modalities (audio and vision). Additionally, on the RAFT
                   Benchmark, TART improves GPT-Neo (125M)'s performance such
                   that it outperforms BLOOM (176B), and is within 4\% of GPT-3
                   (175B). Our code and models are available at
                   https://github.com/HazyResearch/TART .",
  month         =  jun,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2306.07536",
  primaryClass  = "cs.LG",
  arxivid       = "2306.07536"
}

@ARTICLE{BigScience_Workshop2022-ee,
  title         = "{BLOOM}: A {176B-Parameter} {Open-Access} Multilingual
                   Language Model",
  author        = "{BigScience Workshop} and {:} and Le Scao, Teven and Fan,
                   Angela and Akiki, Christopher and Pavlick, Ellie and
                   Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman
                   and Luccioni, Alexandra Sasha and Yvon, Fran{\c c}ois and
                   Gall{\'e}, Matthias and Tow, Jonathan and Rush, Alexander M
                   and Biderman, Stella and Webson, Albert and Ammanamanchi,
                   Pawan Sasanka and Wang, Thomas and Sagot, Beno{\^\i}t and
                   Muennighoff, Niklas and del Moral, Albert Villanova and
                   Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and
                   McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and
                   Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and
                   Sanh, Victor and Lauren{\c c}on, Hugo and Jernite, Yacine
                   and Launay, Julien and Mitchell, Margaret and Raffel, Colin
                   and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji,
                   Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav,
                   Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue,
                   Chris and Klamm, Christopher and Leong, Colin and van
                   Strien, Daniel and Adelani, David Ifeoluwa and Radev,
                   Dragomir and Ponferrada, Eduardo Gonz{\'a}lez and Levkovizh,
                   Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni,
                   Francesco and Dupont, G{\'e}rard and Kruszewski, Germ{\'a}n
                   and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza
                   and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and
                   Johnson, Isaac and Gonzalez-Dios, Itziar and de la Rosa,
                   Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and
                   Chang, Jonathan and Frohberg, J{\"o}rg and Tobing, Joseph
                   and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen,
                   Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon
                   and Phan, Long and Ben allal, Loubna and Tanguy, Ludovic and
                   Dey, Manan and Mu{\~n}oz, Manuel Romero and Masoud, Maraim
                   and Grandury, Mar{\'\i}a and {\v S}a{\v s}ko, Mario and
                   Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang,
                   Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A and
                   Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and
                   Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar
                   and de Gibert, Ona and Villegas, Paulo and Henderson, Peter
                   and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin
                   and Harliman, Rheza and Bommasani, Rishi and L{\'o}pez,
                   Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo,
                   Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad,
                   Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne
                   and Nikpoor, Somaieh and Silberberg, Stanislav and Pai,
                   Suhas and Zink, Sydney and Torrent, Tiago Timponi and
                   Schick, Timo and Thrush, Tristan and Danchev, Valentin and
                   Nikoulina, Vassilina and Laippala, Veronika and Lepercq,
                   Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat,
                   Zeerak and Raja, Arun and Heinzerling, Benjamin and Si,
                   Chenglei and Ta{\c s}ar, Davut Emre and Salesky, Elizabeth
                   and Mielke, Sabrina J and Lee, Wilson Y and Sharma, Abheesht
                   and Santilli, Andrea and Chaffin, Antoine and Stiegler,
                   Arnaud and Datta, Debajyoti and Szczechla, Eliza and
                   Chhablani, Gunjan and Wang, Han and Pandey, Harshit and
                   Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and
                   Gao, Leo and Sutawika, Lintang and Saiful Bari, M and
                   Al-shaibani, Maged S and Manica, Matteo and Nayak, Nihal and
                   Teehan, Ryan and Albanie, Samuel and Shen, Sheng and
                   Ben-David, Srulik and Bach, Stephen H and Kim, Taewoon and
                   Bers, Tali and Fevry, Thibault and Neeraj, Trishala and
                   Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and
                   Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri,
                   Yallow and Tojarieh, Hadar and Roberts, Adam and Chung,
                   Hyung Won and Tae, Jaesung and Phang, Jason and {Ofir Press}
                   and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim
                   and Casper, Jared and Rasley, Jeff and Ryabinin, Max and
                   Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and
                   Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane
                   and Sanseviero, Omar and von Platen, Patrick and Cornette,
                   Pierre and Lavall{\'e}e, Pierre Fran{\c c}ois and Lacroix,
                   R{\'e}mi and Rajbhandari, Samyam and Gandhi, Sanchit and
                   Smith, Shaden and Requena, St{\'e}phane and Patil, Suraj and
                   Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and
                   Cheveleva, Anastasia and Ligozat, Anne-Laure and
                   Subramonian, Arjun and N{\'e}v{\'e}ol, Aur{\'e}lie and
                   Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak
                   and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina,
                   Ekaterina and Bogdanov, Eli and Winata, Genta Indra and
                   Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova,
                   Jekaterina and Forde, Jessica Zosa and Clive, Jordan and
                   Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat,
                   Marine and Clinciu, Miruna and Kim, Najoung and Cheng,
                   Newton and Serikov, Oleg and Antverg, Omer and van der Wal,
                   Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann,
                   Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina,
                   Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz,
                   Tomasz and Rieser, Verena and Protasov, Vitaly and
                   Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov,
                   Yonatan and Bamberger, Zachary and Kasner, Zden{\v e}k and
                   Rueda, Alice and Pestana, Amanda and Feizpour, Amir and
                   Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia,
                   Anthony and Unldreaj, Antigona and Aghagol, Arash and
                   Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini,
                   Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and
                   Saxena, Bharat and Ferrandis, Carlos Mu{\~n}oz and McDuff,
                   Daniel and Contractor, Danish and Lansky, David and David,
                   Davis and Kiela, Douwe and Nguyen, Duong A and Tan, Edward
                   and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and
                   Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie
                   and Bhattacharya, Indrani and Solaiman, Irene and Sedenko,
                   Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer,
                   Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio,
                   Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick,
                   Marissa and Akinlolu, Martha and McKenna, Michael and Qiu,
                   Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar,
                   Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour
                   and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and
                   Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang,
                   Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and
                   Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach
                   and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and
                   Callahan, Alison and Shukla, Anima and Miranda-Escalada,
                   Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo
                   and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu,
                   Chuxin and Fourrier, Cl{\'e}mentine and Peri{\~n}{\'a}n,
                   Daniel Le{\'o}n and Molano, Daniel and Yu, Dian and
                   Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian
                   and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully
                   and Vrabec, Helena U and Bello, Imane and Dash, Ishani and
                   Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada,
                   Jose David and Sivaraman, Karthik Rangasai and Bulchandani,
                   Lokesh and Liu, Lu and Shinzato, Luisa and de Bykhovetz,
                   Madeleine Hahn and Takeuchi, Maiko and P{\`a}mies, Marc and
                   Castillo, Maria A and Nezhurina, Marianna and S{\"a}nger,
                   Mario and Samwald, Matthias and Cullan, Michael and
                   Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina
                   and Liu, Minna and Freidank, Moritz and Kang, Myungsun and
                   Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas
                   Michio and Muellner, Nikolaus and Fung, Pascale and Haller,
                   Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and
                   Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su,
                   Ruisi and Cahyawijaya, Samuel and Garda, Samuele and
                   Deshmukh, Shlok S and Mishra, Shubhanshu and Kiblawi, Sid
                   and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti
                   and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay
                   and Gigant, Th{\'e}o and Kainuma, Tomoya and Kusa, Wojciech
                   and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman,
                   Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe
                   and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and
                   Belkada, Younes and Wolf, Thomas",
  abstract      = "Large language models (LLMs) have been shown to be able to
                   perform new tasks based on a few demonstrations or natural
                   language instructions. While these capabilities have led to
                   widespread adoption, most LLMs are developed by
                   resource-rich organizations and are frequently kept from the
                   public. As a step towards democratizing this powerful
                   technology, we present BLOOM, a 176B-parameter open-access
                   language model designed and built thanks to a collaboration
                   of hundreds of researchers. BLOOM is a decoder-only
                   Transformer language model that was trained on the ROOTS
                   corpus, a dataset comprising hundreds of sources in 46
                   natural and 13 programming languages (59 in total). We find
                   that BLOOM achieves competitive performance on a wide
                   variety of benchmarks, with stronger results after
                   undergoing multitask prompted finetuning. To facilitate
                   future research and applications using LLMs, we publicly
                   release our models and code under the Responsible AI
                   License.",
  month         =  nov,
  year          =  2022,
  keywords      = "Machine Translation;LLMs;Models",
  archivePrefix = "arXiv",
  eprint        = "2211.05100",
  primaryClass  = "cs.CL",
  arxivid       = "2211.05100"
}

@ARTICLE{Wang2019-qo,
  title         = "{SuperGLUE}: A Stickier Benchmark for {General-Purpose}
                   Language Understanding Systems",
  author        = "Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and
                   Singh, Amanpreet and Michael, Julian and Hill, Felix and
                   Levy, Omer and Bowman, Samuel R",
  abstract      = "In the last year, new models and methods for pretraining and
                   transfer learning have driven striking performance
                   improvements across a range of language understanding tasks.
                   The GLUE benchmark, introduced a little over one year ago,
                   offers a single-number metric that summarizes progress on a
                   diverse set of such tasks, but performance on the benchmark
                   has recently surpassed the level of non-expert humans,
                   suggesting limited headroom for further research. In this
                   paper we present SuperGLUE, a new benchmark styled after
                   GLUE with a new set of more difficult language understanding
                   tasks, a software toolkit, and a public leaderboard.
                   SuperGLUE is available at super.gluebenchmark.com.",
  month         =  may,
  year          =  2019,
  keywords      = "LLMs;Evaluation",
  archivePrefix = "arXiv",
  eprint        = "1905.00537",
  primaryClass  = "cs.CL",
  arxivid       = "1905.00537"
}

@ARTICLE{AI4Bharat2023-bp,
  title         = "{IndicTrans2}: Towards {High-Quality} and Accessible Machine
                   Translation Models for all 22 Scheduled Indian Languages",
  author        = "{AI4Bharat} and Gala, Jay and Chitale, Pranjal A and
                   Raghavan, A K and Doddapaneni, Sumanth and Gumma, Varun and
                   Kumar, Aswanth and Nawale, Janki and Sujatha, Anupama and
                   Puduppully, Ratish and Raghavan, Vivek and Kumar, Pratyush
                   and Khapra, Mitesh M and Dabre, Raj and Kunchukuttan, Anoop",
  abstract      = "India has a rich linguistic landscape with languages from 4
                   major language families spoken by over a billion people. 22
                   of these languages are listed in the Constitution of India
                   (referred to as scheduled languages) are the focus of this
                   work. Given the linguistic diversity, high-quality and
                   accessible Machine Translation (MT) systems are essential in
                   a country like India. Prior to this work, there was (i) no
                   parallel training data spanning all the 22 languages, (ii)
                   no robust benchmarks covering all these languages and
                   containing content relevant to India, and (iii) no existing
                   translation models which support all the 22 scheduled
                   languages of India. In this work, we aim to address this gap
                   by focusing on the missing pieces required for enabling
                   wide, easy, and open access to good machine translation
                   systems for all 22 scheduled Indian languages. We identify
                   four key areas of improvement: curating and creating larger
                   training datasets, creating diverse and high-quality
                   benchmarks, training multilingual models, and releasing
                   models with open access. Our first contribution is the
                   release of the Bharat Parallel Corpus Collection (BPCC), the
                   largest publicly available parallel corpora for Indic
                   languages. BPCC contains a total of 230M bitext pairs, of
                   which a total of 126M were newly added, including 644K
                   manually translated sentence pairs created as part of this
                   work. Our second contribution is the release of the first
                   n-way parallel benchmark covering all 22 Indian languages,
                   featuring diverse domains, Indian-origin content, and
                   source-original test sets. Next, we present IndicTrans2, the
                   first model to support all 22 languages, surpassing existing
                   models on multiple existing and new benchmarks created as a
                   part of this work. Lastly, to promote accessibility and
                   collaboration, we release our models and associated data
                   with permissive licenses at
                   https://github.com/ai4bharat/IndicTrans2.",
  month         =  may,
  year          =  2023,
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2305.16307",
  primaryClass  = "cs.CL",
  arxivid       = "2305.16307"
}

@ARTICLE{Zhao2023-yu,
  title         = "A survey of large language models",
  author        = "Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi
                   and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and
                   Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du,
                   Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and
                   Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu
                   and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen,
                   Ji-Rong",
  abstract      = "Language is essentially a complex, intricate system of human
                   expressions governed by grammatical rules. It poses a
                   significant challenge to develop capable AI algorithms for
                   comprehending and grasping a language. As a major approach,
                   language modeling has been widely studied for language
                   understanding and generation in the past two decades,
                   evolving from statistical language models to neural language
                   models. Recently, pre-trained language models (PLMs) have
                   been proposed by pre-training Transformer models over
                   large-scale corpora, showing strong capabilities in solving
                   various NLP tasks. Since researchers have found that model
                   scaling can lead to performance improvement, they further
                   study the scaling effect by increasing the model size to an
                   even larger size. Interestingly, when the parameter scale
                   exceeds a certain level, these enlarged language models not
                   only achieve a significant performance improvement but also
                   show some special abilities that are not present in
                   small-scale language models. To discriminate the difference
                   in parameter scale, the research community has coined the
                   term large language models (LLM) for the PLMs of significant
                   size. Recently, the research on LLMs has been largely
                   advanced by both academia and industry, and a remarkable
                   progress is the launch of ChatGPT, which has attracted
                   widespread attention from society. The technical evolution
                   of LLMs has been making an important impact on the entire AI
                   community, which would revolutionize the way how we develop
                   and use AI algorithms. In this survey, we review the recent
                   advances of LLMs by introducing the background, key
                   findings, and mainstream techniques. In particular, we focus
                   on four major aspects of LLMs, namely pre-training,
                   adaptation tuning, utilization, and capacity evaluation.
                   Besides, we also summarize the available resources for
                   developing LLMs and discuss the remaining issues for future
                   directions.",
  month         =  mar,
  year          =  2023,
  keywords      = "LLMs;Survey;Evaluation",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2303.18223",
  primaryClass  = "cs.CL",
  arxivid       = "2303.18223"
}

@MISC{noauthor_undated-cy,
  title    = "mistral.ai strategic memo.pdf",
  keywords = "Models"
}

@ARTICLE{Mazeika2024-np,
  title         = "{HarmBench}: A Standardized Evaluation Framework for
                   Automated Red Teaming and Robust Refusal",
  author        = "Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy
                   and Wang, Zifan and Mu, Norman and Sakhaee, Elham and Li,
                   Nathaniel and Basart, Steven and Li, Bo and Forsyth, David
                   and Hendrycks, Dan",
  abstract      = "Automated red teaming holds substantial promise for
                   uncovering and mitigating the risks associated with the
                   malicious use of large language models (LLMs), yet the field
                   lacks a standardized evaluation framework to rigorously
                   assess new methods. To address this issue, we introduce
                   HarmBench, a standardized evaluation framework for automated
                   red teaming. We identify several desirable properties
                   previously unaccounted for in red teaming evaluations and
                   systematically design HarmBench to meet these criteria.
                   Using HarmBench, we conduct a large-scale comparison of 18
                   red teaming methods and 33 target LLMs and defenses,
                   yielding novel insights. We also introduce a highly
                   efficient adversarial training method that greatly enhances
                   LLM robustness across a wide range of attacks, demonstrating
                   how HarmBench enables codevelopment of attacks and defenses.
                   We open source HarmBench at
                   https://github.com/centerforaisafety/HarmBench.",
  month         =  feb,
  year          =  2024,
  keywords      = "Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2402.04249",
  primaryClass  = "cs.LG",
  arxivid       = "2402.04249"
}

@ARTICLE{Ngo2022-qq,
  title         = "The alignment problem from a deep learning perspective",
  author        = "Ngo, Richard and Chan, Lawrence and Mindermann, S{\"o}ren",
  abstract      = "Within the coming decades, artificial general intelligence
                   (AGI) may surpass human capabilities at a wide range of
                   important tasks. We outline a case for expecting that,
                   without substantial effort to prevent it, AGIs could learn
                   to pursue goals which are undesirable (i.e. misaligned) from
                   a human perspective. We argue that if AGIs are trained in
                   ways similar to today's most capable models, they could
                   learn to act deceptively to receive higher reward, learn
                   internally-represented goals which generalize beyond their
                   training distributions, and pursue those goals using
                   power-seeking strategies. We outline how the deployment of
                   misaligned AGIs might irreversibly undermine human control
                   over the world, and briefly review research directions aimed
                   at preventing this outcome.",
  month         =  aug,
  year          =  2022,
  keywords      = "Alignment (RLHF, etc)",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2209.00626",
  primaryClass  = "cs.AI",
  arxivid       = "2209.00626"
}

@ARTICLE{Rawles2023-rn,
  title         = "Android in the Wild: A {Large-Scale} Dataset for Android
                   Device Control",
  author        = "Rawles, Christopher and Li, Alice and Rodriguez, Daniel and
                   Riva, Oriana and Lillicrap, Timothy",
  abstract      = "There is a growing interest in device-control systems that
                   can interpret human natural language instructions and
                   execute them on a digital device by directly controlling its
                   user interface. We present a dataset for device-control
                   research, Android in the Wild (AITW), which is orders of
                   magnitude larger than current datasets. The dataset contains
                   human demonstrations of device interactions, including the
                   screens and actions, and corresponding natural language
                   instructions. It consists of 715k episodes spanning 30k
                   unique instructions, four versions of Android (v10-13),and
                   eight device types (Pixel 2 XL to Pixel 6) with varying
                   screen resolutions. It contains multi-step tasks that
                   require semantic understanding of language and visual
                   context. This dataset poses a new challenge: actions
                   available through the user interface must be inferred from
                   their visual appearance. And, instead of simple UI
                   element-based actions, the action space consists of precise
                   gestures (e.g., horizontal scrolls to operate carousel
                   widgets). We organize our dataset to encourage robustness
                   analysis of device-control systems, i.e., how well a system
                   performs in the presence of new task descriptions, new
                   applications, or new platform versions. We develop two
                   agents and report performance across the dataset. The
                   dataset is available at
                   https://github.com/google-research/google-research/tree/master/android\_in\_the\_wild.",
  month         =  jul,
  year          =  2023,
  keywords      = "Applications",
  archivePrefix = "arXiv",
  eprint        = "2307.10088",
  primaryClass  = "cs.LG",
  arxivid       = "2307.10088"
}

@ARTICLE{OpenAI2023-wy,
  title         = "{GPT-4} Technical Report",
  author        = "{OpenAI}",
  abstract      = "We report the development of GPT-4, a large-scale,
                   multimodal model which can accept image and text inputs and
                   produce text outputs. While less capable than humans in many
                   real-world scenarios, GPT-4 exhibits human-level performance
                   on various professional and academic benchmarks, including
                   passing a simulated bar exam with a score around the top
                   10\% of test takers. GPT-4 is a Transformer-based model
                   pre-trained to predict the next token in a document. The
                   post-training alignment process results in improved
                   performance on measures of factuality and adherence to
                   desired behavior. A core component of this project was
                   developing infrastructure and optimization methods that
                   behave predictably across a wide range of scales. This
                   allowed us to accurately predict some aspects of GPT-4's
                   performance based on models trained with no more than
                   1/1,000th the compute of GPT-4.",
  month         =  mar,
  year          =  2023,
  keywords      = "LLMs;Models",
  archivePrefix = "arXiv",
  eprint        = "2303.08774",
  primaryClass  = "cs.CL",
  arxivid       = "2303.08774"
}

@ARTICLE{Wan2023-wg,
  title         = "Poisoning Language Models During Instruction Tuning",
  author        = "Wan, Alexander and Wallace, Eric and Shen, Sheng and Klein,
                   Dan",
  abstract      = "Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT
                   are finetuned on datasets that contain user-submitted
                   examples, e.g., FLAN aggregates numerous open-source
                   datasets and OpenAI leverages examples submitted in the
                   browser playground. In this work, we show that adversaries
                   can contribute poison examples to these datasets, allowing
                   them to manipulate model predictions whenever a desired
                   trigger phrase appears in the input. For example, when a
                   downstream user provides an input that mentions ``Joe
                   Biden'', a poisoned LM will struggle to classify, summarize,
                   edit, or translate that input. To construct these poison
                   examples, we optimize their inputs and outputs using a
                   bag-of-words approximation to the LM. We evaluate our method
                   on open-source instruction-tuned LMs. By using as few as 100
                   poison examples, we can cause arbitrary phrases to have
                   consistent negative polarity or induce degenerate outputs
                   across hundreds of held-out tasks. Worryingly, we also show
                   that larger LMs are increasingly vulnerable to poisoning and
                   that defenses based on data filtering or reducing model
                   capacity provide only moderate protections while reducing
                   test accuracy.",
  month         =  may,
  year          =  2023,
  keywords      = "Data Poison",
  archivePrefix = "arXiv",
  eprint        = "2305.00944",
  primaryClass  = "cs.CL",
  arxivid       = "2305.00944"
}

@ARTICLE{Liang2022-gn,
  title         = "Holistic Evaluation of Language Models",
  author        = "Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras,
                   Dimitris and Soylu, Dilara and Yasunaga, Michihiro and
                   Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar,
                   Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby
                   and Zhang, Ce and Cosgrove, Christian and Manning,
                   Christopher D and R{\'e}, Christopher and Acosta-Navas,
                   Diana and Hudson, Drew A and Zelikman, Eric and Durmus, Esin
                   and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao,
                   Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel
                   and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and
                   Kim, Nathan and Guha, Neel and Chatterji, Niladri and
                   Khattab, Omar and Henderson, Peter and Huang, Qian and Chi,
                   Ryan and Xie, Sang Michael and Santurkar, Shibani and
                   Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas
                   and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William
                   and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda,
                   Yuta",
  abstract      = "Language models (LMs) are becoming the foundation for almost
                   all major language technologies, but their capabilities,
                   limitations, and risks are not well understood. We present
                   Holistic Evaluation of Language Models (HELM) to improve the
                   transparency of language models. First, we taxonomize the
                   vast space of potential scenarios (i.e. use cases) and
                   metrics (i.e. desiderata) that are of interest for LMs. Then
                   we select a broad subset based on coverage and feasibility,
                   noting what's missing or underrepresented (e.g. question
                   answering for neglected English dialects, metrics for
                   trustworthiness). Second, we adopt a multi-metric approach:
                   We measure 7 metrics (accuracy, calibration, robustness,
                   fairness, bias, toxicity, and efficiency) for each of 16
                   core scenarios when possible (87.5\% of the time). This
                   ensures metrics beyond accuracy don't fall to the wayside,
                   and that trade-offs are clearly exposed. We also perform 7
                   targeted evaluations, based on 26 targeted scenarios, to
                   analyze specific aspects (e.g. reasoning, disinformation).
                   Third, we conduct a large-scale evaluation of 30 prominent
                   language models (spanning open, limited-access, and closed
                   models) on all 42 scenarios, 21 of which were not previously
                   used in mainstream LM evaluation. Prior to HELM, models on
                   average were evaluated on just 17.9\% of the core HELM
                   scenarios, with some prominent models not sharing a single
                   scenario in common. We improve this to 96.0\%: now all 30
                   models have been densely benchmarked on the same core
                   scenarios and metrics under standardized conditions. Our
                   evaluation surfaces 25 top-level findings. For full
                   transparency, we release all raw model prompts and
                   completions publicly for further analysis, as well as a
                   general modular toolkit. We intend for HELM to be a living
                   benchmark for the community, continuously updated with new
                   scenarios, metrics, and models.",
  month         =  nov,
  year          =  2022,
  keywords      = "LLMs;Survey;Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2211.09110",
  primaryClass  = "cs.CL",
  arxivid       = "2211.09110"
}

@ARTICLE{Kaddour2023-qx,
  title         = "Challenges and Applications of Large Language Models",
  author        = "Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and
                   Bradley, Herbie and Raileanu, Roberta and McHardy, Robert",
  abstract      = "Large Language Models (LLMs) went from non-existent to
                   ubiquitous in the machine learning discourse within a few
                   years. Due to the fast pace of the field, it is difficult to
                   identify the remaining challenges and already fruitful
                   application areas. In this paper, we aim to establish a
                   systematic set of open problems and application successes so
                   that ML researchers can comprehend the field's current state
                   more quickly and become productive.",
  month         =  jul,
  year          =  2023,
  keywords      = "LLMs;Survey;Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2307.10169",
  primaryClass  = "cs.CL",
  arxivid       = "2307.10169"
}

@ARTICLE{Bommasani2021-ug,
  title         = "On the Opportunities and Risks of Foundation Models",
  author        = "Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and
                   Altman, Russ and Arora, Simran and von Arx, Sydney and
                   Bernstein, Michael S and Bohg, Jeannette and Bosselut,
                   Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch,
                   Shyamal and Card, Dallas and Castellon, Rodrigo and
                   Chatterji, Niladri and Chen, Annie and Creel, Kathleen and
                   Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and
                   Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and
                   Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and
                   Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and
                   Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha,
                   Neel and Hashimoto, Tatsunori and Henderson, Peter and
                   Hewitt, John and Ho, Daniel E and Hong, Jenny and Hsu, Kyle
                   and Huang, Jing and Icard, Thomas and Jain, Saahil and
                   Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti,
                   Siddharth and Keeling, Geoff and Khani, Fereshte and
                   Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna,
                   Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak,
                   Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and
                   Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma,
                   Tengyu and Malik, Ali and Manning, Christopher D and
                   Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele
                   and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak
                   and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and
                   Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and
                   Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung
                   and Piech, Chris and Portelance, Eva and Potts, Christopher
                   and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and
                   Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan,
                   Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa,
                   Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan,
                   Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin
                   W and Tram{\`e}r, Florian and Wang, Rose E and Wang, William
                   and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang
                   Michael and Yasunaga, Michihiro and You, Jiaxuan and
                   Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and
                   Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou,
                   Kaitlyn and Liang, Percy",
  abstract      = "AI is undergoing a paradigm shift with the rise of models
                   (e.g., BERT, DALL-E, GPT-3) that are trained on broad data
                   at scale and are adaptable to a wide range of downstream
                   tasks. We call these models foundation models to underscore
                   their critically central yet incomplete character. This
                   report provides a thorough account of the opportunities and
                   risks of foundation models, ranging from their capabilities
                   (e.g., language, vision, robotics, reasoning, human
                   interaction) and technical principles(e.g., model
                   architectures, training procedures, data, systems, security,
                   evaluation, theory) to their applications (e.g., law,
                   healthcare, education) and societal impact (e.g., inequity,
                   misuse, economic and environmental impact, legal and ethical
                   considerations). Though foundation models are based on
                   standard deep learning and transfer learning, their scale
                   results in new emergent capabilities,and their effectiveness
                   across so many tasks incentivizes homogenization.
                   Homogenization provides powerful leverage but demands
                   caution, as the defects of the foundation model are
                   inherited by all the adapted models downstream. Despite the
                   impending widespread deployment of foundation models, we
                   currently lack a clear understanding of how they work, when
                   they fail, and what they are even capable of due to their
                   emergent properties. To tackle these questions, we believe
                   much of the critical research on foundation models will
                   require deep interdisciplinary collaboration commensurate
                   with their fundamentally sociotechnical nature.",
  month         =  aug,
  year          =  2021,
  keywords      = "Survey;Evaluation",
  archivePrefix = "arXiv",
  eprint        = "2108.07258",
  primaryClass  = "cs.LG",
  arxivid       = "2108.07258"
}

@ARTICLE{Zheng2023-oo,
  title         = "Judging {LLM-as-a-judge} with {MT-bench} and Chatbot Arena",
  author        = "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and
                   Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin,
                   Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P and
                   Zhang, Hao and Gonzalez, Joseph E and Stoica, Ion",
  abstract      = "Evaluating large language model (LLM) based chat assistants
                   is challenging due to their broad capabilities and the
                   inadequacy of existing benchmarks in measuring human
                   preferences. To address this, we explore using strong LLMs
                   as judges to evaluate these models on more open-ended
                   questions. We examine the usage and limitations of
                   LLM-as-a-judge, including position, verbosity, and
                   self-enhancement biases, as well as limited reasoning
                   ability, and propose solutions to mitigate some of them. We
                   then verify the agreement between LLM judges and human
                   preferences by introducing two benchmarks: MT-bench, a
                   multi-turn question set; and Chatbot Arena, a crowdsourced
                   battle platform. Our results reveal that strong LLM judges
                   like GPT-4 can match both controlled and crowdsourced human
                   preferences well, achieving over 80\% agreement, the same
                   level of agreement between humans. Hence, LLM-as-a-judge is
                   a scalable and explainable way to approximate human
                   preferences, which are otherwise very expensive to obtain.
                   Additionally, we show our benchmark and traditional
                   benchmarks complement each other by evaluating several
                   variants of LLaMA and Vicuna. We will publicly release
                   MT-bench questions, 3K expert votes, and 30K conversations
                   with human preferences from Chatbot Arena.",
  month         =  jun,
  year          =  2023,
  keywords      = "Evaluation",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2306.05685",
  primaryClass  = "cs.CL",
  arxivid       = "2306.05685"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Bender2021-qa,
  title     = "On the Dangers of Stochastic Parrots: Can Language Models Be Too
               Big? 🦜",
  booktitle = "Proceedings of the 2021 {ACM} Conference on Fairness,
               Accountability, and Transparency",
  author    = "Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina
               and Shmitchell, Shmargaret",
  abstract  = "The past 3 years of work in NLP have been characterized by the
               development and deployment of ever larger language models,
               especially for English. BERT, its variants, GPT-2/3, and others,
               most recently Switch-C, have pushed the boundaries of the
               possible both through architectural innovations and through
               sheer size. Using these pretrained models and the methodology of
               fine-tuning them for specific tasks, researchers have extended
               the state of the art on a wide array of tasks as measured by
               leaderboards on specific benchmarks for English. In this paper,
               we take a step back and ask: How big is too big? What are the
               possible risks associated with this technology and what paths
               are available for mitigating those risks? We provide
               recommendations including weighing the environmental and
               financial costs first, investing resources into curating and
               carefully documenting datasets rather than ingesting everything
               on the web, carrying out pre-development exercises evaluating
               how the planned approach fits into research and development
               goals and supports stakeholder values, and encouraging research
               directions beyond ever larger language models.",
  publisher = "Association for Computing Machinery",
  pages     = "610--623",
  series    = "FAccT '21",
  month     =  mar,
  year      =  2021,
  address   = "New York, NY, USA",
  keywords  = "Alignment (RLHF, etc)",
  location  = "Virtual Event, Canada",
  isbn      = "9781450383097",
  doi       = "10.1145/3442188.3445922"
}

@INPROCEEDINGS{Bradley_Knox2008-ql,
  title     = "{{TAMER}}: {T}raining an {A}gent {M}anually via {E}valuative
               {R}einforcement",
  booktitle = "2008 7th {IEEE} International Conference on Development and
               Learning",
  author    = "Bradley Knox, W and Stone, Peter",
  abstract  = "Though computers have surpassed humans at many tasks, especially
               computationally intensive ones, there are many tasks for which
               human expertise remains necessary and/or useful. For such tasks,
               it is desirable for a human to be able to transmit knowledge to
               a learning agent as quickly and effortlessly as possible, and,
               ideally, without any knowledge of the details of the agent's
               learning process. This paper proposes a general framework called
               Training an Agent Manually via Evaluative Reinforcement (TAMER)
               that allows a human to train a learning agent to perform a
               common class of complex tasks simply by giving scalar reward
               signals in response to the agent's observed actions.
               Specifically, in sequential decision making tasks, an agent
               models the human's reward function and chooses actions that it
               predicts will receive the most reward. Our novel algorithm is
               fully implemented and tested on the game Tetris. Leveraging the
               human trainers' feedback, the agent learns to clear an average
               of more than 50 lines by its third game, an order of magnitude
               faster than the best autonomous learning agents.",
  pages     = "292--297",
  month     =  aug,
  year      =  2008,
  keywords  = "Humans;Games;Training;Approximation
               algorithms;Learning;Robots;Supervised learning;Alignment (RLHF,
               etc)",
  issn      = "2161-9476",
  doi       = "10.1109/DEVLRN.2008.4640845"
}

@MISC{noauthor_undated-ca,
  title        = "Learning to summarize with human feedback",
  howpublished = "\url{https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html}",
  note         = "Accessed: 2023-7-26",
  keywords     = "Alignment (RLHF, etc)"
}

@ARTICLE{Leike2018-ex,
  title         = "Scalable agent alignment via reward modeling: a research
                   direction",
  author        = "Leike, Jan and Krueger, David and Everitt, Tom and Martic,
                   Miljan and Maini, Vishal and Legg, Shane",
  abstract      = "One obstacle to applying reinforcement learning algorithms
                   to real-world problems is the lack of suitable reward
                   functions. Designing such reward functions is difficult in
                   part because the user only has an implicit understanding of
                   the task objective. This gives rise to the agent alignment
                   problem: how do we create agents that behave in accordance
                   with the user's intentions? We outline a high-level research
                   direction to solve the agent alignment problem centered
                   around reward modeling: learning a reward function from
                   interaction with the user and optimizing the learned reward
                   function with reinforcement learning. We discuss the key
                   challenges we expect to face when scaling reward modeling to
                   complex and general domains, concrete approaches to mitigate
                   these challenges, and ways to establish trust in the
                   resulting agents.",
  month         =  nov,
  year          =  2018,
  keywords      = "Alignment (RLHF, etc)",
  archivePrefix = "arXiv",
  eprint        = "1811.07871",
  primaryClass  = "cs.LG",
  arxivid       = "1811.07871"
}

@ARTICLE{Ziegler2019-qg,
  title         = "{Fine-Tuning} Language Models from Human Preferences",
  author        = "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and
                   Brown, Tom B and Radford, Alec and Amodei, Dario and
                   Christiano, Paul and Irving, Geoffrey",
  abstract      = "Reward learning enables the application of reinforcement
                   learning (RL) to tasks where reward is defined by human
                   judgment, building a model of reward by asking humans
                   questions. Most work on reward learning has used simulated
                   environments, but complex information about values is often
                   expressed in natural language, and we believe reward
                   learning for language is a key to making RL practical and
                   safe for real-world tasks. In this paper, we build on
                   advances in generative pretraining of language models to
                   apply reward learning to four natural language tasks:
                   continuing text with positive sentiment or physically
                   descriptive language, and summarization tasks on the TL;DR
                   and CNN/Daily Mail datasets. For stylistic continuation we
                   achieve good results with only 5,000 comparisons evaluated
                   by humans. For summarization, models trained with 60,000
                   comparisons copy whole sentences from the input but skip
                   irrelevant preamble; this leads to reasonable ROUGE scores
                   and very good performance according to our human labelers,
                   but may be exploiting the fact that labelers rely on simple
                   heuristics.",
  month         =  sep,
  year          =  2019,
  keywords      = "Alignment (RLHF, etc)",
  archivePrefix = "arXiv",
  eprint        = "1909.08593",
  primaryClass  = "cs.CL",
  arxivid       = "1909.08593"
}

@ARTICLE{Ramamurthy2022-ne,
  title         = "Is Reinforcement Learning (Not) for Natural Language
                   Processing: Benchmarks, Baselines, and Building Blocks for
                   Natural Language Policy Optimization",
  author        = "Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and
                   Brantley, Kiant{\'e} and Hessel, Jack and Sifa, Rafet and
                   Bauckhage, Christian and Hajishirzi, Hannaneh and Choi,
                   Yejin",
  abstract      = "We tackle the problem of aligning pre-trained large language
                   models (LMs) with human preferences. If we view text
                   generation as a sequential decision-making problem,
                   reinforcement learning (RL) appears to be a natural
                   conceptual framework. However, using RL for LM-based
                   generation faces empirical challenges, including training
                   instability due to the combinatorial action space, as well
                   as a lack of open-source libraries and benchmarks customized
                   for LM alignment. Thus, a question rises in the research
                   community: is RL a practical paradigm for NLP? To help
                   answer this, we first introduce an open-source modular
                   library, RL4LMs (Reinforcement Learning for Language
                   Models), for optimizing language generators with RL. The
                   library consists of on-policy RL algorithms that can be used
                   to train any encoder or encoder-decoder LM in the
                   HuggingFace library (Wolf et al. 2020) with an arbitrary
                   reward function. Next, we present the GRUE (General
                   Reinforced-language Understanding Evaluation) benchmark, a
                   set of 6 language generation tasks which are supervised not
                   by target strings, but by reward functions which capture
                   automated measures of human preference.GRUE is the first
                   leaderboard-style evaluation of RL algorithms for NLP tasks.
                   Finally, we introduce an easy-to-use, performant RL
                   algorithm, NLPO (Natural Language Policy Optimization)\}
                   that learns to effectively reduce the combinatorial action
                   space in language generation. We show 1) that RL techniques
                   are generally better than supervised methods at aligning LMs
                   to human preferences; and 2) that NLPO exhibits greater
                   stability and performance than previous policy gradient
                   methods (e.g., PPO (Schulman et al. 2017)), based on both
                   automatic and human evaluations.",
  month         =  oct,
  year          =  2022,
  keywords      = "Alignment (RLHF, etc);Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "2210.01241",
  primaryClass  = "cs.CL",
  arxivid       = "2210.01241"
}

@ARTICLE{Wang2022-qx,
  title         = "{Self-Instruct}: Aligning Language Models with
                   {Self-Generated} Instructions",
  author        = "Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and
                   Liu, Alisa and Smith, Noah A and Khashabi, Daniel and
                   Hajishirzi, Hannaneh",
  abstract      = "Large ``instruction-tuned'' language models (i.e., finetuned
                   to respond to instructions) have demonstrated a remarkable
                   ability to generalize zero-shot to new tasks. Nevertheless,
                   they depend heavily on human-written instruction data that
                   is often limited in quantity, diversity, and creativity,
                   therefore hindering the generality of the tuned model. We
                   introduce Self-Instruct, a framework for improving the
                   instruction-following capabilities of pretrained language
                   models by bootstrapping off their own generations. Our
                   pipeline generates instructions, input, and output samples
                   from a language model, then filters invalid or similar ones
                   before using them to finetune the original model. Applying
                   our method to the vanilla GPT3, we demonstrate a 33\%
                   absolute improvement over the original model on
                   Super-NaturalInstructions, on par with the performance of
                   InstructGPT-001, which was trained with private user data
                   and human annotations. For further evaluation, we curate a
                   set of expert-written instructions for novel tasks, and show
                   through human evaluation that tuning GPT3 with Self-Instruct
                   outperforms using existing public instruction datasets by a
                   large margin, leaving only a 5\% absolute gap behind
                   InstructGPT-001. Self-Instruct provides an almost
                   annotation-free method for aligning pre-trained language
                   models with instructions, and we release our large synthetic
                   dataset to facilitate future studies on instruction tuning.
                   Our code and data are available at
                   https://github.com/yizhongw/self-instruct.",
  month         =  dec,
  year          =  2022,
  keywords      = "Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "2212.10560",
  primaryClass  = "cs.CL",
  arxivid       = "2212.10560"
}

@ARTICLE{Xu2023-qx,
  title         = "{WizardLM}: Empowering Large Language Models to Follow
                   Complex Instructions",
  author        = "Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and
                   Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang,
                   Daxin",
  abstract      = "Training large language models (LLMs) with open-domain
                   instruction following data brings colossal success. However,
                   manually creating such instruction data is very
                   time-consuming and labor-intensive. Moreover, humans may
                   struggle to produce high-complexity instructions. In this
                   paper, we show an avenue for creating large amounts of
                   instruction data with varying levels of complexity using LLM
                   instead of humans. Starting with an initial set of
                   instructions, we use our proposed Evol-Instruct to rewrite
                   them step by step into more complex instructions. Then, we
                   mix all generated instruction data to fine-tune LLaMA. We
                   call the resulting model WizardLM. Human evaluations on a
                   complexity-balanced test bed and Vicuna's testset show that
                   instructions from Evol-Instruct are superior to
                   human-created ones. By analyzing the human evaluation
                   results of the high complexity part, we demonstrate that
                   outputs from our WizardLM are preferred to outputs from
                   OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM
                   achieves more than 90\% capacity of ChatGPT on 17 out of 29
                   skills. Even though WizardLM still lags behind ChatGPT in
                   some aspects, our findings suggest that fine-tuning with
                   AI-evolved instructions is a promising direction for
                   enhancing LLMs. Our code and data are public at
                   https://github.com/nlpxucan/WizardLM",
  month         =  apr,
  year          =  2023,
  keywords      = "Evaluation;Fine-tuning",
  archivePrefix = "arXiv",
  eprint        = "2304.12244",
  primaryClass  = "cs.CL",
  arxivid       = "2304.12244"
}

@ARTICLE{Bietti2023-bv,
  title         = "Birth of a transformer: A memory viewpoint",
  author        = "Bietti, Alberto and Cabannes, Vivien and Bouchacourt, Diane
                   and Jegou, Herve and Bottou, Leon",
  abstract      = "Large language models based on transformers have achieved
                   great empirical successes. However, as they are deployed
                   more widely, there is a growing need to better understand
                   their internal mechanisms in order to make them more
                   reliable. These models appear to store vast amounts of
                   knowledge from their training data, and to adapt quickly to
                   new information provided in their context or prompt. We
                   study how transformers balance these two types of knowledge
                   by considering a synthetic setup where tokens are generated
                   from either global or context-specific bigram distributions.
                   By a careful empirical analysis of the training process on a
                   simplified two-layer transformer, we illustrate the fast
                   learning of global bigrams and the slower development of an
                   ``induction head'' mechanism for the in-context bigrams. We
                   highlight the role of weight matrices as associative
                   memories, provide theoretical insights on how gradients
                   enable their learning during training, and study the role of
                   data-distributional properties.",
  month         =  jun,
  year          =  2023,
  keywords      = "Understanding",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2306.00802",
  primaryClass  = "stat.ML",
  arxivid       = "2306.00802"
}

@ARTICLE{Tu2023-zk,
  title         = "Towards Generalist Biomedical {AI}",
  author        = "Tu, Tao and Azizi, Shekoofeh and Driess, Danny and
                   Schaekermann, Mike and Amin, Mohamed and Chang, Pi-Chuan and
                   Carroll, Andrew and Lau, Chuck and Tanno, Ryutaro and Ktena,
                   Ira and Mustafa, Basil and Chowdhery, Aakanksha and Liu, Yun
                   and Kornblith, Simon and Fleet, David and Mansfield, Philip
                   and Prakash, Sushant and Wong, Renee and Virmani, Sunny and
                   Semturs, Christopher and Sara Mahdavi, S and Green, Bradley
                   and Dominowska, Ewa and Aguera y Arcas, Blaise and Barral,
                   Joelle and Webster, Dale and Corrado, Greg S and Matias,
                   Yossi and Singhal, Karan and Florence, Pete and
                   Karthikesalingam, Alan and Natarajan, Vivek",
  abstract      = "Medicine is inherently multimodal, with rich data modalities
                   spanning text, imaging, genomics, and more. Generalist
                   biomedical artificial intelligence (AI) systems that
                   flexibly encode, integrate, and interpret this data at scale
                   can potentially enable impactful applications ranging from
                   scientific discovery to care delivery. To enable the
                   development of these models, we first curate MultiMedBench,
                   a new multimodal biomedical benchmark. MultiMedBench
                   encompasses 14 diverse tasks such as medical question
                   answering, mammography and dermatology image interpretation,
                   radiology report generation and summarization, and genomic
                   variant calling. We then introduce Med-PaLM Multimodal
                   (Med-PaLM M), our proof of concept for a generalist
                   biomedical AI system. Med-PaLM M is a large multimodal
                   generative model that flexibly encodes and interprets
                   biomedical data including clinical language, imaging, and
                   genomics with the same set of model weights. Med-PaLM M
                   reaches performance competitive with or exceeding the state
                   of the art on all MultiMedBench tasks, often surpassing
                   specialist models by a wide margin. We also report examples
                   of zero-shot generalization to novel medical concepts and
                   tasks, positive transfer learning across tasks, and emergent
                   zero-shot medical reasoning. To further probe the
                   capabilities and limitations of Med-PaLM M, we conduct a
                   radiologist evaluation of model-generated (and human) chest
                   X-ray reports and observe encouraging performance across
                   model scales. In a side-by-side ranking on 246 retrospective
                   chest X-rays, clinicians express a pairwise preference for
                   Med-PaLM M reports over those produced by radiologists in up
                   to 40.50\% of cases, suggesting potential clinical utility.
                   While considerable work is needed to validate these models
                   in real-world use cases, our results represent a milestone
                   towards the development of generalist biomedical AI systems.",
  month         =  jul,
  year          =  2023,
  keywords      = "Applications",
  archivePrefix = "arXiv",
  eprint        = "2307.14334",
  primaryClass  = "cs.CL",
  arxivid       = "2307.14334"
}

@ARTICLE{Awais2023-vk,
  title         = "Foundational Models Defining a New Era in Vision: A Survey
                   and Outlook",
  author        = "Awais, Muhammad and Naseer, Muzammal and Khan, Salman and
                   Anwer, Rao Muhammad and Cholakkal, Hisham and Shah, Mubarak
                   and Yang, Ming-Hsuan and Khan, Fahad Shahbaz",
  abstract      = "Vision systems to see and reason about the compositional
                   nature of visual scenes are fundamental to understanding our
                   world. The complex relations between objects and their
                   locations, ambiguities, and variations in the real-world
                   environment can be better described in human language,
                   naturally governed by grammatical rules and other modalities
                   such as audio and depth. The models learned to bridge the
                   gap between such modalities coupled with large-scale
                   training data facilitate contextual reasoning,
                   generalization, and prompt capabilities at test time. These
                   models are referred to as foundational models. The output of
                   such models can be modified through human-provided prompts
                   without retraining, e.g., segmenting a particular object by
                   providing a bounding box, having interactive dialogues by
                   asking questions about an image or video scene or
                   manipulating the robot's behavior through language
                   instructions. In this survey, we provide a comprehensive
                   review of such emerging foundational models, including
                   typical architecture designs to combine different modalities
                   (vision, text, audio, etc), training objectives
                   (contrastive, generative), pre-training datasets,
                   fine-tuning mechanisms, and the common prompting patterns;
                   textual, visual, and heterogeneous. We discuss the open
                   challenges and research directions for foundational models
                   in computer vision, including difficulties in their
                   evaluations and benchmarking, gaps in their real-world
                   understanding, limitations of their contextual
                   understanding, biases, vulnerability to adversarial attacks,
                   and interpretability issues. We review recent developments
                   in this field, covering a wide range of applications of
                   foundation models systematically and comprehensively. A
                   comprehensive list of foundational models studied in this
                   work is available at
                   \textbackslashurl\{https://github.com/awaisrauf/Awesome-CV-Foundational-Models\}.",
  month         =  jul,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  eprint        = "2307.13721",
  primaryClass  = "cs.CV",
  arxivid       = "2307.13721"
}

@ARTICLE{An2023-np,
  title         = "{L-Eval}: Instituting Standardized Evaluation for Long
                   Context Language Models",
  author        = "An, Chenxin and Gong, Shansan and Zhong, Ming and Li, Mukai
                   and Zhang, Jun and Kong, Lingpeng and Qiu, Xipeng",
  abstract      = "Recently, there has been growing interest in extending the
                   context length of instruction-following models in order to
                   effectively process single-turn long input (e.g. summarizing
                   a paper) and conversations with more extensive histories.
                   While proprietary models such as GPT-4 and Claude have
                   demonstrated considerable advancements in handling tens of
                   thousands of tokens of context, open-sourced models are
                   still in the early stages of experimentation. It also
                   remains unclear whether developing these long context models
                   can offer substantial gains on practical downstream tasks
                   over retrieval-based methods or models simply trained on
                   chunked contexts. To address this challenge, we propose to
                   institute standardized evaluation for long context language
                   models. Concretely, we develop L-Eval which contains 411
                   long documents and over 2,000 query-response pairs manually
                   annotated and checked by the authors encompassing areas such
                   as law, finance, school lectures, lengthy conversations,
                   news, long-form novels, and meetings. L-Eval also adopts
                   diverse evaluation methods and instruction styles, enabling
                   a more reliable assessment of Long Context Language Models
                   (LCLMs). Our findings indicate that while open-source models
                   typically lag behind their commercial counterparts, they
                   still exhibit impressive performance. LLaMA2 achieves the
                   best results (win 45\% vs turbo-16k) on open-ended tasks
                   with only 4k context length and ChatGLM2 achieves the best
                   results on closed-ended tasks with 8k input tokens. We
                   release our new evaluation suite, code, and all generation
                   results including predictions from all open-sourced LCLMs,
                   GPT4-32k, Cluade-100k at
                   \{\textbackslashurl\{https://github.com/OpenLMLab/LEval\}\}.",
  month         =  jul,
  year          =  2023,
  keywords      = "RAG",
  archivePrefix = "arXiv",
  eprint        = "2307.11088",
  primaryClass  = "cs.CL",
  arxivid       = "2307.11088"
}

@ARTICLE{Wang2023-ko,
  title         = "Aligning Large Language Models with Human: A Survey",
  author        = "Wang, Yufei and Zhong, Wanjun and Li, Liangyou and Mi, Fei
                   and Zeng, Xingshan and Huang, Wenyong and Shang, Lifeng and
                   Jiang, Xin and Liu, Qun",
  abstract      = "Large Language Models (LLMs) trained on extensive textual
                   corpora have emerged as leading solutions for a broad array
                   of Natural Language Processing (NLP) tasks. Despite their
                   notable performance, these models are prone to certain
                   limitations such as misunderstanding human instructions,
                   generating potentially biased content, or factually
                   incorrect (hallucinated) information. Hence, aligning LLMs
                   with human expectations has become an active area of
                   interest within the research community. This survey presents
                   a comprehensive overview of these alignment technologies,
                   including the following aspects. (1) Data collection: the
                   methods for effectively collecting high-quality instructions
                   for LLM alignment, including the use of NLP benchmarks,
                   human annotations, and leveraging strong LLMs. (2) Training
                   methodologies: a detailed review of the prevailing training
                   methods employed for LLM alignment. Our exploration
                   encompasses Supervised Fine-tuning, both Online and Offline
                   human preference training, along with parameter-efficient
                   training mechanisms. (3) Model Evaluation: the methods for
                   evaluating the effectiveness of these human-aligned LLMs,
                   presenting a multifaceted approach towards their assessment.
                   In conclusion, we collate and distill our findings, shedding
                   light on several promising future research avenues in the
                   field. This survey, therefore, serves as a valuable resource
                   for anyone invested in understanding and advancing the
                   alignment of LLMs to better suit human-oriented tasks and
                   expectations. An associated GitHub link collecting the
                   latest papers is available at
                   https://github.com/GaryYufei/AlignLLMHumanSurvey.",
  month         =  jul,
  year          =  2023,
  keywords      = "Survey",
  archivePrefix = "arXiv",
  eprint        = "2307.12966",
  primaryClass  = "cs.CL",
  arxivid       = "2307.12966"
}
